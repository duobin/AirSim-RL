{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this notebook is a copy of train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "['WayPoint0', 'WayPoint1', 'WayPoint2', 'WayPoint3', 'WayPoint4', 'WayPoint5', 'WayPoint6', 'WayPoint7', 'WayPoint8', 'WayPoint9']\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,687,719\n",
      "Trainable params: 1,687,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hoang\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\rl\\util.py:79: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training for 2000000 steps ...\n",
      "     199/2000000: episode: 1, duration: 10.704s, episode steps: 199, steps per second: 19, episode reward: 76.000, mean reward: 0.382 [-1.000, 0.500], mean action: 2.879 [0.000, 6.000], mean observation: 172.088 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     328/2000000: episode: 2, duration: 6.242s, episode steps: 129, steps per second: 21, episode reward: 76.200, mean reward: 0.591 [-1.000, 1.000], mean action: 3.341 [0.000, 6.000], mean observation: 173.314 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     547/2000000: episode: 3, duration: 9.981s, episode steps: 219, steps per second: 22, episode reward: 158.500, mean reward: 0.724 [-1.000, 1.000], mean action: 3.087 [0.000, 6.000], mean observation: 171.977 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     753/2000000: episode: 4, duration: 9.152s, episode steps: 206, steps per second: 23, episode reward: 92.300, mean reward: 0.448 [-1.000, 0.500], mean action: 2.942 [0.000, 6.000], mean observation: 171.996 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     986/2000000: episode: 5, duration: 10.713s, episode steps: 233, steps per second: 22, episode reward: 179.200, mean reward: 0.769 [-1.000, 1.000], mean action: 3.124 [0.000, 6.000], mean observation: 172.555 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1194/2000000: episode: 6, duration: 9.159s, episode steps: 208, steps per second: 23, episode reward: 99.600, mean reward: 0.479 [-1.000, 1.000], mean action: 2.928 [0.000, 6.000], mean observation: 172.077 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1435/2000000: episode: 7, duration: 8.011s, episode steps: 241, steps per second: 30, episode reward: 92.600, mean reward: 0.384 [-1.000, 0.500], mean action: 3.012 [0.000, 6.000], mean observation: 172.366 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1622/2000000: episode: 8, duration: 6.817s, episode steps: 187, steps per second: 27, episode reward: 69.200, mean reward: 0.370 [-1.000, 0.500], mean action: 2.791 [0.000, 6.000], mean observation: 172.568 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1834/2000000: episode: 9, duration: 9.402s, episode steps: 212, steps per second: 23, episode reward: 145.800, mean reward: 0.688 [-1.000, 1.000], mean action: 2.948 [0.000, 6.000], mean observation: 171.858 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2068/2000000: episode: 10, duration: 10.568s, episode steps: 234, steps per second: 22, episode reward: 123.400, mean reward: 0.527 [-1.000, 1.000], mean action: 3.094 [0.000, 6.000], mean observation: 173.597 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2234/2000000: episode: 11, duration: 7.214s, episode steps: 166, steps per second: 23, episode reward: 63.100, mean reward: 0.380 [-1.000, 0.500], mean action: 2.855 [0.000, 6.000], mean observation: 172.364 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2448/2000000: episode: 12, duration: 10.043s, episode steps: 214, steps per second: 21, episode reward: 130.700, mean reward: 0.611 [-1.000, 1.000], mean action: 2.902 [0.000, 6.000], mean observation: 172.849 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2634/2000000: episode: 13, duration: 8.973s, episode steps: 186, steps per second: 21, episode reward: 84.900, mean reward: 0.456 [-1.000, 1.000], mean action: 3.059 [0.000, 6.000], mean observation: 172.492 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2812/2000000: episode: 14, duration: 8.998s, episode steps: 178, steps per second: 20, episode reward: 107.600, mean reward: 0.604 [-1.000, 1.000], mean action: 3.090 [0.000, 6.000], mean observation: 171.758 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2998/2000000: episode: 15, duration: 9.258s, episode steps: 186, steps per second: 20, episode reward: 120.700, mean reward: 0.649 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 171.620 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    3193/2000000: episode: 16, duration: 10.010s, episode steps: 195, steps per second: 19, episode reward: 133.000, mean reward: 0.682 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 172.904 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3383/2000000: episode: 17, duration: 9.572s, episode steps: 190, steps per second: 20, episode reward: 142.600, mean reward: 0.751 [-1.000, 1.000], mean action: 2.968 [0.000, 6.000], mean observation: 172.108 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    3565/2000000: episode: 18, duration: 8.958s, episode steps: 182, steps per second: 20, episode reward: 76.300, mean reward: 0.419 [-1.000, 0.500], mean action: 2.951 [0.000, 6.000], mean observation: 173.011 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    3748/2000000: episode: 19, duration: 8.958s, episode steps: 183, steps per second: 20, episode reward: 116.300, mean reward: 0.636 [-1.000, 1.000], mean action: 3.131 [0.000, 6.000], mean observation: 171.915 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    3925/2000000: episode: 20, duration: 8.607s, episode steps: 177, steps per second: 21, episode reward: 80.400, mean reward: 0.454 [-1.000, 1.000], mean action: 2.768 [0.000, 6.000], mean observation: 172.785 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    4103/2000000: episode: 21, duration: 8.674s, episode steps: 178, steps per second: 21, episode reward: 73.100, mean reward: 0.411 [-1.000, 0.500], mean action: 2.831 [0.000, 6.000], mean observation: 172.741 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    4286/2000000: episode: 22, duration: 9.039s, episode steps: 183, steps per second: 20, episode reward: 78.000, mean reward: 0.426 [-1.000, 0.500], mean action: 3.016 [0.000, 6.000], mean observation: 172.759 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    4485/2000000: episode: 23, duration: 10.185s, episode steps: 199, steps per second: 20, episode reward: 133.700, mean reward: 0.672 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 173.022 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    4660/2000000: episode: 24, duration: 8.473s, episode steps: 175, steps per second: 21, episode reward: 68.800, mean reward: 0.393 [-1.000, 0.500], mean action: 2.994 [0.000, 6.000], mean observation: 172.955 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    4832/2000000: episode: 25, duration: 8.263s, episode steps: 172, steps per second: 21, episode reward: 67.300, mean reward: 0.391 [-1.000, 0.500], mean action: 3.047 [0.000, 6.000], mean observation: 173.071 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    5040/2000000: episode: 26, duration: 10.818s, episode steps: 208, steps per second: 19, episode reward: 135.100, mean reward: 0.650 [-1.000, 1.000], mean action: 3.144 [0.000, 6.000], mean observation: 173.397 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    5201/2000000: episode: 27, duration: 7.748s, episode steps: 161, steps per second: 21, episode reward: 67.400, mean reward: 0.419 [-1.000, 0.500], mean action: 3.093 [0.000, 6.000], mean observation: 171.963 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    5382/2000000: episode: 28, duration: 8.343s, episode steps: 181, steps per second: 22, episode reward: 71.400, mean reward: 0.394 [-1.000, 0.500], mean action: 2.934 [0.000, 6.000], mean observation: 172.450 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    5573/2000000: episode: 29, duration: 8.220s, episode steps: 191, steps per second: 23, episode reward: 76.400, mean reward: 0.400 [-1.000, 0.500], mean action: 3.042 [0.000, 6.000], mean observation: 172.314 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    5750/2000000: episode: 30, duration: 7.371s, episode steps: 177, steps per second: 24, episode reward: 74.200, mean reward: 0.419 [-1.000, 0.500], mean action: 2.859 [0.000, 6.000], mean observation: 172.281 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6009/2000000: episode: 31, duration: 12.020s, episode steps: 259, steps per second: 22, episode reward: 186.900, mean reward: 0.722 [-1.000, 1.000], mean action: 3.212 [0.000, 6.000], mean observation: 173.295 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6177/2000000: episode: 32, duration: 13.067s, episode steps: 168, steps per second: 13, episode reward: 71.700, mean reward: 0.427 [-1.000, 0.500], mean action: 2.661 [0.000, 6.000], mean observation: 172.332 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6334/2000000: episode: 33, duration: 6.478s, episode steps: 157, steps per second: 24, episode reward: 55.800, mean reward: 0.355 [-1.000, 0.500], mean action: 2.701 [0.000, 6.000], mean observation: 172.876 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6547/2000000: episode: 34, duration: 9.899s, episode steps: 213, steps per second: 22, episode reward: 148.300, mean reward: 0.696 [-1.000, 1.000], mean action: 2.991 [0.000, 6.000], mean observation: 172.421 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6717/2000000: episode: 35, duration: 7.332s, episode steps: 170, steps per second: 23, episode reward: 72.300, mean reward: 0.425 [-1.000, 0.500], mean action: 2.812 [0.000, 6.000], mean observation: 172.064 [25.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    6947/2000000: episode: 36, duration: 10.764s, episode steps: 230, steps per second: 21, episode reward: 166.800, mean reward: 0.725 [-1.000, 1.000], mean action: 3.096 [0.000, 6.000], mean observation: 173.137 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    7150/2000000: episode: 37, duration: 9.266s, episode steps: 203, steps per second: 22, episode reward: 101.200, mean reward: 0.499 [-1.000, 1.000], mean action: 2.961 [0.000, 6.000], mean observation: 172.228 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    7307/2000000: episode: 38, duration: 6.525s, episode steps: 157, steps per second: 24, episode reward: 56.200, mean reward: 0.358 [-1.000, 0.500], mean action: 2.873 [0.000, 6.000], mean observation: 172.788 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    7535/2000000: episode: 39, duration: 10.691s, episode steps: 228, steps per second: 21, episode reward: 166.000, mean reward: 0.728 [-1.000, 1.000], mean action: 3.211 [0.000, 6.000], mean observation: 172.835 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    7741/2000000: episode: 40, duration: 9.329s, episode steps: 206, steps per second: 22, episode reward: 95.800, mean reward: 0.465 [-1.000, 1.000], mean action: 2.888 [0.000, 6.000], mean observation: 172.127 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    7934/2000000: episode: 41, duration: 8.283s, episode steps: 193, steps per second: 23, episode reward: 78.200, mean reward: 0.405 [-1.000, 0.500], mean action: 2.995 [0.000, 6.000], mean observation: 171.874 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    8113/2000000: episode: 42, duration: 7.591s, episode steps: 179, steps per second: 24, episode reward: 71.200, mean reward: 0.398 [-1.000, 0.500], mean action: 2.989 [0.000, 6.000], mean observation: 172.090 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    8312/2000000: episode: 43, duration: 8.755s, episode steps: 199, steps per second: 23, episode reward: 122.100, mean reward: 0.614 [-1.000, 1.000], mean action: 2.884 [0.000, 6.000], mean observation: 171.885 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    8486/2000000: episode: 44, duration: 7.390s, episode steps: 174, steps per second: 24, episode reward: 73.500, mean reward: 0.422 [-1.000, 0.500], mean action: 2.862 [0.000, 6.000], mean observation: 172.436 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    8653/2000000: episode: 45, duration: 7.117s, episode steps: 167, steps per second: 23, episode reward: 70.800, mean reward: 0.424 [-1.000, 0.500], mean action: 2.862 [0.000, 6.000], mean observation: 172.189 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8833/2000000: episode: 46, duration: 7.686s, episode steps: 180, steps per second: 23, episode reward: 73.300, mean reward: 0.407 [-1.000, 0.500], mean action: 2.833 [0.000, 6.000], mean observation: 172.125 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    9013/2000000: episode: 47, duration: 7.582s, episode steps: 180, steps per second: 24, episode reward: 55.700, mean reward: 0.309 [-1.000, 0.500], mean action: 2.883 [0.000, 6.000], mean observation: 172.735 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    9218/2000000: episode: 48, duration: 9.074s, episode steps: 205, steps per second: 23, episode reward: 78.600, mean reward: 0.383 [-1.000, 0.500], mean action: 2.971 [0.000, 6.000], mean observation: 172.466 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    9443/2000000: episode: 49, duration: 10.508s, episode steps: 225, steps per second: 21, episode reward: 137.600, mean reward: 0.612 [-1.000, 1.000], mean action: 3.062 [0.000, 6.000], mean observation: 172.838 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    9650/2000000: episode: 50, duration: 9.408s, episode steps: 207, steps per second: 22, episode reward: 142.400, mean reward: 0.688 [-1.000, 1.000], mean action: 2.947 [0.000, 6.000], mean observation: 171.901 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    9883/2000000: episode: 51, duration: 11.032s, episode steps: 233, steps per second: 21, episode reward: 168.000, mean reward: 0.721 [-1.000, 1.000], mean action: 3.292 [0.000, 6.000], mean observation: 172.251 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   10057/2000000: episode: 52, duration: 7.387s, episode steps: 174, steps per second: 24, episode reward: 71.100, mean reward: 0.409 [-1.000, 0.500], mean action: 2.920 [0.000, 6.000], mean observation: 172.244 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   10270/2000000: episode: 53, duration: 9.782s, episode steps: 213, steps per second: 22, episode reward: 135.800, mean reward: 0.638 [-1.000, 1.000], mean action: 3.117 [0.000, 6.000], mean observation: 172.368 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   10489/2000000: episode: 54, duration: 10.159s, episode steps: 219, steps per second: 22, episode reward: 158.400, mean reward: 0.723 [-1.000, 1.000], mean action: 3.059 [0.000, 6.000], mean observation: 172.572 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   10681/2000000: episode: 55, duration: 8.444s, episode steps: 192, steps per second: 23, episode reward: 117.100, mean reward: 0.610 [-1.000, 1.000], mean action: 2.698 [0.000, 6.000], mean observation: 171.274 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   10884/2000000: episode: 56, duration: 9.150s, episode steps: 203, steps per second: 22, episode reward: 98.900, mean reward: 0.487 [-1.000, 1.000], mean action: 3.108 [0.000, 6.000], mean observation: 171.959 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   11098/2000000: episode: 57, duration: 9.845s, episode steps: 214, steps per second: 22, episode reward: 139.900, mean reward: 0.654 [-1.000, 1.000], mean action: 2.967 [0.000, 6.000], mean observation: 172.025 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   11326/2000000: episode: 58, duration: 10.574s, episode steps: 228, steps per second: 22, episode reward: 143.300, mean reward: 0.629 [-1.000, 1.000], mean action: 2.952 [0.000, 6.000], mean observation: 173.185 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   11505/2000000: episode: 59, duration: 7.708s, episode steps: 179, steps per second: 23, episode reward: 75.600, mean reward: 0.422 [-1.000, 0.500], mean action: 2.793 [0.000, 6.000], mean observation: 171.311 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   11689/2000000: episode: 60, duration: 7.818s, episode steps: 184, steps per second: 24, episode reward: 74.900, mean reward: 0.407 [-1.000, 0.500], mean action: 2.940 [0.000, 6.000], mean observation: 171.569 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   11871/2000000: episode: 61, duration: 7.816s, episode steps: 182, steps per second: 23, episode reward: 76.300, mean reward: 0.419 [-1.000, 0.500], mean action: 2.863 [0.000, 6.000], mean observation: 171.312 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   12087/2000000: episode: 62, duration: 9.749s, episode steps: 216, steps per second: 22, episode reward: 96.900, mean reward: 0.449 [-1.000, 0.500], mean action: 3.102 [0.000, 6.000], mean observation: 172.433 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   12287/2000000: episode: 63, duration: 8.714s, episode steps: 200, steps per second: 23, episode reward: 81.300, mean reward: 0.406 [-1.000, 0.500], mean action: 2.925 [0.000, 6.000], mean observation: 171.690 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   12496/2000000: episode: 64, duration: 9.645s, episode steps: 209, steps per second: 22, episode reward: 104.900, mean reward: 0.502 [-1.000, 1.000], mean action: 3.038 [0.000, 6.000], mean observation: 171.839 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   12679/2000000: episode: 65, duration: 7.834s, episode steps: 183, steps per second: 23, episode reward: 70.400, mean reward: 0.385 [-1.000, 0.500], mean action: 3.022 [0.000, 6.000], mean observation: 171.542 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   12893/2000000: episode: 66, duration: 9.923s, episode steps: 214, steps per second: 22, episode reward: 103.000, mean reward: 0.481 [-1.000, 1.000], mean action: 3.210 [0.000, 6.000], mean observation: 172.044 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   13084/2000000: episode: 67, duration: 8.265s, episode steps: 191, steps per second: 23, episode reward: 78.400, mean reward: 0.410 [-1.000, 0.500], mean action: 2.791 [0.000, 6.000], mean observation: 171.603 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   13289/2000000: episode: 68, duration: 9.941s, episode steps: 205, steps per second: 21, episode reward: 114.100, mean reward: 0.557 [-1.000, 1.000], mean action: 3.366 [0.000, 6.000], mean observation: 173.412 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   13508/2000000: episode: 69, duration: 10.151s, episode steps: 219, steps per second: 22, episode reward: 153.800, mean reward: 0.702 [-1.000, 1.000], mean action: 3.151 [0.000, 6.000], mean observation: 171.916 [25.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   13722/2000000: episode: 70, duration: 9.857s, episode steps: 214, steps per second: 22, episode reward: 99.800, mean reward: 0.466 [-1.000, 1.000], mean action: 3.126 [0.000, 6.000], mean observation: 172.032 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   13948/2000000: episode: 71, duration: 10.481s, episode steps: 226, steps per second: 22, episode reward: 129.500, mean reward: 0.573 [-1.000, 1.000], mean action: 3.150 [0.000, 6.000], mean observation: 172.620 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   14158/2000000: episode: 72, duration: 9.680s, episode steps: 210, steps per second: 22, episode reward: 114.500, mean reward: 0.545 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 171.425 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   14351/2000000: episode: 73, duration: 8.333s, episode steps: 193, steps per second: 23, episode reward: 78.600, mean reward: 0.407 [-1.000, 0.500], mean action: 2.803 [0.000, 6.000], mean observation: 171.394 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   14588/2000000: episode: 74, duration: 11.146s, episode steps: 237, steps per second: 21, episode reward: 169.900, mean reward: 0.717 [-1.000, 1.000], mean action: 3.135 [0.000, 6.000], mean observation: 172.312 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14798/2000000: episode: 75, duration: 9.678s, episode steps: 210, steps per second: 22, episode reward: 113.400, mean reward: 0.540 [-1.000, 1.000], mean action: 3.043 [0.000, 6.000], mean observation: 171.495 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   15028/2000000: episode: 76, duration: 10.852s, episode steps: 230, steps per second: 21, episode reward: 143.700, mean reward: 0.625 [-1.000, 1.000], mean action: 3.122 [0.000, 6.000], mean observation: 172.358 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   15198/2000000: episode: 77, duration: 7.170s, episode steps: 170, steps per second: 24, episode reward: 58.300, mean reward: 0.343 [-1.000, 0.500], mean action: 3.076 [0.000, 6.000], mean observation: 171.806 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   15393/2000000: episode: 78, duration: 8.357s, episode steps: 195, steps per second: 23, episode reward: 76.800, mean reward: 0.394 [-1.000, 0.500], mean action: 2.723 [0.000, 6.000], mean observation: 171.629 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   15593/2000000: episode: 79, duration: 8.776s, episode steps: 200, steps per second: 23, episode reward: 81.700, mean reward: 0.408 [-1.000, 0.500], mean action: 2.990 [0.000, 6.000], mean observation: 171.757 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   15823/2000000: episode: 80, duration: 10.861s, episode steps: 230, steps per second: 21, episode reward: 147.200, mean reward: 0.640 [-1.000, 1.000], mean action: 3.091 [0.000, 6.000], mean observation: 172.220 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   16014/2000000: episode: 81, duration: 8.266s, episode steps: 191, steps per second: 23, episode reward: 115.400, mean reward: 0.604 [-1.000, 1.000], mean action: 2.974 [0.000, 6.000], mean observation: 170.585 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   16223/2000000: episode: 82, duration: 9.450s, episode steps: 209, steps per second: 22, episode reward: 134.200, mean reward: 0.642 [-1.000, 1.000], mean action: 2.885 [0.000, 6.000], mean observation: 170.981 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   16476/2000000: episode: 83, duration: 12.218s, episode steps: 253, steps per second: 21, episode reward: 179.400, mean reward: 0.709 [-1.000, 1.000], mean action: 3.221 [0.000, 6.000], mean observation: 172.155 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   16676/2000000: episode: 84, duration: 8.696s, episode steps: 200, steps per second: 23, episode reward: 79.300, mean reward: 0.396 [-1.000, 0.500], mean action: 3.065 [0.000, 6.000], mean observation: 171.615 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   16880/2000000: episode: 85, duration: 9.157s, episode steps: 204, steps per second: 22, episode reward: 110.400, mean reward: 0.541 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 171.248 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   17055/2000000: episode: 86, duration: 7.455s, episode steps: 175, steps per second: 23, episode reward: 89.200, mean reward: 0.510 [-1.000, 1.000], mean action: 2.954 [0.000, 6.000], mean observation: 171.032 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   17254/2000000: episode: 87, duration: 8.678s, episode steps: 199, steps per second: 23, episode reward: 116.400, mean reward: 0.585 [-1.000, 1.000], mean action: 2.980 [0.000, 6.000], mean observation: 170.721 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   17436/2000000: episode: 88, duration: 7.855s, episode steps: 182, steps per second: 23, episode reward: 101.600, mean reward: 0.558 [-1.000, 1.000], mean action: 2.786 [0.000, 6.000], mean observation: 170.892 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   17618/2000000: episode: 89, duration: 7.721s, episode steps: 182, steps per second: 24, episode reward: 67.900, mean reward: 0.373 [-1.000, 0.500], mean action: 3.060 [0.000, 6.000], mean observation: 171.738 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   17823/2000000: episode: 90, duration: 9.296s, episode steps: 205, steps per second: 22, episode reward: 95.300, mean reward: 0.465 [-1.000, 1.000], mean action: 3.068 [0.000, 6.000], mean observation: 171.326 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   18041/2000000: episode: 91, duration: 10.019s, episode steps: 218, steps per second: 22, episode reward: 108.800, mean reward: 0.499 [-1.000, 1.000], mean action: 3.165 [0.000, 6.000], mean observation: 171.850 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   18289/2000000: episode: 92, duration: 11.831s, episode steps: 248, steps per second: 21, episode reward: 123.800, mean reward: 0.499 [-1.000, 1.000], mean action: 3.173 [0.000, 6.000], mean observation: 172.613 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   18547/2000000: episode: 93, duration: 12.347s, episode steps: 258, steps per second: 21, episode reward: 141.700, mean reward: 0.549 [-1.000, 1.000], mean action: 3.178 [0.000, 6.000], mean observation: 172.391 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   18758/2000000: episode: 94, duration: 9.553s, episode steps: 211, steps per second: 22, episode reward: 146.300, mean reward: 0.693 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 171.300 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   18968/2000000: episode: 95, duration: 9.619s, episode steps: 210, steps per second: 22, episode reward: 111.000, mean reward: 0.529 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 172.027 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   19185/2000000: episode: 96, duration: 9.977s, episode steps: 217, steps per second: 22, episode reward: 152.800, mean reward: 0.704 [-1.000, 1.000], mean action: 3.032 [0.000, 6.000], mean observation: 171.861 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   19412/2000000: episode: 97, duration: 10.417s, episode steps: 227, steps per second: 22, episode reward: 136.700, mean reward: 0.602 [-1.000, 1.000], mean action: 3.040 [0.000, 6.000], mean observation: 173.295 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   19613/2000000: episode: 98, duration: 8.756s, episode steps: 201, steps per second: 23, episode reward: 87.300, mean reward: 0.434 [-1.000, 1.000], mean action: 2.925 [0.000, 6.000], mean observation: 171.712 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   19839/2000000: episode: 99, duration: 10.513s, episode steps: 226, steps per second: 21, episode reward: 131.000, mean reward: 0.580 [-1.000, 1.000], mean action: 3.044 [0.000, 6.000], mean observation: 172.626 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   20050/2000000: episode: 100, duration: 9.463s, episode steps: 211, steps per second: 22, episode reward: 83.600, mean reward: 0.396 [-1.000, 0.500], mean action: 3.085 [0.000, 6.000], mean observation: 172.526 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   20218/2000000: episode: 101, duration: 6.885s, episode steps: 168, steps per second: 24, episode reward: 55.300, mean reward: 0.329 [-1.000, 0.500], mean action: 2.804 [0.000, 6.000], mean observation: 172.805 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   20468/2000000: episode: 102, duration: 11.842s, episode steps: 250, steps per second: 21, episode reward: 131.600, mean reward: 0.526 [-1.000, 1.000], mean action: 3.176 [0.000, 6.000], mean observation: 173.046 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   20664/2000000: episode: 103, duration: 8.402s, episode steps: 196, steps per second: 23, episode reward: 75.700, mean reward: 0.386 [-1.000, 0.500], mean action: 2.949 [0.000, 6.000], mean observation: 172.791 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20844/2000000: episode: 104, duration: 7.683s, episode steps: 180, steps per second: 23, episode reward: 75.300, mean reward: 0.418 [-1.000, 0.500], mean action: 2.944 [0.000, 6.000], mean observation: 172.096 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   21051/2000000: episode: 105, duration: 9.094s, episode steps: 207, steps per second: 23, episode reward: 92.000, mean reward: 0.444 [-1.000, 0.500], mean action: 2.894 [0.000, 6.000], mean observation: 172.848 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   21278/2000000: episode: 106, duration: 10.525s, episode steps: 227, steps per second: 22, episode reward: 170.900, mean reward: 0.753 [-1.000, 1.000], mean action: 3.048 [0.000, 6.000], mean observation: 173.538 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   21480/2000000: episode: 107, duration: 8.678s, episode steps: 202, steps per second: 23, episode reward: 77.900, mean reward: 0.386 [-1.000, 0.500], mean action: 2.995 [0.000, 6.000], mean observation: 172.588 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   21646/2000000: episode: 108, duration: 8.029s, episode steps: 166, steps per second: 21, episode reward: 93.800, mean reward: 0.565 [-1.000, 1.000], mean action: 3.295 [0.000, 6.000], mean observation: 172.751 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   21859/2000000: episode: 109, duration: 9.782s, episode steps: 213, steps per second: 22, episode reward: 140.200, mean reward: 0.658 [-1.000, 1.000], mean action: 3.192 [0.000, 6.000], mean observation: 171.968 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   22085/2000000: episode: 110, duration: 10.362s, episode steps: 226, steps per second: 22, episode reward: 145.700, mean reward: 0.645 [-1.000, 1.000], mean action: 2.938 [0.000, 6.000], mean observation: 173.627 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   22286/2000000: episode: 111, duration: 8.907s, episode steps: 201, steps per second: 23, episode reward: 92.400, mean reward: 0.460 [-1.000, 1.000], mean action: 3.080 [0.000, 6.000], mean observation: 172.106 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   22487/2000000: episode: 112, duration: 8.719s, episode steps: 201, steps per second: 23, episode reward: 80.600, mean reward: 0.401 [-1.000, 0.500], mean action: 2.950 [0.000, 6.000], mean observation: 172.466 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   22715/2000000: episode: 113, duration: 10.598s, episode steps: 228, steps per second: 22, episode reward: 157.100, mean reward: 0.689 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 173.383 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   22955/2000000: episode: 114, duration: 11.307s, episode steps: 240, steps per second: 21, episode reward: 143.700, mean reward: 0.599 [-1.000, 1.000], mean action: 3.058 [0.000, 6.000], mean observation: 172.843 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   23188/2000000: episode: 115, duration: 10.982s, episode steps: 233, steps per second: 21, episode reward: 161.800, mean reward: 0.694 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 173.120 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   23384/2000000: episode: 116, duration: 8.507s, episode steps: 196, steps per second: 23, episode reward: 85.600, mean reward: 0.437 [-1.000, 1.000], mean action: 3.092 [0.000, 6.000], mean observation: 172.106 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   23567/2000000: episode: 117, duration: 7.691s, episode steps: 183, steps per second: 24, episode reward: 65.200, mean reward: 0.356 [-1.000, 0.500], mean action: 2.770 [0.000, 6.000], mean observation: 172.474 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   23776/2000000: episode: 118, duration: 9.628s, episode steps: 209, steps per second: 22, episode reward: 126.400, mean reward: 0.605 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 172.400 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   23976/2000000: episode: 119, duration: 8.625s, episode steps: 200, steps per second: 23, episode reward: 82.400, mean reward: 0.412 [-1.000, 1.000], mean action: 2.965 [0.000, 6.000], mean observation: 172.483 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   24197/2000000: episode: 120, duration: 10.166s, episode steps: 221, steps per second: 22, episode reward: 142.000, mean reward: 0.643 [-1.000, 1.000], mean action: 3.041 [0.000, 6.000], mean observation: 172.981 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   24383/2000000: episode: 121, duration: 7.908s, episode steps: 186, steps per second: 24, episode reward: 76.300, mean reward: 0.410 [-1.000, 0.500], mean action: 2.887 [0.000, 6.000], mean observation: 172.164 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   24583/2000000: episode: 122, duration: 8.568s, episode steps: 200, steps per second: 23, episode reward: 78.500, mean reward: 0.393 [-1.000, 0.500], mean action: 2.895 [0.000, 6.000], mean observation: 172.676 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   24786/2000000: episode: 123, duration: 8.956s, episode steps: 203, steps per second: 23, episode reward: 96.900, mean reward: 0.477 [-1.000, 1.000], mean action: 2.847 [0.000, 6.000], mean observation: 172.023 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   25004/2000000: episode: 124, duration: 10.094s, episode steps: 218, steps per second: 22, episode reward: 156.200, mean reward: 0.717 [-1.000, 1.000], mean action: 2.922 [0.000, 6.000], mean observation: 172.305 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   25169/2000000: episode: 125, duration: 6.827s, episode steps: 165, steps per second: 24, episode reward: 59.000, mean reward: 0.358 [-1.000, 0.500], mean action: 2.655 [0.000, 6.000], mean observation: 173.022 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   25349/2000000: episode: 126, duration: 7.660s, episode steps: 180, steps per second: 23, episode reward: 90.200, mean reward: 0.501 [-1.000, 1.000], mean action: 2.989 [0.000, 6.000], mean observation: 171.844 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   25592/2000000: episode: 127, duration: 11.476s, episode steps: 243, steps per second: 21, episode reward: 124.000, mean reward: 0.510 [-1.000, 1.000], mean action: 3.173 [0.000, 6.000], mean observation: 173.229 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   25824/2000000: episode: 128, duration: 10.877s, episode steps: 232, steps per second: 21, episode reward: 134.100, mean reward: 0.578 [-1.000, 1.000], mean action: 3.121 [0.000, 6.000], mean observation: 173.011 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   26025/2000000: episode: 129, duration: 8.846s, episode steps: 201, steps per second: 23, episode reward: 85.500, mean reward: 0.425 [-1.000, 1.000], mean action: 3.174 [0.000, 6.000], mean observation: 172.220 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   26198/2000000: episode: 130, duration: 7.324s, episode steps: 173, steps per second: 24, episode reward: 70.200, mean reward: 0.406 [-1.000, 0.500], mean action: 2.844 [0.000, 6.000], mean observation: 172.188 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   26446/2000000: episode: 131, duration: 11.682s, episode steps: 248, steps per second: 21, episode reward: 169.600, mean reward: 0.684 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 172.821 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26611/2000000: episode: 132, duration: 6.796s, episode steps: 165, steps per second: 24, episode reward: 57.000, mean reward: 0.345 [-1.000, 0.500], mean action: 2.836 [0.000, 6.000], mean observation: 172.686 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   26812/2000000: episode: 133, duration: 9.603s, episode steps: 201, steps per second: 21, episode reward: 122.400, mean reward: 0.609 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 172.008 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27010/2000000: episode: 134, duration: 9.048s, episode steps: 198, steps per second: 22, episode reward: 112.000, mean reward: 0.566 [-1.000, 1.000], mean action: 2.929 [0.000, 6.000], mean observation: 171.319 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27184/2000000: episode: 135, duration: 7.749s, episode steps: 174, steps per second: 22, episode reward: 71.500, mean reward: 0.411 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 171.786 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27372/2000000: episode: 136, duration: 8.029s, episode steps: 188, steps per second: 23, episode reward: 58.500, mean reward: 0.311 [-1.000, 0.500], mean action: 2.851 [0.000, 6.000], mean observation: 172.617 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27541/2000000: episode: 137, duration: 7.135s, episode steps: 169, steps per second: 24, episode reward: 65.800, mean reward: 0.389 [-1.000, 0.500], mean action: 2.598 [0.000, 6.000], mean observation: 171.897 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27758/2000000: episode: 138, duration: 9.568s, episode steps: 217, steps per second: 23, episode reward: 68.600, mean reward: 0.316 [-1.000, 0.500], mean action: 3.194 [0.000, 6.000], mean observation: 172.609 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   27952/2000000: episode: 139, duration: 8.508s, episode steps: 194, steps per second: 23, episode reward: 74.300, mean reward: 0.383 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 172.329 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   28142/2000000: episode: 140, duration: 8.408s, episode steps: 190, steps per second: 23, episode reward: 74.700, mean reward: 0.393 [-1.000, 0.500], mean action: 2.947 [0.000, 6.000], mean observation: 172.056 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   28352/2000000: episode: 141, duration: 9.697s, episode steps: 210, steps per second: 22, episode reward: 148.800, mean reward: 0.709 [-1.000, 1.000], mean action: 3.029 [0.000, 6.000], mean observation: 171.537 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   28555/2000000: episode: 142, duration: 9.081s, episode steps: 203, steps per second: 22, episode reward: 97.400, mean reward: 0.480 [-1.000, 1.000], mean action: 3.049 [0.000, 6.000], mean observation: 171.560 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   28756/2000000: episode: 143, duration: 8.794s, episode steps: 201, steps per second: 23, episode reward: 84.400, mean reward: 0.420 [-1.000, 1.000], mean action: 2.925 [0.000, 6.000], mean observation: 171.488 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   28916/2000000: episode: 144, duration: 6.645s, episode steps: 160, steps per second: 24, episode reward: 59.300, mean reward: 0.371 [-1.000, 0.500], mean action: 2.706 [0.000, 6.000], mean observation: 172.288 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   29070/2000000: episode: 145, duration: 7.492s, episode steps: 154, steps per second: 21, episode reward: 78.200, mean reward: 0.508 [-1.000, 1.000], mean action: 3.299 [0.000, 6.000], mean observation: 173.013 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   29273/2000000: episode: 146, duration: 9.128s, episode steps: 203, steps per second: 22, episode reward: 125.500, mean reward: 0.618 [-1.000, 1.000], mean action: 2.724 [0.000, 6.000], mean observation: 171.419 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   29460/2000000: episode: 147, duration: 7.936s, episode steps: 187, steps per second: 24, episode reward: 73.200, mean reward: 0.391 [-1.000, 0.500], mean action: 2.877 [0.000, 6.000], mean observation: 171.513 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   29673/2000000: episode: 148, duration: 9.724s, episode steps: 213, steps per second: 22, episode reward: 139.300, mean reward: 0.654 [-1.000, 1.000], mean action: 3.141 [0.000, 6.000], mean observation: 171.604 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   29880/2000000: episode: 149, duration: 9.240s, episode steps: 207, steps per second: 22, episode reward: 81.200, mean reward: 0.392 [-1.000, 0.500], mean action: 3.227 [0.000, 6.000], mean observation: 171.690 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   30047/2000000: episode: 150, duration: 8.162s, episode steps: 167, steps per second: 20, episode reward: 87.400, mean reward: 0.523 [-1.000, 1.000], mean action: 3.210 [0.000, 6.000], mean observation: 173.341 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   30257/2000000: episode: 151, duration: 9.461s, episode steps: 210, steps per second: 22, episode reward: 155.700, mean reward: 0.741 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 171.120 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   30472/2000000: episode: 152, duration: 9.958s, episode steps: 215, steps per second: 22, episode reward: 144.500, mean reward: 0.672 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 171.977 [19.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   30706/2000000: episode: 153, duration: 11.061s, episode steps: 234, steps per second: 21, episode reward: 159.200, mean reward: 0.680 [-1.000, 1.000], mean action: 3.231 [0.000, 6.000], mean observation: 172.699 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   30906/2000000: episode: 154, duration: 8.796s, episode steps: 200, steps per second: 23, episode reward: 98.300, mean reward: 0.491 [-1.000, 1.000], mean action: 2.900 [0.000, 6.000], mean observation: 172.090 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   31109/2000000: episode: 155, duration: 8.959s, episode steps: 203, steps per second: 23, episode reward: 87.400, mean reward: 0.431 [-1.000, 1.000], mean action: 3.030 [0.000, 6.000], mean observation: 171.984 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   31335/2000000: episode: 156, duration: 10.458s, episode steps: 226, steps per second: 22, episode reward: 143.300, mean reward: 0.634 [-1.000, 1.000], mean action: 2.925 [0.000, 6.000], mean observation: 172.888 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   31531/2000000: episode: 157, duration: 8.391s, episode steps: 196, steps per second: 23, episode reward: 77.700, mean reward: 0.396 [-1.000, 0.500], mean action: 2.796 [0.000, 6.000], mean observation: 172.350 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   31678/2000000: episode: 158, duration: 7.188s, episode steps: 147, steps per second: 20, episode reward: 84.100, mean reward: 0.572 [-1.000, 1.000], mean action: 3.286 [0.000, 6.000], mean observation: 172.763 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   31889/2000000: episode: 159, duration: 9.664s, episode steps: 211, steps per second: 22, episode reward: 111.700, mean reward: 0.529 [-1.000, 1.000], mean action: 3.024 [0.000, 6.000], mean observation: 172.385 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   32103/2000000: episode: 160, duration: 9.528s, episode steps: 214, steps per second: 22, episode reward: 80.700, mean reward: 0.377 [-1.000, 0.500], mean action: 3.262 [0.000, 6.000], mean observation: 172.652 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32314/2000000: episode: 161, duration: 9.654s, episode steps: 211, steps per second: 22, episode reward: 117.400, mean reward: 0.556 [-1.000, 1.000], mean action: 3.052 [0.000, 6.000], mean observation: 172.373 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   32554/2000000: episode: 162, duration: 11.241s, episode steps: 240, steps per second: 21, episode reward: 173.400, mean reward: 0.722 [-1.000, 1.000], mean action: 3.075 [0.000, 6.000], mean observation: 172.816 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   32764/2000000: episode: 163, duration: 9.404s, episode steps: 210, steps per second: 22, episode reward: 96.700, mean reward: 0.460 [-1.000, 0.500], mean action: 3.019 [0.000, 6.000], mean observation: 172.997 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   32953/2000000: episode: 164, duration: 8.097s, episode steps: 189, steps per second: 23, episode reward: 78.200, mean reward: 0.414 [-1.000, 0.500], mean action: 3.122 [0.000, 6.000], mean observation: 172.263 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   33167/2000000: episode: 165, duration: 9.736s, episode steps: 214, steps per second: 22, episode reward: 156.700, mean reward: 0.732 [-1.000, 1.000], mean action: 2.986 [0.000, 6.000], mean observation: 172.309 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   33378/2000000: episode: 166, duration: 9.567s, episode steps: 211, steps per second: 22, episode reward: 96.000, mean reward: 0.455 [-1.000, 0.500], mean action: 3.033 [0.000, 6.000], mean observation: 173.068 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   33558/2000000: episode: 167, duration: 7.604s, episode steps: 180, steps per second: 24, episode reward: 70.500, mean reward: 0.392 [-1.000, 0.500], mean action: 2.861 [0.000, 6.000], mean observation: 172.901 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   33759/2000000: episode: 168, duration: 8.803s, episode steps: 201, steps per second: 23, episode reward: 83.400, mean reward: 0.415 [-1.000, 0.500], mean action: 2.995 [0.000, 6.000], mean observation: 172.856 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   33954/2000000: episode: 169, duration: 8.468s, episode steps: 195, steps per second: 23, episode reward: 81.300, mean reward: 0.417 [-1.000, 1.000], mean action: 2.979 [0.000, 6.000], mean observation: 172.595 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   34186/2000000: episode: 170, duration: 10.949s, episode steps: 232, steps per second: 21, episode reward: 119.900, mean reward: 0.517 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 173.380 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   34412/2000000: episode: 171, duration: 10.507s, episode steps: 226, steps per second: 22, episode reward: 141.800, mean reward: 0.627 [-1.000, 1.000], mean action: 3.124 [0.000, 6.000], mean observation: 173.256 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   34613/2000000: episode: 172, duration: 8.719s, episode steps: 201, steps per second: 23, episode reward: 80.200, mean reward: 0.399 [-1.000, 0.500], mean action: 2.985 [0.000, 6.000], mean observation: 172.500 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   34813/2000000: episode: 173, duration: 8.724s, episode steps: 200, steps per second: 23, episode reward: 82.500, mean reward: 0.412 [-1.000, 0.500], mean action: 3.045 [0.000, 6.000], mean observation: 172.544 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35015/2000000: episode: 174, duration: 8.756s, episode steps: 202, steps per second: 23, episode reward: 80.300, mean reward: 0.398 [-1.000, 0.500], mean action: 3.084 [0.000, 6.000], mean observation: 172.548 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35218/2000000: episode: 175, duration: 8.885s, episode steps: 203, steps per second: 23, episode reward: 91.600, mean reward: 0.451 [-1.000, 1.000], mean action: 2.936 [0.000, 6.000], mean observation: 172.361 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35414/2000000: episode: 176, duration: 8.346s, episode steps: 196, steps per second: 23, episode reward: 78.100, mean reward: 0.398 [-1.000, 0.500], mean action: 3.066 [0.000, 6.000], mean observation: 172.388 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35624/2000000: episode: 177, duration: 9.504s, episode steps: 210, steps per second: 22, episode reward: 135.200, mean reward: 0.644 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 172.162 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35793/2000000: episode: 178, duration: 7.107s, episode steps: 169, steps per second: 24, episode reward: 64.600, mean reward: 0.382 [-1.000, 0.500], mean action: 2.751 [0.000, 6.000], mean observation: 172.736 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   35996/2000000: episode: 179, duration: 9.108s, episode steps: 203, steps per second: 22, episode reward: 138.400, mean reward: 0.682 [-1.000, 1.000], mean action: 2.956 [0.000, 6.000], mean observation: 171.603 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   36198/2000000: episode: 180, duration: 8.903s, episode steps: 202, steps per second: 23, episode reward: 93.800, mean reward: 0.464 [-1.000, 1.000], mean action: 2.851 [0.000, 6.000], mean observation: 172.407 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   36407/2000000: episode: 181, duration: 9.711s, episode steps: 209, steps per second: 22, episode reward: 121.400, mean reward: 0.581 [-1.000, 1.000], mean action: 2.976 [0.000, 6.000], mean observation: 172.449 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   36607/2000000: episode: 182, duration: 8.755s, episode steps: 200, steps per second: 23, episode reward: 90.200, mean reward: 0.451 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 171.858 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   36783/2000000: episode: 183, duration: 7.480s, episode steps: 176, steps per second: 24, episode reward: 72.500, mean reward: 0.412 [-1.000, 0.500], mean action: 2.972 [0.000, 6.000], mean observation: 172.268 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   36979/2000000: episode: 184, duration: 8.421s, episode steps: 196, steps per second: 23, episode reward: 90.800, mean reward: 0.463 [-1.000, 1.000], mean action: 2.893 [0.000, 6.000], mean observation: 172.028 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   37189/2000000: episode: 185, duration: 9.598s, episode steps: 210, steps per second: 22, episode reward: 115.900, mean reward: 0.552 [-1.000, 1.000], mean action: 3.067 [0.000, 6.000], mean observation: 172.427 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   37350/2000000: episode: 186, duration: 6.753s, episode steps: 161, steps per second: 24, episode reward: 59.000, mean reward: 0.366 [-1.000, 0.500], mean action: 2.932 [0.000, 6.000], mean observation: 172.672 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   37561/2000000: episode: 187, duration: 9.636s, episode steps: 211, steps per second: 22, episode reward: 122.300, mean reward: 0.580 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 172.451 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   37811/2000000: episode: 188, duration: 11.818s, episode steps: 250, steps per second: 21, episode reward: 153.800, mean reward: 0.615 [-1.000, 1.000], mean action: 3.220 [0.000, 6.000], mean observation: 172.865 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   38056/2000000: episode: 189, duration: 11.569s, episode steps: 245, steps per second: 21, episode reward: 121.900, mean reward: 0.498 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 173.257 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38239/2000000: episode: 190, duration: 7.835s, episode steps: 183, steps per second: 23, episode reward: 76.400, mean reward: 0.417 [-1.000, 0.500], mean action: 2.863 [0.000, 6.000], mean observation: 171.720 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   38453/2000000: episode: 191, duration: 9.723s, episode steps: 214, steps per second: 22, episode reward: 124.400, mean reward: 0.581 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 172.619 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   38653/2000000: episode: 192, duration: 8.648s, episode steps: 200, steps per second: 23, episode reward: 87.700, mean reward: 0.438 [-1.000, 1.000], mean action: 2.900 [0.000, 6.000], mean observation: 172.369 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   38864/2000000: episode: 193, duration: 9.705s, episode steps: 211, steps per second: 22, episode reward: 144.900, mean reward: 0.687 [-1.000, 1.000], mean action: 3.033 [0.000, 6.000], mean observation: 172.066 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   39064/2000000: episode: 194, duration: 8.765s, episode steps: 200, steps per second: 23, episode reward: 90.100, mean reward: 0.450 [-1.000, 1.000], mean action: 2.930 [0.000, 6.000], mean observation: 172.072 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   39263/2000000: episode: 195, duration: 8.686s, episode steps: 199, steps per second: 23, episode reward: 83.400, mean reward: 0.419 [-1.000, 1.000], mean action: 3.231 [0.000, 6.000], mean observation: 172.039 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   39473/2000000: episode: 196, duration: 9.395s, episode steps: 210, steps per second: 22, episode reward: 122.400, mean reward: 0.583 [-1.000, 1.000], mean action: 3.019 [0.000, 6.000], mean observation: 172.086 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   39704/2000000: episode: 197, duration: 10.771s, episode steps: 231, steps per second: 21, episode reward: 155.200, mean reward: 0.672 [-1.000, 1.000], mean action: 3.069 [0.000, 6.000], mean observation: 173.278 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   39880/2000000: episode: 198, duration: 7.368s, episode steps: 176, steps per second: 24, episode reward: 59.700, mean reward: 0.339 [-1.000, 0.500], mean action: 2.886 [0.000, 6.000], mean observation: 172.396 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   40104/2000000: episode: 199, duration: 10.455s, episode steps: 224, steps per second: 21, episode reward: 150.700, mean reward: 0.673 [-1.000, 1.000], mean action: 3.094 [0.000, 6.000], mean observation: 172.585 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   40313/2000000: episode: 200, duration: 9.540s, episode steps: 209, steps per second: 22, episode reward: 131.900, mean reward: 0.631 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 172.133 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   40526/2000000: episode: 201, duration: 9.810s, episode steps: 213, steps per second: 22, episode reward: 122.400, mean reward: 0.575 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 172.419 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   40727/2000000: episode: 202, duration: 8.748s, episode steps: 201, steps per second: 23, episode reward: 87.000, mean reward: 0.433 [-1.000, 1.000], mean action: 3.045 [0.000, 6.000], mean observation: 171.932 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   40962/2000000: episode: 203, duration: 11.023s, episode steps: 235, steps per second: 21, episode reward: 138.400, mean reward: 0.589 [-1.000, 1.000], mean action: 3.179 [0.000, 6.000], mean observation: 172.812 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   41170/2000000: episode: 204, duration: 9.384s, episode steps: 208, steps per second: 22, episode reward: 132.400, mean reward: 0.637 [-1.000, 1.000], mean action: 3.072 [0.000, 6.000], mean observation: 171.577 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   41387/2000000: episode: 205, duration: 9.829s, episode steps: 217, steps per second: 22, episode reward: 137.100, mean reward: 0.632 [-1.000, 1.000], mean action: 2.963 [0.000, 6.000], mean observation: 171.444 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   41592/2000000: episode: 206, duration: 8.736s, episode steps: 205, steps per second: 23, episode reward: 71.400, mean reward: 0.348 [-1.000, 0.500], mean action: 2.956 [0.000, 6.000], mean observation: 172.400 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   41752/2000000: episode: 207, duration: 7.795s, episode steps: 160, steps per second: 21, episode reward: 81.400, mean reward: 0.509 [-1.000, 1.000], mean action: 3.263 [0.000, 6.000], mean observation: 173.440 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   41926/2000000: episode: 208, duration: 7.405s, episode steps: 174, steps per second: 23, episode reward: 67.100, mean reward: 0.386 [-1.000, 0.500], mean action: 2.822 [0.000, 6.000], mean observation: 171.752 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   42151/2000000: episode: 209, duration: 10.447s, episode steps: 225, steps per second: 22, episode reward: 146.200, mean reward: 0.650 [-1.000, 1.000], mean action: 3.107 [0.000, 6.000], mean observation: 172.819 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   42412/2000000: episode: 210, duration: 12.516s, episode steps: 261, steps per second: 21, episode reward: 155.200, mean reward: 0.595 [-1.000, 1.000], mean action: 3.241 [0.000, 6.000], mean observation: 172.565 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   42613/2000000: episode: 211, duration: 8.857s, episode steps: 201, steps per second: 23, episode reward: 83.000, mean reward: 0.413 [-1.000, 0.500], mean action: 3.050 [0.000, 6.000], mean observation: 171.942 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   42837/2000000: episode: 212, duration: 10.334s, episode steps: 224, steps per second: 22, episode reward: 165.800, mean reward: 0.740 [-1.000, 1.000], mean action: 2.973 [0.000, 6.000], mean observation: 172.539 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   43041/2000000: episode: 213, duration: 9.087s, episode steps: 204, steps per second: 22, episode reward: 116.000, mean reward: 0.569 [-1.000, 1.000], mean action: 2.941 [0.000, 6.000], mean observation: 171.352 [22.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   43268/2000000: episode: 214, duration: 10.597s, episode steps: 227, steps per second: 21, episode reward: 157.100, mean reward: 0.692 [-1.000, 1.000], mean action: 3.159 [0.000, 6.000], mean observation: 172.551 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   43485/2000000: episode: 215, duration: 10.134s, episode steps: 217, steps per second: 21, episode reward: 147.400, mean reward: 0.679 [-1.000, 1.000], mean action: 3.101 [0.000, 6.000], mean observation: 172.069 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   43708/2000000: episode: 216, duration: 10.518s, episode steps: 223, steps per second: 21, episode reward: 142.800, mean reward: 0.640 [-1.000, 1.000], mean action: 3.179 [0.000, 6.000], mean observation: 172.290 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   43911/2000000: episode: 217, duration: 9.329s, episode steps: 203, steps per second: 22, episode reward: 139.700, mean reward: 0.688 [-1.000, 1.000], mean action: 2.956 [0.000, 6.000], mean observation: 170.930 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44114/2000000: episode: 218, duration: 9.128s, episode steps: 203, steps per second: 22, episode reward: 117.200, mean reward: 0.577 [-1.000, 1.000], mean action: 3.103 [0.000, 6.000], mean observation: 170.876 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   44339/2000000: episode: 219, duration: 10.498s, episode steps: 225, steps per second: 21, episode reward: 160.400, mean reward: 0.713 [-1.000, 1.000], mean action: 3.164 [0.000, 6.000], mean observation: 172.074 [21.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   44521/2000000: episode: 220, duration: 7.707s, episode steps: 182, steps per second: 24, episode reward: 59.100, mean reward: 0.325 [-1.000, 0.500], mean action: 3.132 [0.000, 6.000], mean observation: 171.685 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   44750/2000000: episode: 221, duration: 10.686s, episode steps: 229, steps per second: 21, episode reward: 148.600, mean reward: 0.649 [-1.000, 1.000], mean action: 3.153 [0.000, 6.000], mean observation: 172.335 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   44946/2000000: episode: 222, duration: 8.391s, episode steps: 196, steps per second: 23, episode reward: 77.300, mean reward: 0.394 [-1.000, 0.500], mean action: 2.990 [0.000, 6.000], mean observation: 171.693 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   45153/2000000: episode: 223, duration: 9.202s, episode steps: 207, steps per second: 22, episode reward: 122.100, mean reward: 0.590 [-1.000, 1.000], mean action: 2.908 [0.000, 6.000], mean observation: 171.030 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   45385/2000000: episode: 224, duration: 10.786s, episode steps: 232, steps per second: 22, episode reward: 115.700, mean reward: 0.499 [-1.000, 1.000], mean action: 3.091 [0.000, 6.000], mean observation: 172.301 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   45587/2000000: episode: 225, duration: 8.873s, episode steps: 202, steps per second: 23, episode reward: 84.700, mean reward: 0.419 [-1.000, 0.500], mean action: 3.153 [0.000, 6.000], mean observation: 171.253 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   45791/2000000: episode: 226, duration: 9.071s, episode steps: 204, steps per second: 22, episode reward: 107.600, mean reward: 0.527 [-1.000, 1.000], mean action: 3.010 [0.000, 6.000], mean observation: 171.159 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46018/2000000: episode: 227, duration: 10.553s, episode steps: 227, steps per second: 22, episode reward: 148.200, mean reward: 0.653 [-1.000, 1.000], mean action: 3.057 [0.000, 6.000], mean observation: 172.104 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46229/2000000: episode: 228, duration: 9.521s, episode steps: 211, steps per second: 22, episode reward: 151.700, mean reward: 0.719 [-1.000, 1.000], mean action: 3.095 [0.000, 6.000], mean observation: 170.784 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46472/2000000: episode: 229, duration: 11.496s, episode steps: 243, steps per second: 21, episode reward: 122.800, mean reward: 0.505 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 172.381 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46677/2000000: episode: 230, duration: 9.077s, episode steps: 205, steps per second: 23, episode reward: 88.300, mean reward: 0.431 [-1.000, 1.000], mean action: 3.161 [0.000, 6.000], mean observation: 170.813 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46842/2000000: episode: 231, duration: 6.909s, episode steps: 165, steps per second: 24, episode reward: 62.600, mean reward: 0.379 [-1.000, 0.500], mean action: 2.679 [0.000, 6.000], mean observation: 171.913 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   46994/2000000: episode: 232, duration: 7.419s, episode steps: 152, steps per second: 20, episode reward: 84.600, mean reward: 0.557 [-1.000, 1.000], mean action: 3.480 [0.000, 6.000], mean observation: 172.272 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   47200/2000000: episode: 233, duration: 9.251s, episode steps: 206, steps per second: 22, episode reward: 82.300, mean reward: 0.400 [-1.000, 0.500], mean action: 3.155 [0.000, 6.000], mean observation: 171.949 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   47418/2000000: episode: 234, duration: 10.118s, episode steps: 218, steps per second: 22, episode reward: 150.500, mean reward: 0.690 [-1.000, 1.000], mean action: 3.073 [0.000, 6.000], mean observation: 171.638 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   47629/2000000: episode: 235, duration: 9.600s, episode steps: 211, steps per second: 22, episode reward: 144.800, mean reward: 0.686 [-1.000, 1.000], mean action: 3.090 [0.000, 6.000], mean observation: 171.161 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   47807/2000000: episode: 236, duration: 7.603s, episode steps: 178, steps per second: 23, episode reward: 73.100, mean reward: 0.411 [-1.000, 0.500], mean action: 2.876 [0.000, 6.000], mean observation: 171.465 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   48009/2000000: episode: 237, duration: 8.848s, episode steps: 202, steps per second: 23, episode reward: 84.700, mean reward: 0.419 [-1.000, 0.500], mean action: 2.861 [0.000, 6.000], mean observation: 171.463 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   48228/2000000: episode: 238, duration: 10.071s, episode steps: 219, steps per second: 22, episode reward: 148.900, mean reward: 0.680 [-1.000, 1.000], mean action: 3.014 [0.000, 6.000], mean observation: 171.621 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   48432/2000000: episode: 239, duration: 9.029s, episode steps: 204, steps per second: 23, episode reward: 84.500, mean reward: 0.414 [-1.000, 0.500], mean action: 2.882 [0.000, 6.000], mean observation: 171.845 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   48651/2000000: episode: 240, duration: 9.947s, episode steps: 219, steps per second: 22, episode reward: 150.400, mean reward: 0.687 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 171.271 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   48826/2000000: episode: 241, duration: 7.301s, episode steps: 175, steps per second: 24, episode reward: 62.400, mean reward: 0.357 [-1.000, 0.500], mean action: 2.931 [0.000, 6.000], mean observation: 172.125 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   49025/2000000: episode: 242, duration: 8.709s, episode steps: 199, steps per second: 23, episode reward: 89.000, mean reward: 0.447 [-1.000, 1.000], mean action: 2.784 [0.000, 6.000], mean observation: 171.804 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   49228/2000000: episode: 243, duration: 8.899s, episode steps: 203, steps per second: 23, episode reward: 89.200, mean reward: 0.439 [-1.000, 1.000], mean action: 3.034 [0.000, 6.000], mean observation: 171.827 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   49462/2000000: episode: 244, duration: 11.006s, episode steps: 234, steps per second: 21, episode reward: 150.700, mean reward: 0.644 [-1.000, 1.000], mean action: 3.282 [0.000, 6.000], mean observation: 172.408 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   49670/2000000: episode: 245, duration: 9.389s, episode steps: 208, steps per second: 22, episode reward: 147.100, mean reward: 0.707 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 171.444 [23.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   49874/2000000: episode: 246, duration: 9.184s, episode steps: 204, steps per second: 22, episode reward: 104.100, mean reward: 0.510 [-1.000, 1.000], mean action: 2.877 [0.000, 6.000], mean observation: 171.890 [24.000, 255.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "   50023/2000000: episode: 247, duration: 8.106s, episode steps: 149, steps per second: 18, episode reward: 75.700, mean reward: 0.508 [-1.000, 1.000], mean action: 3.007 [0.000, 6.000], mean observation: 172.351 [24.000, 255.000], loss: 0.150327, mean_absolute_error: 0.181348, mean_q: 0.211592, mean_eps: 0.954989\n",
      "   50210/2000000: episode: 248, duration: 9.323s, episode steps: 187, steps per second: 20, episode reward: 130.000, mean reward: 0.695 [-1.000, 1.000], mean action: 2.909 [0.000, 6.000], mean observation: 171.113 [23.000, 255.000], loss: 0.046410, mean_absolute_error: 0.510706, mean_q: 0.656141, mean_eps: 0.954896\n",
      "   50399/2000000: episode: 249, duration: 9.451s, episode steps: 189, steps per second: 20, episode reward: 122.200, mean reward: 0.647 [-1.000, 1.000], mean action: 3.053 [0.000, 6.000], mean observation: 171.401 [23.000, 255.000], loss: 0.048807, mean_absolute_error: 0.455513, mean_q: 0.559109, mean_eps: 0.954726\n",
      "   50578/2000000: episode: 250, duration: 8.757s, episode steps: 179, steps per second: 20, episode reward: 72.400, mean reward: 0.404 [-1.000, 0.500], mean action: 3.011 [0.000, 6.000], mean observation: 172.759 [24.000, 255.000], loss: 0.050763, mean_absolute_error: 0.464311, mean_q: 0.585579, mean_eps: 0.954561\n",
      "   50812/2000000: episode: 251, duration: 12.109s, episode steps: 234, steps per second: 19, episode reward: 119.300, mean reward: 0.510 [-1.000, 1.000], mean action: 3.137 [0.000, 6.000], mean observation: 173.082 [24.000, 255.000], loss: 0.037626, mean_absolute_error: 0.473969, mean_q: 0.590443, mean_eps: 0.954375\n",
      "   50972/2000000: episode: 252, duration: 7.668s, episode steps: 160, steps per second: 21, episode reward: 66.100, mean reward: 0.413 [-1.000, 0.500], mean action: 2.975 [0.000, 6.000], mean observation: 172.380 [23.000, 255.000], loss: 0.030827, mean_absolute_error: 0.486619, mean_q: 0.592212, mean_eps: 0.954199\n",
      "   51156/2000000: episode: 253, duration: 9.058s, episode steps: 184, steps per second: 20, episode reward: 107.100, mean reward: 0.582 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 171.919 [24.000, 255.000], loss: 0.033003, mean_absolute_error: 0.453955, mean_q: 0.574673, mean_eps: 0.954044\n",
      "   51345/2000000: episode: 254, duration: 9.394s, episode steps: 189, steps per second: 20, episode reward: 131.100, mean reward: 0.694 [-1.000, 1.000], mean action: 3.132 [0.000, 6.000], mean observation: 171.615 [24.000, 255.000], loss: 0.026693, mean_absolute_error: 0.455529, mean_q: 0.577768, mean_eps: 0.953875\n",
      "   51519/2000000: episode: 255, duration: 8.357s, episode steps: 174, steps per second: 21, episode reward: 70.700, mean reward: 0.406 [-1.000, 0.500], mean action: 2.902 [0.000, 6.000], mean observation: 172.403 [25.000, 255.000], loss: 0.022252, mean_absolute_error: 0.448983, mean_q: 0.574222, mean_eps: 0.953711\n",
      "   51710/2000000: episode: 256, duration: 9.593s, episode steps: 191, steps per second: 20, episode reward: 142.500, mean reward: 0.746 [-1.000, 1.000], mean action: 3.079 [0.000, 6.000], mean observation: 172.076 [23.000, 255.000], loss: 0.026892, mean_absolute_error: 0.464262, mean_q: 0.590008, mean_eps: 0.953547\n",
      "   51871/2000000: episode: 257, duration: 7.693s, episode steps: 161, steps per second: 21, episode reward: 67.000, mean reward: 0.416 [-1.000, 0.500], mean action: 2.851 [0.000, 6.000], mean observation: 172.383 [24.000, 255.000], loss: 0.028978, mean_absolute_error: 0.507344, mean_q: 0.633201, mean_eps: 0.953389\n",
      "   52058/2000000: episode: 258, duration: 9.242s, episode steps: 187, steps per second: 20, episode reward: 83.600, mean reward: 0.447 [-1.000, 0.500], mean action: 2.941 [0.000, 6.000], mean observation: 172.381 [24.000, 255.000], loss: 0.022975, mean_absolute_error: 0.431253, mean_q: 0.540116, mean_eps: 0.953232\n",
      "   52282/2000000: episode: 259, duration: 11.703s, episode steps: 224, steps per second: 19, episode reward: 111.900, mean reward: 0.500 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 173.191 [23.000, 255.000], loss: 0.025217, mean_absolute_error: 0.461977, mean_q: 0.567221, mean_eps: 0.953047\n",
      "   52423/2000000: episode: 260, duration: 6.655s, episode steps: 141, steps per second: 21, episode reward: 52.600, mean reward: 0.373 [-1.000, 0.500], mean action: 2.773 [0.000, 6.000], mean observation: 172.890 [23.000, 255.000], loss: 0.019220, mean_absolute_error: 0.472120, mean_q: 0.585537, mean_eps: 0.952883\n",
      "   52595/2000000: episode: 261, duration: 8.315s, episode steps: 172, steps per second: 21, episode reward: 71.700, mean reward: 0.417 [-1.000, 0.500], mean action: 2.907 [0.000, 6.000], mean observation: 172.441 [24.000, 255.000], loss: 0.020192, mean_absolute_error: 0.491752, mean_q: 0.611247, mean_eps: 0.952743\n",
      "   52777/2000000: episode: 262, duration: 8.996s, episode steps: 182, steps per second: 20, episode reward: 82.100, mean reward: 0.451 [-1.000, 1.000], mean action: 3.121 [0.000, 6.000], mean observation: 172.216 [23.000, 255.000], loss: 0.021443, mean_absolute_error: 0.483005, mean_q: 0.594395, mean_eps: 0.952583\n",
      "   52983/2000000: episode: 263, duration: 10.563s, episode steps: 206, steps per second: 20, episode reward: 131.600, mean reward: 0.639 [-1.000, 1.000], mean action: 3.102 [0.000, 6.000], mean observation: 173.159 [23.000, 255.000], loss: 0.018653, mean_absolute_error: 0.455745, mean_q: 0.560807, mean_eps: 0.952408\n",
      "   53195/2000000: episode: 264, duration: 10.913s, episode steps: 212, steps per second: 19, episode reward: 141.500, mean reward: 0.667 [-1.000, 1.000], mean action: 2.962 [0.000, 6.000], mean observation: 173.308 [24.000, 255.000], loss: 0.018460, mean_absolute_error: 0.487878, mean_q: 0.588896, mean_eps: 0.952221\n",
      "   53416/2000000: episode: 265, duration: 11.341s, episode steps: 221, steps per second: 19, episode reward: 136.000, mean reward: 0.615 [-1.000, 1.000], mean action: 3.271 [0.000, 6.000], mean observation: 173.076 [23.000, 255.000], loss: 0.016818, mean_absolute_error: 0.469244, mean_q: 0.578394, mean_eps: 0.952026\n",
      "   53587/2000000: episode: 266, duration: 8.280s, episode steps: 171, steps per second: 21, episode reward: 70.800, mean reward: 0.414 [-1.000, 0.500], mean action: 3.099 [0.000, 6.000], mean observation: 172.301 [23.000, 255.000], loss: 0.018044, mean_absolute_error: 0.469694, mean_q: 0.570612, mean_eps: 0.951850\n",
      "   53741/2000000: episode: 267, duration: 7.314s, episode steps: 154, steps per second: 21, episode reward: 59.500, mean reward: 0.386 [-1.000, 0.500], mean action: 2.812 [0.000, 6.000], mean observation: 172.581 [25.000, 255.000], loss: 0.019622, mean_absolute_error: 0.483800, mean_q: 0.602266, mean_eps: 0.951702\n",
      "   53922/2000000: episode: 268, duration: 8.911s, episode steps: 181, steps per second: 20, episode reward: 71.500, mean reward: 0.395 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 172.318 [24.000, 255.000], loss: 0.016073, mean_absolute_error: 0.432445, mean_q: 0.528590, mean_eps: 0.951551\n",
      "   54073/2000000: episode: 269, duration: 7.253s, episode steps: 151, steps per second: 21, episode reward: 64.000, mean reward: 0.424 [-1.000, 0.500], mean action: 2.834 [0.000, 6.000], mean observation: 172.294 [23.000, 255.000], loss: 0.016433, mean_absolute_error: 0.448424, mean_q: 0.546383, mean_eps: 0.951402\n",
      "   54242/2000000: episode: 270, duration: 8.089s, episode steps: 169, steps per second: 21, episode reward: 61.400, mean reward: 0.363 [-1.000, 0.500], mean action: 2.953 [0.000, 6.000], mean observation: 172.679 [23.000, 255.000], loss: 0.016270, mean_absolute_error: 0.491960, mean_q: 0.604208, mean_eps: 0.951258\n",
      "   54425/2000000: episode: 271, duration: 8.809s, episode steps: 183, steps per second: 21, episode reward: 72.800, mean reward: 0.398 [-1.000, 0.500], mean action: 3.098 [0.000, 6.000], mean observation: 172.611 [24.000, 255.000], loss: 0.013923, mean_absolute_error: 0.473321, mean_q: 0.574173, mean_eps: 0.951099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54613/2000000: episode: 272, duration: 9.401s, episode steps: 188, steps per second: 20, episode reward: 86.100, mean reward: 0.458 [-1.000, 0.500], mean action: 2.989 [0.000, 6.000], mean observation: 172.783 [24.000, 255.000], loss: 0.014972, mean_absolute_error: 0.460921, mean_q: 0.571793, mean_eps: 0.950932\n",
      "   54794/2000000: episode: 273, duration: 8.927s, episode steps: 181, steps per second: 20, episode reward: 79.700, mean reward: 0.440 [-1.000, 1.000], mean action: 2.989 [0.000, 6.000], mean observation: 172.170 [24.000, 255.000], loss: 0.015722, mean_absolute_error: 0.445454, mean_q: 0.536109, mean_eps: 0.950766\n",
      "   54977/2000000: episode: 274, duration: 8.919s, episode steps: 183, steps per second: 21, episode reward: 55.600, mean reward: 0.304 [-1.000, 0.500], mean action: 2.962 [0.000, 6.000], mean observation: 173.008 [24.000, 255.000], loss: 0.014787, mean_absolute_error: 0.488343, mean_q: 0.590598, mean_eps: 0.950603\n",
      "   55140/2000000: episode: 275, duration: 7.933s, episode steps: 163, steps per second: 21, episode reward: 77.700, mean reward: 0.477 [-1.000, 1.000], mean action: 2.840 [0.000, 6.000], mean observation: 171.914 [24.000, 255.000], loss: 0.015701, mean_absolute_error: 0.469300, mean_q: 0.561419, mean_eps: 0.950448\n",
      "   55326/2000000: episode: 276, duration: 9.309s, episode steps: 186, steps per second: 20, episode reward: 129.500, mean reward: 0.696 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 171.546 [23.000, 255.000], loss: 0.014581, mean_absolute_error: 0.464491, mean_q: 0.567589, mean_eps: 0.950291\n",
      "   55499/2000000: episode: 277, duration: 8.380s, episode steps: 173, steps per second: 21, episode reward: 67.800, mean reward: 0.392 [-1.000, 0.500], mean action: 2.988 [0.000, 6.000], mean observation: 172.574 [23.000, 255.000], loss: 0.014121, mean_absolute_error: 0.408989, mean_q: 0.494174, mean_eps: 0.950129\n",
      "   55659/2000000: episode: 278, duration: 7.794s, episode steps: 160, steps per second: 21, episode reward: 68.500, mean reward: 0.428 [-1.000, 0.500], mean action: 2.700 [0.000, 6.000], mean observation: 171.647 [24.000, 255.000], loss: 0.013027, mean_absolute_error: 0.437884, mean_q: 0.529887, mean_eps: 0.949980\n",
      "   55811/2000000: episode: 279, duration: 7.414s, episode steps: 152, steps per second: 21, episode reward: 64.900, mean reward: 0.427 [-1.000, 0.500], mean action: 2.750 [0.000, 6.000], mean observation: 172.196 [23.000, 255.000], loss: 0.014033, mean_absolute_error: 0.450560, mean_q: 0.533832, mean_eps: 0.949839\n",
      "   55981/2000000: episode: 280, duration: 8.252s, episode steps: 170, steps per second: 21, episode reward: 67.900, mean reward: 0.399 [-1.000, 0.500], mean action: 2.965 [0.000, 6.000], mean observation: 172.313 [22.000, 255.000], loss: 0.014168, mean_absolute_error: 0.473428, mean_q: 0.568164, mean_eps: 0.949694\n",
      "   56176/2000000: episode: 281, duration: 9.936s, episode steps: 195, steps per second: 20, episode reward: 125.800, mean reward: 0.645 [-1.000, 1.000], mean action: 3.082 [0.000, 6.000], mean observation: 172.119 [25.000, 255.000], loss: 0.013895, mean_absolute_error: 0.476563, mean_q: 0.581839, mean_eps: 0.949530\n",
      "   56386/2000000: episode: 282, duration: 10.687s, episode steps: 210, steps per second: 20, episode reward: 137.500, mean reward: 0.655 [-1.000, 1.000], mean action: 3.219 [0.000, 6.000], mean observation: 173.135 [23.000, 255.000], loss: 0.014568, mean_absolute_error: 0.467911, mean_q: 0.565778, mean_eps: 0.949348\n",
      "   56528/2000000: episode: 283, duration: 6.698s, episode steps: 142, steps per second: 21, episode reward: 51.100, mean reward: 0.360 [-1.000, 0.500], mean action: 2.754 [0.000, 6.000], mean observation: 172.353 [23.000, 255.000], loss: 0.012627, mean_absolute_error: 0.481646, mean_q: 0.588224, mean_eps: 0.949190\n",
      "   56754/2000000: episode: 284, duration: 11.670s, episode steps: 226, steps per second: 19, episode reward: 106.700, mean reward: 0.472 [-1.000, 1.000], mean action: 2.982 [0.000, 6.000], mean observation: 173.431 [23.000, 255.000], loss: 0.014304, mean_absolute_error: 0.436000, mean_q: 0.521832, mean_eps: 0.949024\n",
      "   56961/2000000: episode: 285, duration: 10.488s, episode steps: 207, steps per second: 20, episode reward: 126.300, mean reward: 0.610 [-1.000, 1.000], mean action: 3.121 [0.000, 6.000], mean observation: 173.358 [23.000, 255.000], loss: 0.012952, mean_absolute_error: 0.456491, mean_q: 0.554695, mean_eps: 0.948828\n",
      "   57122/2000000: episode: 286, duration: 7.682s, episode steps: 161, steps per second: 21, episode reward: 63.000, mean reward: 0.391 [-1.000, 0.500], mean action: 2.857 [0.000, 6.000], mean observation: 172.085 [24.000, 255.000], loss: 0.011022, mean_absolute_error: 0.476243, mean_q: 0.578774, mean_eps: 0.948662\n",
      "   57342/2000000: episode: 287, duration: 11.349s, episode steps: 220, steps per second: 19, episode reward: 129.200, mean reward: 0.587 [-1.000, 1.000], mean action: 3.009 [0.000, 6.000], mean observation: 172.869 [23.000, 255.000], loss: 0.011256, mean_absolute_error: 0.456415, mean_q: 0.556924, mean_eps: 0.948491\n",
      "   57515/2000000: episode: 288, duration: 8.338s, episode steps: 173, steps per second: 21, episode reward: 107.800, mean reward: 0.623 [-1.000, 1.000], mean action: 2.931 [0.000, 6.000], mean observation: 170.871 [24.000, 255.000], loss: 0.011760, mean_absolute_error: 0.472947, mean_q: 0.576329, mean_eps: 0.948315\n",
      "   57727/2000000: episode: 289, duration: 10.769s, episode steps: 212, steps per second: 20, episode reward: 126.400, mean reward: 0.596 [-1.000, 1.000], mean action: 3.222 [0.000, 6.000], mean observation: 172.659 [23.000, 255.000], loss: 0.011743, mean_absolute_error: 0.465530, mean_q: 0.559040, mean_eps: 0.948142\n",
      "   57938/2000000: episode: 290, duration: 10.772s, episode steps: 211, steps per second: 20, episode reward: 150.100, mean reward: 0.711 [-1.000, 1.000], mean action: 3.114 [0.000, 6.000], mean observation: 172.556 [24.000, 255.000], loss: 0.011303, mean_absolute_error: 0.444749, mean_q: 0.540740, mean_eps: 0.947951\n",
      "   58055/2000000: episode: 291, duration: 6.266s, episode steps: 117, steps per second: 19, episode reward: 68.500, mean reward: 0.585 [-1.000, 1.000], mean action: 3.325 [0.000, 6.000], mean observation: 173.363 [24.000, 255.000], loss: 0.011437, mean_absolute_error: 0.467009, mean_q: 0.566412, mean_eps: 0.947804\n",
      "   58260/2000000: episode: 292, duration: 10.425s, episode steps: 205, steps per second: 20, episode reward: 127.300, mean reward: 0.621 [-1.000, 1.000], mean action: 3.034 [0.000, 6.000], mean observation: 172.727 [25.000, 255.000], loss: 0.013389, mean_absolute_error: 0.466574, mean_q: 0.563048, mean_eps: 0.947660\n",
      "   58481/2000000: episode: 293, duration: 11.442s, episode steps: 221, steps per second: 19, episode reward: 107.700, mean reward: 0.487 [-1.000, 1.000], mean action: 3.199 [0.000, 6.000], mean observation: 172.836 [24.000, 255.000], loss: 0.010531, mean_absolute_error: 0.482168, mean_q: 0.581542, mean_eps: 0.947467\n",
      "   58623/2000000: episode: 294, duration: 6.654s, episode steps: 142, steps per second: 21, episode reward: 51.500, mean reward: 0.363 [-1.000, 0.500], mean action: 2.754 [0.000, 6.000], mean observation: 172.344 [24.000, 255.000], loss: 0.009745, mean_absolute_error: 0.467594, mean_q: 0.568881, mean_eps: 0.947303\n",
      "   58805/2000000: episode: 295, duration: 8.867s, episode steps: 182, steps per second: 21, episode reward: 73.900, mean reward: 0.406 [-1.000, 0.500], mean action: 3.093 [0.000, 6.000], mean observation: 171.629 [24.000, 255.000], loss: 0.014783, mean_absolute_error: 0.486472, mean_q: 0.591918, mean_eps: 0.947157\n",
      "   58956/2000000: episode: 296, duration: 7.168s, episode steps: 151, steps per second: 21, episode reward: 54.400, mean reward: 0.360 [-1.000, 0.500], mean action: 2.768 [0.000, 6.000], mean observation: 172.161 [25.000, 255.000], loss: 0.014752, mean_absolute_error: 0.446220, mean_q: 0.530405, mean_eps: 0.947008\n",
      "   59172/2000000: episode: 297, duration: 11.083s, episode steps: 216, steps per second: 19, episode reward: 138.600, mean reward: 0.642 [-1.000, 1.000], mean action: 2.949 [0.000, 6.000], mean observation: 172.390 [23.000, 255.000], loss: 0.011570, mean_absolute_error: 0.447359, mean_q: 0.539897, mean_eps: 0.946844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   59360/2000000: episode: 298, duration: 9.237s, episode steps: 188, steps per second: 20, episode reward: 74.900, mean reward: 0.398 [-1.000, 0.500], mean action: 3.043 [0.000, 6.000], mean observation: 171.805 [23.000, 255.000], loss: 0.010908, mean_absolute_error: 0.493544, mean_q: 0.589539, mean_eps: 0.946662\n",
      "   59498/2000000: episode: 299, duration: 6.583s, episode steps: 138, steps per second: 21, episode reward: 51.900, mean reward: 0.376 [-1.000, 0.500], mean action: 2.696 [0.000, 6.000], mean observation: 172.357 [23.000, 255.000], loss: 0.010719, mean_absolute_error: 0.445182, mean_q: 0.536936, mean_eps: 0.946515\n",
      "   59691/2000000: episode: 300, duration: 9.655s, episode steps: 193, steps per second: 20, episode reward: 137.300, mean reward: 0.711 [-1.000, 1.000], mean action: 2.922 [0.000, 6.000], mean observation: 171.572 [24.000, 255.000], loss: 0.012662, mean_absolute_error: 0.454572, mean_q: 0.545243, mean_eps: 0.946365\n",
      "   59834/2000000: episode: 301, duration: 6.791s, episode steps: 143, steps per second: 21, episode reward: 55.200, mean reward: 0.386 [-1.000, 0.500], mean action: 2.902 [0.000, 6.000], mean observation: 172.324 [23.000, 255.000], loss: 0.010763, mean_absolute_error: 0.451344, mean_q: 0.546242, mean_eps: 0.946214\n",
      "   59974/2000000: episode: 302, duration: 6.539s, episode steps: 140, steps per second: 21, episode reward: 47.700, mean reward: 0.341 [-1.000, 0.500], mean action: 2.800 [0.000, 6.000], mean observation: 173.175 [23.000, 255.000], loss: 0.012208, mean_absolute_error: 0.463307, mean_q: 0.559618, mean_eps: 0.946086\n",
      "   60180/2000000: episode: 303, duration: 10.607s, episode steps: 206, steps per second: 19, episode reward: 153.000, mean reward: 0.743 [-1.000, 1.000], mean action: 3.165 [0.000, 6.000], mean observation: 172.581 [25.000, 255.000], loss: 0.029071, mean_absolute_error: 0.833734, mean_q: 1.010844, mean_eps: 0.945932\n",
      "   60360/2000000: episode: 304, duration: 8.802s, episode steps: 180, steps per second: 20, episode reward: 95.100, mean reward: 0.528 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 171.835 [21.000, 255.000], loss: 0.010465, mean_absolute_error: 0.898955, mean_q: 1.088772, mean_eps: 0.945759\n",
      "   60592/2000000: episode: 305, duration: 12.084s, episode steps: 232, steps per second: 19, episode reward: 137.400, mean reward: 0.592 [-1.000, 1.000], mean action: 3.047 [0.000, 6.000], mean observation: 172.991 [23.000, 255.000], loss: 0.011732, mean_absolute_error: 0.901621, mean_q: 1.079102, mean_eps: 0.945573\n",
      "   60738/2000000: episode: 306, duration: 6.913s, episode steps: 146, steps per second: 21, episode reward: 53.100, mean reward: 0.364 [-1.000, 0.500], mean action: 2.753 [0.000, 6.000], mean observation: 172.346 [23.000, 255.000], loss: 0.010292, mean_absolute_error: 0.909064, mean_q: 1.084360, mean_eps: 0.945402\n",
      "   60965/2000000: episode: 307, duration: 11.798s, episode steps: 227, steps per second: 19, episode reward: 124.800, mean reward: 0.550 [-1.000, 1.000], mean action: 3.225 [0.000, 6.000], mean observation: 173.072 [24.000, 255.000], loss: 0.011118, mean_absolute_error: 0.903587, mean_q: 1.089652, mean_eps: 0.945233\n",
      "   61147/2000000: episode: 308, duration: 8.900s, episode steps: 182, steps per second: 20, episode reward: 73.900, mean reward: 0.406 [-1.000, 0.500], mean action: 3.099 [0.000, 6.000], mean observation: 172.408 [24.000, 255.000], loss: 0.010382, mean_absolute_error: 0.871454, mean_q: 1.044728, mean_eps: 0.945050\n",
      "   61336/2000000: episode: 309, duration: 9.397s, episode steps: 189, steps per second: 20, episode reward: 120.100, mean reward: 0.635 [-1.000, 1.000], mean action: 3.048 [0.000, 6.000], mean observation: 171.637 [23.000, 255.000], loss: 0.009913, mean_absolute_error: 0.895546, mean_q: 1.068591, mean_eps: 0.944884\n",
      "   61488/2000000: episode: 310, duration: 7.197s, episode steps: 152, steps per second: 21, episode reward: 54.500, mean reward: 0.359 [-1.000, 0.500], mean action: 2.645 [0.000, 6.000], mean observation: 173.149 [23.000, 255.000], loss: 0.010676, mean_absolute_error: 0.892533, mean_q: 1.056365, mean_eps: 0.944731\n",
      "   61648/2000000: episode: 311, duration: 7.746s, episode steps: 160, steps per second: 21, episode reward: 66.500, mean reward: 0.416 [-1.000, 0.500], mean action: 2.744 [0.000, 6.000], mean observation: 172.742 [23.000, 255.000], loss: 0.010235, mean_absolute_error: 0.914815, mean_q: 1.086117, mean_eps: 0.944591\n",
      "   61827/2000000: episode: 312, duration: 8.772s, episode steps: 179, steps per second: 20, episode reward: 105.900, mean reward: 0.592 [-1.000, 1.000], mean action: 2.810 [0.000, 6.000], mean observation: 172.217 [25.000, 255.000], loss: 0.010877, mean_absolute_error: 0.906523, mean_q: 1.088906, mean_eps: 0.944438\n",
      "   61976/2000000: episode: 313, duration: 7.129s, episode steps: 149, steps per second: 21, episode reward: 59.400, mean reward: 0.399 [-1.000, 0.500], mean action: 2.812 [0.000, 6.000], mean observation: 172.647 [23.000, 255.000], loss: 0.010795, mean_absolute_error: 0.882679, mean_q: 1.045943, mean_eps: 0.944290\n",
      "   62174/2000000: episode: 314, duration: 10.031s, episode steps: 198, steps per second: 20, episode reward: 120.500, mean reward: 0.609 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 172.808 [24.000, 255.000], loss: 0.008103, mean_absolute_error: 0.890625, mean_q: 1.063217, mean_eps: 0.944133\n",
      "   62338/2000000: episode: 315, duration: 7.869s, episode steps: 164, steps per second: 21, episode reward: 60.900, mean reward: 0.371 [-1.000, 0.500], mean action: 2.841 [0.000, 6.000], mean observation: 172.802 [23.000, 255.000], loss: 0.010003, mean_absolute_error: 0.892234, mean_q: 1.056951, mean_eps: 0.943970\n",
      "   62547/2000000: episode: 316, duration: 10.701s, episode steps: 209, steps per second: 20, episode reward: 133.000, mean reward: 0.636 [-1.000, 1.000], mean action: 3.177 [0.000, 6.000], mean observation: 173.382 [24.000, 255.000], loss: 0.008380, mean_absolute_error: 0.910724, mean_q: 1.079931, mean_eps: 0.943802\n",
      "   62720/2000000: episode: 317, duration: 8.415s, episode steps: 173, steps per second: 21, episode reward: 72.500, mean reward: 0.419 [-1.000, 1.000], mean action: 2.931 [0.000, 6.000], mean observation: 172.379 [24.000, 255.000], loss: 0.009959, mean_absolute_error: 0.919501, mean_q: 1.090172, mean_eps: 0.943631\n",
      "   62910/2000000: episode: 318, duration: 9.555s, episode steps: 190, steps per second: 20, episode reward: 124.100, mean reward: 0.653 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 172.320 [23.000, 255.000], loss: 0.008744, mean_absolute_error: 0.880572, mean_q: 1.045157, mean_eps: 0.943467\n",
      "   63094/2000000: episode: 319, duration: 9.170s, episode steps: 184, steps per second: 20, episode reward: 107.100, mean reward: 0.582 [-1.000, 1.000], mean action: 2.908 [0.000, 6.000], mean observation: 172.026 [24.000, 255.000], loss: 0.008742, mean_absolute_error: 0.911619, mean_q: 1.081175, mean_eps: 0.943298\n",
      "   63304/2000000: episode: 320, duration: 10.755s, episode steps: 210, steps per second: 20, episode reward: 146.900, mean reward: 0.700 [-1.000, 1.000], mean action: 3.167 [0.000, 6.000], mean observation: 172.944 [24.000, 255.000], loss: 0.009127, mean_absolute_error: 0.904435, mean_q: 1.070635, mean_eps: 0.943122\n",
      "   63487/2000000: episode: 321, duration: 9.048s, episode steps: 183, steps per second: 20, episode reward: 86.400, mean reward: 0.472 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 172.306 [23.000, 255.000], loss: 0.009032, mean_absolute_error: 0.909590, mean_q: 1.085397, mean_eps: 0.942945\n",
      "   63710/2000000: episode: 322, duration: 11.529s, episode steps: 223, steps per second: 19, episode reward: 114.100, mean reward: 0.512 [-1.000, 1.000], mean action: 3.117 [0.000, 6.000], mean observation: 173.390 [24.000, 255.000], loss: 0.009443, mean_absolute_error: 0.892522, mean_q: 1.067477, mean_eps: 0.942762\n",
      "   63918/2000000: episode: 323, duration: 10.678s, episode steps: 208, steps per second: 19, episode reward: 124.600, mean reward: 0.599 [-1.000, 1.000], mean action: 3.087 [0.000, 6.000], mean observation: 172.842 [24.000, 255.000], loss: 0.008096, mean_absolute_error: 0.911229, mean_q: 1.079585, mean_eps: 0.942567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   64055/2000000: episode: 324, duration: 6.403s, episode steps: 137, steps per second: 21, episode reward: 51.000, mean reward: 0.372 [-1.000, 0.500], mean action: 2.752 [0.000, 6.000], mean observation: 172.856 [24.000, 255.000], loss: 0.008219, mean_absolute_error: 0.900894, mean_q: 1.069656, mean_eps: 0.942413\n",
      "   64235/2000000: episode: 325, duration: 8.798s, episode steps: 180, steps per second: 20, episode reward: 110.900, mean reward: 0.616 [-1.000, 1.000], mean action: 2.950 [0.000, 6.000], mean observation: 171.906 [24.000, 255.000], loss: 0.008486, mean_absolute_error: 0.908802, mean_q: 1.081210, mean_eps: 0.942270\n",
      "   64415/2000000: episode: 326, duration: 8.933s, episode steps: 180, steps per second: 20, episode reward: 90.700, mean reward: 0.504 [-1.000, 1.000], mean action: 2.883 [0.000, 6.000], mean observation: 172.283 [24.000, 255.000], loss: 0.008894, mean_absolute_error: 0.893136, mean_q: 1.060574, mean_eps: 0.942108\n",
      "   64604/2000000: episode: 327, duration: 9.474s, episode steps: 189, steps per second: 20, episode reward: 122.100, mean reward: 0.646 [-1.000, 1.000], mean action: 3.190 [0.000, 6.000], mean observation: 171.842 [23.000, 255.000], loss: 0.009042, mean_absolute_error: 0.898661, mean_q: 1.053502, mean_eps: 0.941943\n",
      "   64782/2000000: episode: 328, duration: 8.651s, episode steps: 178, steps per second: 21, episode reward: 74.600, mean reward: 0.419 [-1.000, 1.000], mean action: 2.843 [0.000, 6.000], mean observation: 172.256 [24.000, 255.000], loss: 0.009104, mean_absolute_error: 0.873954, mean_q: 1.026332, mean_eps: 0.941777\n",
      "   64998/2000000: episode: 329, duration: 11.013s, episode steps: 216, steps per second: 20, episode reward: 144.400, mean reward: 0.669 [-1.000, 1.000], mean action: 3.111 [0.000, 6.000], mean observation: 173.069 [22.000, 255.000], loss: 0.007541, mean_absolute_error: 0.891068, mean_q: 1.055158, mean_eps: 0.941599\n",
      "   65135/2000000: episode: 330, duration: 6.469s, episode steps: 137, steps per second: 21, episode reward: 50.200, mean reward: 0.366 [-1.000, 0.500], mean action: 2.715 [0.000, 6.000], mean observation: 172.687 [24.000, 255.000], loss: 0.008324, mean_absolute_error: 0.909442, mean_q: 1.082951, mean_eps: 0.941441\n",
      "   65339/2000000: episode: 331, duration: 10.466s, episode steps: 204, steps per second: 19, episode reward: 147.200, mean reward: 0.722 [-1.000, 1.000], mean action: 3.083 [0.000, 6.000], mean observation: 173.257 [24.000, 255.000], loss: 0.008212, mean_absolute_error: 0.897580, mean_q: 1.067008, mean_eps: 0.941288\n",
      "   65477/2000000: episode: 332, duration: 6.536s, episode steps: 138, steps per second: 21, episode reward: 51.900, mean reward: 0.376 [-1.000, 0.500], mean action: 2.913 [0.000, 6.000], mean observation: 172.798 [24.000, 255.000], loss: 0.008309, mean_absolute_error: 0.893114, mean_q: 1.059213, mean_eps: 0.941133\n",
      "   65659/2000000: episode: 333, duration: 8.972s, episode steps: 182, steps per second: 20, episode reward: 86.300, mean reward: 0.474 [-1.000, 1.000], mean action: 2.962 [0.000, 6.000], mean observation: 172.390 [24.000, 255.000], loss: 0.007719, mean_absolute_error: 0.887213, mean_q: 1.059185, mean_eps: 0.940989\n",
      "   65814/2000000: episode: 334, duration: 7.479s, episode steps: 155, steps per second: 21, episode reward: 66.400, mean reward: 0.428 [-1.000, 0.500], mean action: 2.729 [0.000, 6.000], mean observation: 171.992 [25.000, 255.000], loss: 0.010024, mean_absolute_error: 0.910071, mean_q: 1.073031, mean_eps: 0.940838\n",
      "   65954/2000000: episode: 335, duration: 6.561s, episode steps: 140, steps per second: 21, episode reward: 52.500, mean reward: 0.375 [-1.000, 0.500], mean action: 2.664 [0.000, 6.000], mean observation: 172.595 [23.000, 255.000], loss: 0.009976, mean_absolute_error: 0.897861, mean_q: 1.065831, mean_eps: 0.940704\n",
      "   66134/2000000: episode: 336, duration: 8.823s, episode steps: 180, steps per second: 20, episode reward: 80.200, mean reward: 0.446 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 171.571 [23.000, 255.000], loss: 0.007899, mean_absolute_error: 0.871070, mean_q: 1.035217, mean_eps: 0.940560\n",
      "   66278/2000000: episode: 337, duration: 6.889s, episode steps: 144, steps per second: 21, episode reward: 56.100, mean reward: 0.390 [-1.000, 0.500], mean action: 2.715 [0.000, 6.000], mean observation: 172.068 [23.000, 255.000], loss: 0.007737, mean_absolute_error: 0.879309, mean_q: 1.043829, mean_eps: 0.940415\n",
      "   66425/2000000: episode: 338, duration: 6.937s, episode steps: 147, steps per second: 21, episode reward: 52.000, mean reward: 0.354 [-1.000, 0.500], mean action: 2.701 [0.000, 6.000], mean observation: 172.559 [24.000, 255.000], loss: 0.008450, mean_absolute_error: 0.915491, mean_q: 1.085341, mean_eps: 0.940283\n",
      "   66604/2000000: episode: 339, duration: 8.708s, episode steps: 179, steps per second: 21, episode reward: 71.200, mean reward: 0.398 [-1.000, 0.500], mean action: 2.782 [0.000, 6.000], mean observation: 172.601 [23.000, 255.000], loss: 0.007825, mean_absolute_error: 0.893536, mean_q: 1.055227, mean_eps: 0.940137\n",
      "   66782/2000000: episode: 340, duration: 8.699s, episode steps: 178, steps per second: 20, episode reward: 91.000, mean reward: 0.511 [-1.000, 1.000], mean action: 3.124 [0.000, 6.000], mean observation: 171.889 [24.000, 255.000], loss: 0.007737, mean_absolute_error: 0.895622, mean_q: 1.067057, mean_eps: 0.939977\n",
      "   66931/2000000: episode: 341, duration: 7.116s, episode steps: 149, steps per second: 21, episode reward: 61.800, mean reward: 0.415 [-1.000, 0.500], mean action: 2.812 [0.000, 6.000], mean observation: 171.964 [24.000, 255.000], loss: 0.008634, mean_absolute_error: 0.899575, mean_q: 1.063626, mean_eps: 0.939830\n",
      "   67122/2000000: episode: 342, duration: 9.609s, episode steps: 191, steps per second: 20, episode reward: 140.000, mean reward: 0.733 [-1.000, 1.000], mean action: 3.010 [0.000, 6.000], mean observation: 171.796 [23.000, 255.000], loss: 0.007540, mean_absolute_error: 0.878357, mean_q: 1.037744, mean_eps: 0.939677\n",
      "   67287/2000000: episode: 343, duration: 7.960s, episode steps: 165, steps per second: 21, episode reward: 69.900, mean reward: 0.424 [-1.000, 1.000], mean action: 2.909 [0.000, 6.000], mean observation: 171.685 [24.000, 255.000], loss: 0.006935, mean_absolute_error: 0.891518, mean_q: 1.056659, mean_eps: 0.939516\n",
      "   67502/2000000: episode: 344, duration: 11.076s, episode steps: 215, steps per second: 19, episode reward: 137.100, mean reward: 0.638 [-1.000, 1.000], mean action: 3.037 [0.000, 6.000], mean observation: 173.266 [24.000, 255.000], loss: 0.007111, mean_absolute_error: 0.908894, mean_q: 1.074666, mean_eps: 0.939345\n",
      "   67657/2000000: episode: 345, duration: 7.356s, episode steps: 155, steps per second: 21, episode reward: 58.400, mean reward: 0.377 [-1.000, 0.500], mean action: 2.916 [0.000, 6.000], mean observation: 172.456 [23.000, 255.000], loss: 0.007522, mean_absolute_error: 0.913464, mean_q: 1.074065, mean_eps: 0.939178\n",
      "   67873/2000000: episode: 346, duration: 11.164s, episode steps: 216, steps per second: 19, episode reward: 149.300, mean reward: 0.691 [-1.000, 1.000], mean action: 3.176 [0.000, 6.000], mean observation: 172.861 [21.000, 255.000], loss: 0.008322, mean_absolute_error: 0.902840, mean_q: 1.066404, mean_eps: 0.939011\n",
      "   68051/2000000: episode: 347, duration: 8.644s, episode steps: 178, steps per second: 21, episode reward: 74.700, mean reward: 0.420 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 172.073 [25.000, 255.000], loss: 0.006624, mean_absolute_error: 0.900964, mean_q: 1.076700, mean_eps: 0.938834\n",
      "   68183/2000000: episode: 348, duration: 6.162s, episode steps: 132, steps per second: 21, episode reward: 46.100, mean reward: 0.349 [-1.000, 0.500], mean action: 2.652 [0.000, 6.000], mean observation: 173.072 [24.000, 255.000], loss: 0.009078, mean_absolute_error: 0.892033, mean_q: 1.056752, mean_eps: 0.938696\n",
      "   68351/2000000: episode: 349, duration: 8.054s, episode steps: 168, steps per second: 21, episode reward: 76.700, mean reward: 0.457 [-1.000, 1.000], mean action: 2.970 [0.000, 6.000], mean observation: 171.846 [23.000, 255.000], loss: 0.007178, mean_absolute_error: 0.896813, mean_q: 1.058375, mean_eps: 0.938561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   68494/2000000: episode: 350, duration: 6.666s, episode steps: 143, steps per second: 21, episode reward: 45.600, mean reward: 0.319 [-1.000, 0.500], mean action: 2.671 [0.000, 6.000], mean observation: 172.908 [23.000, 255.000], loss: 0.007048, mean_absolute_error: 0.899343, mean_q: 1.070269, mean_eps: 0.938420\n",
      "   68630/2000000: episode: 351, duration: 6.486s, episode steps: 136, steps per second: 21, episode reward: 53.300, mean reward: 0.392 [-1.000, 0.500], mean action: 2.691 [0.000, 6.000], mean observation: 172.170 [23.000, 255.000], loss: 0.008127, mean_absolute_error: 0.904683, mean_q: 1.067586, mean_eps: 0.938294\n",
      "   68821/2000000: episode: 352, duration: 9.652s, episode steps: 191, steps per second: 20, episode reward: 128.100, mean reward: 0.671 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 171.741 [24.000, 255.000], loss: 0.008356, mean_absolute_error: 0.895456, mean_q: 1.055705, mean_eps: 0.938147\n",
      "   69001/2000000: episode: 353, duration: 8.839s, episode steps: 180, steps per second: 20, episode reward: 98.200, mean reward: 0.546 [-1.000, 1.000], mean action: 2.817 [0.000, 6.000], mean observation: 172.083 [24.000, 255.000], loss: 0.006691, mean_absolute_error: 0.891661, mean_q: 1.063461, mean_eps: 0.937979\n",
      "   69166/2000000: episode: 354, duration: 7.894s, episode steps: 165, steps per second: 21, episode reward: 68.200, mean reward: 0.413 [-1.000, 0.500], mean action: 2.945 [0.000, 6.000], mean observation: 171.927 [24.000, 255.000], loss: 0.007838, mean_absolute_error: 0.894937, mean_q: 1.057371, mean_eps: 0.937824\n",
      "   69323/2000000: episode: 355, duration: 7.507s, episode steps: 157, steps per second: 21, episode reward: 62.200, mean reward: 0.396 [-1.000, 0.500], mean action: 2.917 [0.000, 6.000], mean observation: 171.791 [25.000, 255.000], loss: 0.006669, mean_absolute_error: 0.890056, mean_q: 1.051414, mean_eps: 0.937680\n",
      "   69494/2000000: episode: 356, duration: 8.267s, episode steps: 171, steps per second: 21, episode reward: 70.000, mean reward: 0.409 [-1.000, 0.500], mean action: 2.883 [0.000, 6.000], mean observation: 171.680 [25.000, 255.000], loss: 0.008079, mean_absolute_error: 0.888246, mean_q: 1.045342, mean_eps: 0.937533\n",
      "   69686/2000000: episode: 357, duration: 9.696s, episode steps: 192, steps per second: 20, episode reward: 141.000, mean reward: 0.734 [-1.000, 1.000], mean action: 2.979 [0.000, 6.000], mean observation: 171.481 [23.000, 255.000], loss: 0.007083, mean_absolute_error: 0.887613, mean_q: 1.051469, mean_eps: 0.937369\n",
      "   69851/2000000: episode: 358, duration: 7.894s, episode steps: 165, steps per second: 21, episode reward: 68.200, mean reward: 0.413 [-1.000, 0.500], mean action: 2.782 [0.000, 6.000], mean observation: 171.515 [24.000, 255.000], loss: 0.006882, mean_absolute_error: 0.875645, mean_q: 1.033993, mean_eps: 0.937209\n",
      "   69990/2000000: episode: 359, duration: 6.503s, episode steps: 139, steps per second: 21, episode reward: 48.800, mean reward: 0.351 [-1.000, 0.500], mean action: 2.978 [0.000, 6.000], mean observation: 172.368 [23.000, 255.000], loss: 0.007398, mean_absolute_error: 0.867767, mean_q: 1.024182, mean_eps: 0.937072\n",
      "   70190/2000000: episode: 360, duration: 10.129s, episode steps: 200, steps per second: 20, episode reward: 99.000, mean reward: 0.495 [-1.000, 1.000], mean action: 3.170 [0.000, 6.000], mean observation: 172.518 [24.000, 255.000], loss: 0.032030, mean_absolute_error: 1.337407, mean_q: 1.595041, mean_eps: 0.936919\n",
      "   70425/2000000: episode: 361, duration: 12.181s, episode steps: 235, steps per second: 19, episode reward: 159.500, mean reward: 0.679 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 172.856 [24.000, 255.000], loss: 0.012125, mean_absolute_error: 1.360931, mean_q: 1.613272, mean_eps: 0.936723\n",
      "   70607/2000000: episode: 362, duration: 8.931s, episode steps: 182, steps per second: 20, episode reward: 115.600, mean reward: 0.635 [-1.000, 1.000], mean action: 2.775 [0.000, 6.000], mean observation: 170.883 [22.000, 255.000], loss: 0.010384, mean_absolute_error: 1.379735, mean_q: 1.626871, mean_eps: 0.936536\n",
      "   70817/2000000: episode: 363, duration: 10.821s, episode steps: 210, steps per second: 19, episode reward: 130.000, mean reward: 0.619 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 172.398 [24.000, 255.000], loss: 0.009228, mean_absolute_error: 1.342791, mean_q: 1.590229, mean_eps: 0.936359\n",
      "   70981/2000000: episode: 364, duration: 7.871s, episode steps: 164, steps per second: 21, episode reward: 64.100, mean reward: 0.391 [-1.000, 0.500], mean action: 2.927 [0.000, 6.000], mean observation: 171.658 [24.000, 255.000], loss: 0.009947, mean_absolute_error: 1.354513, mean_q: 1.604003, mean_eps: 0.936190\n",
      "   71188/2000000: episode: 365, duration: 10.542s, episode steps: 207, steps per second: 20, episode reward: 123.500, mean reward: 0.597 [-1.000, 1.000], mean action: 2.966 [0.000, 6.000], mean observation: 172.641 [24.000, 255.000], loss: 0.008620, mean_absolute_error: 1.333171, mean_q: 1.565217, mean_eps: 0.936024\n",
      "   71313/2000000: episode: 366, duration: 6.688s, episode steps: 125, steps per second: 19, episode reward: 66.700, mean reward: 0.534 [-1.000, 1.000], mean action: 3.400 [0.000, 6.000], mean observation: 173.177 [23.000, 255.000], loss: 0.007595, mean_absolute_error: 1.394537, mean_q: 1.644073, mean_eps: 0.935875\n",
      "   71521/2000000: episode: 367, duration: 10.658s, episode steps: 208, steps per second: 20, episode reward: 127.400, mean reward: 0.612 [-1.000, 1.000], mean action: 3.087 [0.000, 6.000], mean observation: 172.285 [24.000, 255.000], loss: 0.008752, mean_absolute_error: 1.347793, mean_q: 1.587878, mean_eps: 0.935724\n",
      "   71702/2000000: episode: 368, duration: 8.899s, episode steps: 181, steps per second: 20, episode reward: 73.400, mean reward: 0.406 [-1.000, 0.500], mean action: 2.884 [0.000, 6.000], mean observation: 171.875 [23.000, 255.000], loss: 0.009164, mean_absolute_error: 1.348433, mean_q: 1.583111, mean_eps: 0.935549\n",
      "   71926/2000000: episode: 369, duration: 11.625s, episode steps: 224, steps per second: 19, episode reward: 104.200, mean reward: 0.465 [-1.000, 1.000], mean action: 3.134 [0.000, 6.000], mean observation: 173.049 [24.000, 255.000], loss: 0.008723, mean_absolute_error: 1.341557, mean_q: 1.583483, mean_eps: 0.935367\n",
      "   72122/2000000: episode: 370, duration: 9.918s, episode steps: 196, steps per second: 20, episode reward: 133.900, mean reward: 0.683 [-1.000, 1.000], mean action: 3.112 [0.000, 6.000], mean observation: 171.453 [22.000, 255.000], loss: 0.007564, mean_absolute_error: 1.384173, mean_q: 1.635370, mean_eps: 0.935178\n",
      "   72316/2000000: episode: 371, duration: 9.757s, episode steps: 194, steps per second: 20, episode reward: 132.400, mean reward: 0.682 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 170.783 [25.000, 255.000], loss: 0.007761, mean_absolute_error: 1.355505, mean_q: 1.595404, mean_eps: 0.935004\n",
      "   72524/2000000: episode: 372, duration: 10.520s, episode steps: 208, steps per second: 20, episode reward: 125.000, mean reward: 0.601 [-1.000, 1.000], mean action: 2.952 [0.000, 6.000], mean observation: 173.110 [23.000, 255.000], loss: 0.008329, mean_absolute_error: 1.366268, mean_q: 1.611336, mean_eps: 0.934824\n",
      "   72715/2000000: episode: 373, duration: 9.548s, episode steps: 191, steps per second: 20, episode reward: 122.300, mean reward: 0.640 [-1.000, 1.000], mean action: 3.021 [0.000, 6.000], mean observation: 170.702 [25.000, 255.000], loss: 0.007250, mean_absolute_error: 1.346282, mean_q: 1.587884, mean_eps: 0.934644\n",
      "   72920/2000000: episode: 374, duration: 10.417s, episode steps: 205, steps per second: 20, episode reward: 133.500, mean reward: 0.651 [-1.000, 1.000], mean action: 3.210 [0.000, 6.000], mean observation: 172.381 [24.000, 255.000], loss: 0.008138, mean_absolute_error: 1.351025, mean_q: 1.590785, mean_eps: 0.934466\n",
      "   73134/2000000: episode: 375, duration: 10.951s, episode steps: 214, steps per second: 20, episode reward: 143.300, mean reward: 0.670 [-1.000, 1.000], mean action: 3.257 [0.000, 6.000], mean observation: 171.606 [23.000, 255.000], loss: 0.008683, mean_absolute_error: 1.331002, mean_q: 1.573437, mean_eps: 0.934277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   73300/2000000: episode: 376, duration: 8.052s, episode steps: 166, steps per second: 21, episode reward: 78.800, mean reward: 0.475 [-1.000, 1.000], mean action: 2.759 [0.000, 6.000], mean observation: 170.652 [24.000, 255.000], loss: 0.006405, mean_absolute_error: 1.349511, mean_q: 1.589148, mean_eps: 0.934106\n",
      "   73450/2000000: episode: 377, duration: 7.101s, episode steps: 150, steps per second: 21, episode reward: 52.700, mean reward: 0.351 [-1.000, 0.500], mean action: 2.860 [0.000, 6.000], mean observation: 171.589 [23.000, 255.000], loss: 0.007306, mean_absolute_error: 1.374422, mean_q: 1.612553, mean_eps: 0.933963\n",
      "   73600/2000000: episode: 378, duration: 7.146s, episode steps: 150, steps per second: 21, episode reward: 58.300, mean reward: 0.389 [-1.000, 0.500], mean action: 3.007 [0.000, 6.000], mean observation: 171.319 [24.000, 255.000], loss: 0.007673, mean_absolute_error: 1.308086, mean_q: 1.536717, mean_eps: 0.933828\n",
      "   73753/2000000: episode: 379, duration: 7.270s, episode steps: 153, steps per second: 21, episode reward: 50.600, mean reward: 0.331 [-1.000, 0.500], mean action: 2.863 [0.000, 6.000], mean observation: 171.548 [24.000, 255.000], loss: 0.007887, mean_absolute_error: 1.354785, mean_q: 1.594843, mean_eps: 0.933692\n",
      "   73953/2000000: episode: 380, duration: 10.373s, episode steps: 200, steps per second: 19, episode reward: 118.900, mean reward: 0.594 [-1.000, 1.000], mean action: 3.045 [0.000, 6.000], mean observation: 172.439 [23.000, 255.000], loss: 0.007166, mean_absolute_error: 1.346069, mean_q: 1.592082, mean_eps: 0.933531\n",
      "   74114/2000000: episode: 381, duration: 7.710s, episode steps: 161, steps per second: 21, episode reward: 51.400, mean reward: 0.319 [-1.000, 0.500], mean action: 2.907 [0.000, 6.000], mean observation: 171.956 [24.000, 255.000], loss: 0.009423, mean_absolute_error: 1.301254, mean_q: 1.535612, mean_eps: 0.933369\n",
      "   74301/2000000: episode: 382, duration: 9.421s, episode steps: 187, steps per second: 20, episode reward: 110.800, mean reward: 0.593 [-1.000, 1.000], mean action: 3.187 [0.000, 6.000], mean observation: 170.845 [24.000, 255.000], loss: 0.007859, mean_absolute_error: 1.328942, mean_q: 1.569660, mean_eps: 0.933213\n",
      "   74457/2000000: episode: 383, duration: 8.187s, episode steps: 156, steps per second: 19, episode reward: 76.300, mean reward: 0.489 [-1.000, 1.000], mean action: 3.353 [0.000, 6.000], mean observation: 172.207 [24.000, 255.000], loss: 0.007294, mean_absolute_error: 1.376869, mean_q: 1.629039, mean_eps: 0.933058\n",
      "   74646/2000000: episode: 384, duration: 9.556s, episode steps: 189, steps per second: 20, episode reward: 92.600, mean reward: 0.490 [-1.000, 1.000], mean action: 2.979 [0.000, 6.000], mean observation: 171.909 [24.000, 255.000], loss: 0.007573, mean_absolute_error: 1.357433, mean_q: 1.596246, mean_eps: 0.932903\n",
      "   74826/2000000: episode: 385, duration: 8.697s, episode steps: 180, steps per second: 21, episode reward: 69.300, mean reward: 0.385 [-1.000, 0.500], mean action: 3.017 [0.000, 6.000], mean observation: 171.447 [23.000, 255.000], loss: 0.006741, mean_absolute_error: 1.335104, mean_q: 1.571146, mean_eps: 0.932738\n",
      "   74976/2000000: episode: 386, duration: 7.113s, episode steps: 150, steps per second: 21, episode reward: 59.500, mean reward: 0.397 [-1.000, 0.500], mean action: 2.520 [0.000, 6.000], mean observation: 171.834 [24.000, 255.000], loss: 0.008156, mean_absolute_error: 1.361497, mean_q: 1.602873, mean_eps: 0.932590\n",
      "   75133/2000000: episode: 387, duration: 7.529s, episode steps: 157, steps per second: 21, episode reward: 68.900, mean reward: 0.439 [-1.000, 1.000], mean action: 2.911 [0.000, 6.000], mean observation: 171.286 [24.000, 255.000], loss: 0.007320, mean_absolute_error: 1.358020, mean_q: 1.599401, mean_eps: 0.932451\n",
      "   75318/2000000: episode: 388, duration: 9.155s, episode steps: 185, steps per second: 20, episode reward: 111.700, mean reward: 0.604 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 171.175 [23.000, 255.000], loss: 0.008159, mean_absolute_error: 1.353847, mean_q: 1.589397, mean_eps: 0.932297\n",
      "   75467/2000000: episode: 389, duration: 7.055s, episode steps: 149, steps per second: 21, episode reward: 57.800, mean reward: 0.388 [-1.000, 0.500], mean action: 2.805 [0.000, 6.000], mean observation: 171.533 [23.000, 255.000], loss: 0.007790, mean_absolute_error: 1.310548, mean_q: 1.532961, mean_eps: 0.932147\n",
      "   75639/2000000: episode: 390, duration: 8.341s, episode steps: 172, steps per second: 21, episode reward: 74.900, mean reward: 0.435 [-1.000, 1.000], mean action: 2.715 [0.000, 6.000], mean observation: 171.459 [24.000, 255.000], loss: 0.007814, mean_absolute_error: 1.333481, mean_q: 1.560753, mean_eps: 0.932003\n",
      "   75821/2000000: episode: 391, duration: 9.017s, episode steps: 182, steps per second: 20, episode reward: 77.500, mean reward: 0.426 [-1.000, 1.000], mean action: 2.819 [0.000, 6.000], mean observation: 171.264 [24.000, 255.000], loss: 0.007621, mean_absolute_error: 1.323330, mean_q: 1.555617, mean_eps: 0.931843\n",
      "   75973/2000000: episode: 392, duration: 7.193s, episode steps: 152, steps per second: 21, episode reward: 56.100, mean reward: 0.369 [-1.000, 0.500], mean action: 2.737 [0.000, 6.000], mean observation: 172.271 [24.000, 255.000], loss: 0.007689, mean_absolute_error: 1.370502, mean_q: 1.614792, mean_eps: 0.931692\n",
      "   76151/2000000: episode: 393, duration: 8.536s, episode steps: 178, steps per second: 21, episode reward: 60.300, mean reward: 0.339 [-1.000, 0.500], mean action: 2.927 [0.000, 6.000], mean observation: 172.661 [25.000, 255.000], loss: 0.007700, mean_absolute_error: 1.357650, mean_q: 1.602126, mean_eps: 0.931544\n",
      "   76356/2000000: episode: 394, duration: 10.370s, episode steps: 205, steps per second: 20, episode reward: 125.200, mean reward: 0.611 [-1.000, 1.000], mean action: 2.961 [0.000, 6.000], mean observation: 172.568 [23.000, 255.000], loss: 0.009088, mean_absolute_error: 1.339679, mean_q: 1.572052, mean_eps: 0.931373\n",
      "   76511/2000000: episode: 395, duration: 7.504s, episode steps: 155, steps per second: 21, episode reward: 65.600, mean reward: 0.423 [-1.000, 0.500], mean action: 2.858 [0.000, 6.000], mean observation: 172.170 [24.000, 255.000], loss: 0.008227, mean_absolute_error: 1.344778, mean_q: 1.574120, mean_eps: 0.931211\n",
      "   76662/2000000: episode: 396, duration: 7.218s, episode steps: 151, steps per second: 21, episode reward: 63.200, mean reward: 0.419 [-1.000, 0.500], mean action: 2.808 [0.000, 6.000], mean observation: 171.946 [24.000, 255.000], loss: 0.007103, mean_absolute_error: 1.354843, mean_q: 1.588869, mean_eps: 0.931073\n",
      "   76839/2000000: episode: 397, duration: 8.567s, episode steps: 177, steps per second: 21, episode reward: 97.900, mean reward: 0.553 [-1.000, 1.000], mean action: 2.898 [0.000, 6.000], mean observation: 171.673 [21.000, 255.000], loss: 0.006337, mean_absolute_error: 1.300512, mean_q: 1.522951, mean_eps: 0.930925\n",
      "   76977/2000000: episode: 398, duration: 6.506s, episode steps: 138, steps per second: 21, episode reward: 49.500, mean reward: 0.359 [-1.000, 0.500], mean action: 2.841 [0.000, 6.000], mean observation: 172.700 [24.000, 255.000], loss: 0.007135, mean_absolute_error: 1.359913, mean_q: 1.605680, mean_eps: 0.930783\n",
      "   77132/2000000: episode: 399, duration: 7.472s, episode steps: 155, steps per second: 21, episode reward: 65.600, mean reward: 0.423 [-1.000, 0.500], mean action: 2.929 [0.000, 6.000], mean observation: 172.153 [24.000, 255.000], loss: 0.007477, mean_absolute_error: 1.356933, mean_q: 1.601438, mean_eps: 0.930651\n",
      "   77320/2000000: episode: 400, duration: 9.408s, episode steps: 188, steps per second: 20, episode reward: 135.000, mean reward: 0.718 [-1.000, 1.000], mean action: 2.910 [0.000, 6.000], mean observation: 171.085 [24.000, 255.000], loss: 0.006374, mean_absolute_error: 1.355057, mean_q: 1.598655, mean_eps: 0.930498\n",
      "   77477/2000000: episode: 401, duration: 8.328s, episode steps: 157, steps per second: 19, episode reward: 85.800, mean reward: 0.546 [-1.000, 1.000], mean action: 3.369 [0.000, 6.000], mean observation: 172.630 [23.000, 255.000], loss: 0.008025, mean_absolute_error: 1.386262, mean_q: 1.634591, mean_eps: 0.930342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   77659/2000000: episode: 402, duration: 9.033s, episode steps: 182, steps per second: 20, episode reward: 109.900, mean reward: 0.604 [-1.000, 1.000], mean action: 2.890 [0.000, 6.000], mean observation: 171.473 [24.000, 255.000], loss: 0.007918, mean_absolute_error: 1.356930, mean_q: 1.597871, mean_eps: 0.930189\n",
      "   77838/2000000: episode: 403, duration: 8.755s, episode steps: 179, steps per second: 20, episode reward: 108.300, mean reward: 0.605 [-1.000, 1.000], mean action: 2.944 [0.000, 6.000], mean observation: 171.697 [24.000, 255.000], loss: 0.006653, mean_absolute_error: 1.363113, mean_q: 1.604275, mean_eps: 0.930027\n",
      "   78013/2000000: episode: 404, duration: 8.379s, episode steps: 175, steps per second: 21, episode reward: 63.600, mean reward: 0.363 [-1.000, 0.500], mean action: 3.006 [0.000, 6.000], mean observation: 172.922 [23.000, 255.000], loss: 0.007678, mean_absolute_error: 1.342396, mean_q: 1.578634, mean_eps: 0.929867\n",
      "   78237/2000000: episode: 405, duration: 11.650s, episode steps: 224, steps per second: 19, episode reward: 117.500, mean reward: 0.525 [-1.000, 1.000], mean action: 3.237 [0.000, 6.000], mean observation: 173.092 [24.000, 255.000], loss: 0.006645, mean_absolute_error: 1.341839, mean_q: 1.579070, mean_eps: 0.929687\n",
      "   78411/2000000: episode: 406, duration: 8.398s, episode steps: 174, steps per second: 21, episode reward: 61.900, mean reward: 0.356 [-1.000, 0.500], mean action: 2.885 [0.000, 6.000], mean observation: 172.823 [24.000, 255.000], loss: 0.005574, mean_absolute_error: 1.362874, mean_q: 1.600626, mean_eps: 0.929508\n",
      "   78588/2000000: episode: 407, duration: 8.425s, episode steps: 177, steps per second: 21, episode reward: 67.800, mean reward: 0.383 [-1.000, 0.500], mean action: 2.949 [0.000, 6.000], mean observation: 172.992 [25.000, 255.000], loss: 0.006692, mean_absolute_error: 1.363737, mean_q: 1.596390, mean_eps: 0.929352\n",
      "   78747/2000000: episode: 408, duration: 7.656s, episode steps: 159, steps per second: 21, episode reward: 59.600, mean reward: 0.375 [-1.000, 0.500], mean action: 3.031 [0.000, 6.000], mean observation: 172.524 [22.000, 255.000], loss: 0.007464, mean_absolute_error: 1.345910, mean_q: 1.573293, mean_eps: 0.929201\n",
      "   78933/2000000: episode: 409, duration: 9.406s, episode steps: 186, steps per second: 20, episode reward: 107.500, mean reward: 0.578 [-1.000, 1.000], mean action: 2.898 [0.000, 6.000], mean observation: 172.335 [23.000, 255.000], loss: 0.007324, mean_absolute_error: 1.327343, mean_q: 1.565836, mean_eps: 0.929044\n",
      "   79120/2000000: episode: 410, duration: 9.392s, episode steps: 187, steps per second: 20, episode reward: 116.900, mean reward: 0.625 [-1.000, 1.000], mean action: 2.797 [0.000, 6.000], mean observation: 172.200 [24.000, 255.000], loss: 0.006509, mean_absolute_error: 1.349387, mean_q: 1.593401, mean_eps: 0.928877\n",
      "   79304/2000000: episode: 411, duration: 9.270s, episode steps: 184, steps per second: 20, episode reward: 107.900, mean reward: 0.586 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 171.914 [24.000, 255.000], loss: 0.006268, mean_absolute_error: 1.335349, mean_q: 1.570636, mean_eps: 0.928711\n",
      "   79535/2000000: episode: 412, duration: 12.079s, episode steps: 231, steps per second: 19, episode reward: 104.200, mean reward: 0.451 [-1.000, 1.000], mean action: 3.203 [0.000, 6.000], mean observation: 173.592 [23.000, 255.000], loss: 0.008107, mean_absolute_error: 1.349846, mean_q: 1.591615, mean_eps: 0.928524\n",
      "   79719/2000000: episode: 413, duration: 9.148s, episode steps: 184, steps per second: 20, episode reward: 125.800, mean reward: 0.684 [-1.000, 1.000], mean action: 2.913 [0.000, 6.000], mean observation: 171.770 [24.000, 255.000], loss: 0.007746, mean_absolute_error: 1.373571, mean_q: 1.621279, mean_eps: 0.928337\n",
      "   79873/2000000: episode: 414, duration: 7.358s, episode steps: 154, steps per second: 21, episode reward: 59.900, mean reward: 0.389 [-1.000, 0.500], mean action: 2.838 [0.000, 6.000], mean observation: 172.276 [23.000, 255.000], loss: 0.007174, mean_absolute_error: 1.384584, mean_q: 1.622039, mean_eps: 0.928184\n",
      "   80101/2000000: episode: 415, duration: 11.924s, episode steps: 228, steps per second: 19, episode reward: 108.400, mean reward: 0.475 [-1.000, 1.000], mean action: 3.189 [0.000, 6.000], mean observation: 173.347 [24.000, 255.000], loss: 0.026861, mean_absolute_error: 1.553252, mean_q: 1.826855, mean_eps: 0.928011\n",
      "   80282/2000000: episode: 416, duration: 8.935s, episode steps: 181, steps per second: 20, episode reward: 76.800, mean reward: 0.424 [-1.000, 1.000], mean action: 2.906 [0.000, 6.000], mean observation: 172.129 [24.000, 255.000], loss: 0.012530, mean_absolute_error: 1.784250, mean_q: 2.101042, mean_eps: 0.927827\n",
      "   80465/2000000: episode: 417, duration: 8.984s, episode steps: 183, steps per second: 20, episode reward: 92.200, mean reward: 0.504 [-1.000, 1.000], mean action: 2.874 [0.000, 6.000], mean observation: 171.893 [24.000, 255.000], loss: 0.008795, mean_absolute_error: 1.794306, mean_q: 2.127380, mean_eps: 0.927663\n",
      "   80699/2000000: episode: 418, duration: 12.195s, episode steps: 234, steps per second: 19, episode reward: 106.400, mean reward: 0.455 [-1.000, 1.000], mean action: 3.137 [0.000, 6.000], mean observation: 173.308 [23.000, 255.000], loss: 0.008512, mean_absolute_error: 1.792922, mean_q: 2.108595, mean_eps: 0.927476\n",
      "   80893/2000000: episode: 419, duration: 9.807s, episode steps: 194, steps per second: 20, episode reward: 124.400, mean reward: 0.641 [-1.000, 1.000], mean action: 2.887 [0.000, 6.000], mean observation: 171.992 [23.000, 255.000], loss: 0.008897, mean_absolute_error: 1.814762, mean_q: 2.134649, mean_eps: 0.927284\n",
      "   81070/2000000: episode: 420, duration: 8.583s, episode steps: 177, steps per second: 21, episode reward: 60.600, mean reward: 0.342 [-1.000, 0.500], mean action: 2.898 [0.000, 6.000], mean observation: 172.953 [24.000, 255.000], loss: 0.006623, mean_absolute_error: 1.802324, mean_q: 2.114404, mean_eps: 0.927116\n",
      "   81258/2000000: episode: 421, duration: 9.581s, episode steps: 188, steps per second: 20, episode reward: 133.000, mean reward: 0.707 [-1.000, 1.000], mean action: 3.181 [0.000, 6.000], mean observation: 171.831 [24.000, 255.000], loss: 0.007626, mean_absolute_error: 1.783007, mean_q: 2.093559, mean_eps: 0.926952\n",
      "   81451/2000000: episode: 422, duration: 9.819s, episode steps: 193, steps per second: 20, episode reward: 132.400, mean reward: 0.686 [-1.000, 1.000], mean action: 3.026 [0.000, 6.000], mean observation: 172.171 [24.000, 255.000], loss: 0.007963, mean_absolute_error: 1.774890, mean_q: 2.083806, mean_eps: 0.926781\n",
      "   81637/2000000: episode: 423, duration: 9.342s, episode steps: 186, steps per second: 20, episode reward: 131.400, mean reward: 0.706 [-1.000, 1.000], mean action: 2.930 [0.000, 6.000], mean observation: 171.896 [23.000, 255.000], loss: 0.008000, mean_absolute_error: 1.762945, mean_q: 2.067660, mean_eps: 0.926610\n",
      "   81794/2000000: episode: 424, duration: 7.510s, episode steps: 157, steps per second: 21, episode reward: 69.800, mean reward: 0.445 [-1.000, 1.000], mean action: 2.987 [0.000, 6.000], mean observation: 172.169 [23.000, 255.000], loss: 0.008625, mean_absolute_error: 1.798677, mean_q: 2.115228, mean_eps: 0.926456\n",
      "   81979/2000000: episode: 425, duration: 9.240s, episode steps: 185, steps per second: 20, episode reward: 117.800, mean reward: 0.637 [-1.000, 1.000], mean action: 2.762 [0.000, 6.000], mean observation: 172.036 [23.000, 255.000], loss: 0.007475, mean_absolute_error: 1.764507, mean_q: 2.069824, mean_eps: 0.926303\n",
      "   82159/2000000: episode: 426, duration: 8.662s, episode steps: 180, steps per second: 21, episode reward: 72.100, mean reward: 0.401 [-1.000, 0.500], mean action: 3.050 [0.000, 6.000], mean observation: 172.801 [24.000, 255.000], loss: 0.007600, mean_absolute_error: 1.797724, mean_q: 2.112794, mean_eps: 0.926139\n",
      "   82310/2000000: episode: 427, duration: 7.102s, episode steps: 151, steps per second: 21, episode reward: 51.200, mean reward: 0.339 [-1.000, 0.500], mean action: 2.934 [0.000, 6.000], mean observation: 172.705 [23.000, 255.000], loss: 0.007172, mean_absolute_error: 1.763128, mean_q: 2.071334, mean_eps: 0.925989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82528/2000000: episode: 428, duration: 11.136s, episode steps: 218, steps per second: 20, episode reward: 144.100, mean reward: 0.661 [-1.000, 1.000], mean action: 3.133 [0.000, 6.000], mean observation: 173.445 [24.000, 255.000], loss: 0.007092, mean_absolute_error: 1.750352, mean_q: 2.059774, mean_eps: 0.925824\n",
      "   82744/2000000: episode: 429, duration: 11.252s, episode steps: 216, steps per second: 19, episode reward: 108.300, mean reward: 0.501 [-1.000, 1.000], mean action: 3.287 [0.000, 6.000], mean observation: 173.188 [23.000, 255.000], loss: 0.007635, mean_absolute_error: 1.834032, mean_q: 2.150734, mean_eps: 0.925629\n",
      "   82927/2000000: episode: 430, duration: 9.065s, episode steps: 183, steps per second: 20, episode reward: 117.000, mean reward: 0.639 [-1.000, 1.000], mean action: 2.902 [0.000, 6.000], mean observation: 171.661 [23.000, 255.000], loss: 0.007473, mean_absolute_error: 1.810379, mean_q: 2.131416, mean_eps: 0.925449\n",
      "   83086/2000000: episode: 431, duration: 7.615s, episode steps: 159, steps per second: 21, episode reward: 66.800, mean reward: 0.420 [-1.000, 0.500], mean action: 2.849 [0.000, 6.000], mean observation: 171.858 [24.000, 255.000], loss: 0.007256, mean_absolute_error: 1.798598, mean_q: 2.114170, mean_eps: 0.925295\n",
      "   83270/2000000: episode: 432, duration: 9.054s, episode steps: 184, steps per second: 20, episode reward: 110.800, mean reward: 0.602 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 171.346 [21.000, 255.000], loss: 0.007067, mean_absolute_error: 1.766407, mean_q: 2.070184, mean_eps: 0.925140\n",
      "   83493/2000000: episode: 433, duration: 11.426s, episode steps: 223, steps per second: 20, episode reward: 158.400, mean reward: 0.710 [-1.000, 1.000], mean action: 3.108 [0.000, 6.000], mean observation: 172.828 [24.000, 255.000], loss: 0.007532, mean_absolute_error: 1.791458, mean_q: 2.102470, mean_eps: 0.924956\n",
      "   83691/2000000: episode: 434, duration: 10.120s, episode steps: 198, steps per second: 20, episode reward: 137.000, mean reward: 0.692 [-1.000, 1.000], mean action: 3.066 [0.000, 6.000], mean observation: 172.662 [23.000, 255.000], loss: 0.006898, mean_absolute_error: 1.812283, mean_q: 2.132631, mean_eps: 0.924767\n",
      "   83840/2000000: episode: 435, duration: 7.040s, episode steps: 149, steps per second: 21, episode reward: 51.000, mean reward: 0.342 [-1.000, 0.500], mean action: 2.879 [0.000, 6.000], mean observation: 172.202 [24.000, 255.000], loss: 0.006155, mean_absolute_error: 1.819282, mean_q: 2.129841, mean_eps: 0.924612\n",
      "   83998/2000000: episode: 436, duration: 8.311s, episode steps: 158, steps per second: 19, episode reward: 86.400, mean reward: 0.547 [-1.000, 1.000], mean action: 3.475 [0.000, 6.000], mean observation: 172.548 [24.000, 255.000], loss: 0.007204, mean_absolute_error: 1.780554, mean_q: 2.082062, mean_eps: 0.924474\n",
      "   84171/2000000: episode: 437, duration: 8.401s, episode steps: 173, steps per second: 21, episode reward: 77.500, mean reward: 0.448 [-1.000, 1.000], mean action: 3.012 [0.000, 6.000], mean observation: 171.607 [23.000, 255.000], loss: 0.007639, mean_absolute_error: 1.765659, mean_q: 2.073223, mean_eps: 0.924324\n",
      "   84334/2000000: episode: 438, duration: 7.801s, episode steps: 163, steps per second: 21, episode reward: 56.400, mean reward: 0.346 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 172.058 [23.000, 255.000], loss: 0.006859, mean_absolute_error: 1.807033, mean_q: 2.127496, mean_eps: 0.924173\n",
      "   84516/2000000: episode: 439, duration: 8.830s, episode steps: 182, steps per second: 21, episode reward: 73.100, mean reward: 0.402 [-1.000, 0.500], mean action: 3.077 [0.000, 6.000], mean observation: 172.117 [22.000, 255.000], loss: 0.006375, mean_absolute_error: 1.781855, mean_q: 2.092513, mean_eps: 0.924018\n",
      "   84703/2000000: episode: 440, duration: 9.203s, episode steps: 187, steps per second: 20, episode reward: 99.100, mean reward: 0.530 [-1.000, 1.000], mean action: 2.904 [0.000, 6.000], mean observation: 171.333 [24.000, 255.000], loss: 0.007831, mean_absolute_error: 1.783892, mean_q: 2.089277, mean_eps: 0.923853\n",
      "   84921/2000000: episode: 441, duration: 11.177s, episode steps: 218, steps per second: 20, episode reward: 120.200, mean reward: 0.551 [-1.000, 1.000], mean action: 3.018 [0.000, 6.000], mean observation: 172.749 [24.000, 255.000], loss: 0.005816, mean_absolute_error: 1.840689, mean_q: 2.166913, mean_eps: 0.923669\n",
      "   85102/2000000: episode: 442, duration: 8.716s, episode steps: 181, steps per second: 21, episode reward: 70.600, mean reward: 0.390 [-1.000, 0.500], mean action: 3.116 [0.000, 6.000], mean observation: 172.048 [25.000, 255.000], loss: 0.007139, mean_absolute_error: 1.758660, mean_q: 2.066843, mean_eps: 0.923489\n",
      "   85282/2000000: episode: 443, duration: 8.701s, episode steps: 180, steps per second: 21, episode reward: 76.100, mean reward: 0.423 [-1.000, 0.500], mean action: 2.939 [0.000, 6.000], mean observation: 172.203 [23.000, 255.000], loss: 0.006677, mean_absolute_error: 1.819370, mean_q: 2.143640, mean_eps: 0.923327\n",
      "   85442/2000000: episode: 444, duration: 7.664s, episode steps: 160, steps per second: 21, episode reward: 65.700, mean reward: 0.411 [-1.000, 0.500], mean action: 2.869 [0.000, 6.000], mean observation: 171.683 [23.000, 255.000], loss: 0.005889, mean_absolute_error: 1.816817, mean_q: 2.128887, mean_eps: 0.923174\n",
      "   85586/2000000: episode: 445, duration: 6.746s, episode steps: 144, steps per second: 21, episode reward: 50.100, mean reward: 0.348 [-1.000, 0.500], mean action: 2.778 [0.000, 6.000], mean observation: 172.253 [23.000, 255.000], loss: 0.006728, mean_absolute_error: 1.753734, mean_q: 2.046864, mean_eps: 0.923037\n",
      "   85769/2000000: episode: 446, duration: 8.994s, episode steps: 183, steps per second: 20, episode reward: 91.700, mean reward: 0.501 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 171.220 [25.000, 255.000], loss: 0.006386, mean_absolute_error: 1.798819, mean_q: 2.113362, mean_eps: 0.922890\n",
      "   85963/2000000: episode: 447, duration: 9.851s, episode steps: 194, steps per second: 20, episode reward: 121.800, mean reward: 0.628 [-1.000, 1.000], mean action: 3.134 [0.000, 6.000], mean observation: 171.718 [23.000, 255.000], loss: 0.006106, mean_absolute_error: 1.763935, mean_q: 2.074267, mean_eps: 0.922721\n",
      "   86148/2000000: episode: 448, duration: 9.464s, episode steps: 185, steps per second: 20, episode reward: 128.000, mean reward: 0.692 [-1.000, 1.000], mean action: 3.070 [0.000, 6.000], mean observation: 171.065 [23.000, 255.000], loss: 0.007812, mean_absolute_error: 1.782049, mean_q: 2.087491, mean_eps: 0.922551\n",
      "   86362/2000000: episode: 449, duration: 10.979s, episode steps: 214, steps per second: 19, episode reward: 150.200, mean reward: 0.702 [-1.000, 1.000], mean action: 3.131 [0.000, 6.000], mean observation: 172.419 [23.000, 255.000], loss: 0.007245, mean_absolute_error: 1.808876, mean_q: 2.126995, mean_eps: 0.922371\n",
      "   86543/2000000: episode: 450, duration: 8.813s, episode steps: 181, steps per second: 21, episode reward: 72.200, mean reward: 0.399 [-1.000, 0.500], mean action: 3.171 [0.000, 6.000], mean observation: 171.651 [24.000, 255.000], loss: 0.005806, mean_absolute_error: 1.796859, mean_q: 2.107514, mean_eps: 0.922193\n",
      "   86727/2000000: episode: 451, duration: 8.993s, episode steps: 184, steps per second: 20, episode reward: 81.400, mean reward: 0.442 [-1.000, 1.000], mean action: 3.277 [0.000, 6.000], mean observation: 171.615 [24.000, 255.000], loss: 0.006257, mean_absolute_error: 1.807752, mean_q: 2.120038, mean_eps: 0.922029\n",
      "   86936/2000000: episode: 452, duration: 10.698s, episode steps: 209, steps per second: 20, episode reward: 150.100, mean reward: 0.718 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 172.809 [24.000, 255.000], loss: 0.007518, mean_absolute_error: 1.789419, mean_q: 2.094319, mean_eps: 0.921853\n",
      "   87169/2000000: episode: 453, duration: 11.865s, episode steps: 233, steps per second: 20, episode reward: 139.400, mean reward: 0.598 [-1.000, 1.000], mean action: 3.150 [0.000, 6.000], mean observation: 172.879 [23.000, 255.000], loss: 0.007185, mean_absolute_error: 1.840709, mean_q: 2.156585, mean_eps: 0.921653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   87387/2000000: episode: 454, duration: 11.166s, episode steps: 218, steps per second: 20, episode reward: 119.000, mean reward: 0.546 [-1.000, 1.000], mean action: 3.073 [0.000, 6.000], mean observation: 172.999 [23.000, 255.000], loss: 0.006898, mean_absolute_error: 1.802990, mean_q: 2.115849, mean_eps: 0.921450\n",
      "   87567/2000000: episode: 455, duration: 8.644s, episode steps: 180, steps per second: 21, episode reward: 83.300, mean reward: 0.463 [-1.000, 1.000], mean action: 2.878 [0.000, 6.000], mean observation: 172.317 [23.000, 255.000], loss: 0.007928, mean_absolute_error: 1.770084, mean_q: 2.079035, mean_eps: 0.921272\n",
      "   87731/2000000: episode: 456, duration: 7.840s, episode steps: 164, steps per second: 21, episode reward: 68.100, mean reward: 0.415 [-1.000, 0.500], mean action: 2.866 [0.000, 6.000], mean observation: 171.774 [25.000, 255.000], loss: 0.006034, mean_absolute_error: 1.764674, mean_q: 2.072710, mean_eps: 0.921117\n",
      "   87887/2000000: episode: 457, duration: 7.447s, episode steps: 156, steps per second: 21, episode reward: 66.100, mean reward: 0.424 [-1.000, 0.500], mean action: 2.731 [0.000, 6.000], mean observation: 172.140 [24.000, 255.000], loss: 0.006086, mean_absolute_error: 1.773795, mean_q: 2.081181, mean_eps: 0.920973\n",
      "   88116/2000000: episode: 458, duration: 11.676s, episode steps: 229, steps per second: 20, episode reward: 145.800, mean reward: 0.637 [-1.000, 1.000], mean action: 3.092 [0.000, 6.000], mean observation: 173.132 [24.000, 255.000], loss: 0.007163, mean_absolute_error: 1.810275, mean_q: 2.122969, mean_eps: 0.920800\n",
      "   88296/2000000: episode: 459, duration: 8.843s, episode steps: 180, steps per second: 20, episode reward: 76.500, mean reward: 0.425 [-1.000, 0.500], mean action: 2.889 [0.000, 6.000], mean observation: 172.097 [24.000, 255.000], loss: 0.006358, mean_absolute_error: 1.829986, mean_q: 2.142424, mean_eps: 0.920616\n",
      "   88429/2000000: episode: 460, duration: 7.092s, episode steps: 133, steps per second: 19, episode reward: 73.200, mean reward: 0.550 [-1.000, 1.000], mean action: 3.368 [0.000, 6.000], mean observation: 173.414 [24.000, 255.000], loss: 0.006486, mean_absolute_error: 1.775643, mean_q: 2.082017, mean_eps: 0.920474\n",
      "   88609/2000000: episode: 461, duration: 8.723s, episode steps: 180, steps per second: 21, episode reward: 67.700, mean reward: 0.376 [-1.000, 0.500], mean action: 3.078 [0.000, 6.000], mean observation: 173.062 [23.000, 255.000], loss: 0.006801, mean_absolute_error: 1.817721, mean_q: 2.136294, mean_eps: 0.920332\n",
      "   88775/2000000: episode: 462, duration: 8.060s, episode steps: 166, steps per second: 21, episode reward: 91.500, mean reward: 0.551 [-1.000, 1.000], mean action: 2.789 [0.000, 6.000], mean observation: 171.573 [24.000, 255.000], loss: 0.005733, mean_absolute_error: 1.835498, mean_q: 2.155707, mean_eps: 0.920177\n",
      "   88956/2000000: episode: 463, duration: 8.821s, episode steps: 181, steps per second: 21, episode reward: 114.400, mean reward: 0.632 [-1.000, 1.000], mean action: 2.912 [0.000, 6.000], mean observation: 171.645 [24.000, 255.000], loss: 0.006753, mean_absolute_error: 1.810868, mean_q: 2.129409, mean_eps: 0.920022\n",
      "   89164/2000000: episode: 464, duration: 10.614s, episode steps: 208, steps per second: 20, episode reward: 143.900, mean reward: 0.692 [-1.000, 1.000], mean action: 3.101 [0.000, 6.000], mean observation: 173.305 [23.000, 255.000], loss: 0.005968, mean_absolute_error: 1.827356, mean_q: 2.138261, mean_eps: 0.919848\n",
      "   89374/2000000: episode: 465, duration: 10.797s, episode steps: 210, steps per second: 19, episode reward: 133.500, mean reward: 0.636 [-1.000, 1.000], mean action: 2.962 [0.000, 6.000], mean observation: 173.153 [23.000, 255.000], loss: 0.008266, mean_absolute_error: 1.787792, mean_q: 2.099155, mean_eps: 0.919659\n",
      "   89571/2000000: episode: 466, duration: 9.941s, episode steps: 197, steps per second: 20, episode reward: 123.700, mean reward: 0.628 [-1.000, 1.000], mean action: 2.975 [0.000, 6.000], mean observation: 173.013 [23.000, 255.000], loss: 0.006781, mean_absolute_error: 1.796688, mean_q: 2.104029, mean_eps: 0.919475\n",
      "   89767/2000000: episode: 467, duration: 9.904s, episode steps: 196, steps per second: 20, episode reward: 102.900, mean reward: 0.525 [-1.000, 1.000], mean action: 3.020 [0.000, 6.000], mean observation: 172.921 [24.000, 255.000], loss: 0.005895, mean_absolute_error: 1.776757, mean_q: 2.076390, mean_eps: 0.919299\n",
      "   89968/2000000: episode: 468, duration: 10.165s, episode steps: 201, steps per second: 20, episode reward: 100.000, mean reward: 0.498 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 173.040 [23.000, 255.000], loss: 0.006930, mean_absolute_error: 1.827305, mean_q: 2.147001, mean_eps: 0.919121\n",
      "   90174/2000000: episode: 469, duration: 10.502s, episode steps: 206, steps per second: 20, episode reward: 125.200, mean reward: 0.608 [-1.000, 1.000], mean action: 3.243 [0.000, 6.000], mean observation: 172.754 [24.000, 255.000], loss: 0.025994, mean_absolute_error: 2.152936, mean_q: 2.535906, mean_eps: 0.918937\n",
      "   90353/2000000: episode: 470, duration: 8.630s, episode steps: 179, steps per second: 21, episode reward: 78.400, mean reward: 0.438 [-1.000, 1.000], mean action: 2.978 [0.000, 6.000], mean observation: 172.768 [24.000, 255.000], loss: 0.009457, mean_absolute_error: 2.212518, mean_q: 2.594539, mean_eps: 0.918762\n",
      "   90498/2000000: episode: 471, duration: 6.752s, episode steps: 145, steps per second: 21, episode reward: 49.400, mean reward: 0.341 [-1.000, 0.500], mean action: 2.766 [0.000, 6.000], mean observation: 173.316 [24.000, 255.000], loss: 0.007634, mean_absolute_error: 2.251281, mean_q: 2.638827, mean_eps: 0.918617\n",
      "   90680/2000000: episode: 472, duration: 8.728s, episode steps: 182, steps per second: 21, episode reward: 64.700, mean reward: 0.355 [-1.000, 0.500], mean action: 2.907 [0.000, 6.000], mean observation: 173.115 [24.000, 255.000], loss: 0.007565, mean_absolute_error: 2.224964, mean_q: 2.618988, mean_eps: 0.918471\n",
      "   90863/2000000: episode: 473, duration: 8.957s, episode steps: 183, steps per second: 20, episode reward: 118.100, mean reward: 0.645 [-1.000, 1.000], mean action: 3.049 [0.000, 6.000], mean observation: 171.792 [24.000, 255.000], loss: 0.008830, mean_absolute_error: 2.214812, mean_q: 2.586909, mean_eps: 0.918307\n",
      "   91015/2000000: episode: 474, duration: 7.368s, episode steps: 152, steps per second: 21, episode reward: 66.100, mean reward: 0.435 [-1.000, 0.500], mean action: 2.717 [0.000, 6.000], mean observation: 172.214 [23.000, 255.000], loss: 0.010827, mean_absolute_error: 2.179292, mean_q: 2.551967, mean_eps: 0.918156\n",
      "   91189/2000000: episode: 475, duration: 8.373s, episode steps: 174, steps per second: 21, episode reward: 70.700, mean reward: 0.406 [-1.000, 0.500], mean action: 2.776 [0.000, 6.000], mean observation: 172.623 [24.000, 255.000], loss: 0.007667, mean_absolute_error: 2.154909, mean_q: 2.515348, mean_eps: 0.918008\n",
      "   91396/2000000: episode: 476, duration: 10.416s, episode steps: 207, steps per second: 20, episode reward: 141.000, mean reward: 0.681 [-1.000, 1.000], mean action: 3.039 [0.000, 6.000], mean observation: 173.212 [23.000, 255.000], loss: 0.006617, mean_absolute_error: 2.160092, mean_q: 2.529212, mean_eps: 0.917837\n",
      "   91640/2000000: episode: 477, duration: 12.761s, episode steps: 244, steps per second: 19, episode reward: 112.000, mean reward: 0.459 [-1.000, 1.000], mean action: 3.266 [0.000, 6.000], mean observation: 173.851 [24.000, 255.000], loss: 0.007681, mean_absolute_error: 2.171002, mean_q: 2.530083, mean_eps: 0.917636\n",
      "   91820/2000000: episode: 478, duration: 8.662s, episode steps: 180, steps per second: 21, episode reward: 68.100, mean reward: 0.378 [-1.000, 0.500], mean action: 2.911 [0.000, 6.000], mean observation: 172.791 [23.000, 255.000], loss: 0.007788, mean_absolute_error: 2.205946, mean_q: 2.566914, mean_eps: 0.917445\n",
      "   92016/2000000: episode: 479, duration: 10.011s, episode steps: 196, steps per second: 20, episode reward: 132.600, mean reward: 0.677 [-1.000, 1.000], mean action: 3.036 [0.000, 6.000], mean observation: 172.211 [24.000, 255.000], loss: 0.008216, mean_absolute_error: 2.237900, mean_q: 2.621980, mean_eps: 0.917276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   92166/2000000: episode: 480, duration: 7.173s, episode steps: 150, steps per second: 21, episode reward: 60.700, mean reward: 0.405 [-1.000, 0.500], mean action: 2.873 [0.000, 6.000], mean observation: 172.159 [24.000, 255.000], loss: 0.006687, mean_absolute_error: 2.202597, mean_q: 2.576636, mean_eps: 0.917119\n",
      "   92310/2000000: episode: 481, duration: 6.860s, episode steps: 144, steps per second: 21, episode reward: 58.100, mean reward: 0.403 [-1.000, 0.500], mean action: 2.604 [0.000, 6.000], mean observation: 172.694 [24.000, 255.000], loss: 0.006645, mean_absolute_error: 2.183942, mean_q: 2.554612, mean_eps: 0.916986\n",
      "   92509/2000000: episode: 482, duration: 9.975s, episode steps: 199, steps per second: 20, episode reward: 125.100, mean reward: 0.629 [-1.000, 1.000], mean action: 3.211 [0.000, 6.000], mean observation: 172.439 [24.000, 255.000], loss: 0.006074, mean_absolute_error: 2.157617, mean_q: 2.524440, mean_eps: 0.916831\n",
      "   92664/2000000: episode: 483, duration: 7.355s, episode steps: 155, steps per second: 21, episode reward: 56.800, mean reward: 0.366 [-1.000, 0.500], mean action: 2.748 [0.000, 6.000], mean observation: 172.422 [23.000, 255.000], loss: 0.007646, mean_absolute_error: 2.217426, mean_q: 2.584455, mean_eps: 0.916673\n",
      "   92827/2000000: episode: 484, duration: 7.784s, episode steps: 163, steps per second: 21, episode reward: 52.800, mean reward: 0.324 [-1.000, 0.500], mean action: 3.018 [0.000, 6.000], mean observation: 172.644 [23.000, 255.000], loss: 0.007400, mean_absolute_error: 2.183406, mean_q: 2.550330, mean_eps: 0.916530\n",
      "   93006/2000000: episode: 485, duration: 8.669s, episode steps: 179, steps per second: 21, episode reward: 78.600, mean reward: 0.439 [-1.000, 1.000], mean action: 2.916 [0.000, 6.000], mean observation: 172.062 [24.000, 255.000], loss: 0.006900, mean_absolute_error: 2.210466, mean_q: 2.581837, mean_eps: 0.916376\n",
      "   93186/2000000: episode: 486, duration: 8.717s, episode steps: 180, steps per second: 21, episode reward: 78.700, mean reward: 0.437 [-1.000, 1.000], mean action: 2.939 [0.000, 6.000], mean observation: 172.146 [22.000, 255.000], loss: 0.007483, mean_absolute_error: 2.129527, mean_q: 2.481029, mean_eps: 0.916214\n",
      "   93384/2000000: episode: 487, duration: 9.996s, episode steps: 198, steps per second: 20, episode reward: 100.800, mean reward: 0.509 [-1.000, 1.000], mean action: 2.965 [0.000, 6.000], mean observation: 172.821 [23.000, 255.000], loss: 0.008238, mean_absolute_error: 2.204384, mean_q: 2.580465, mean_eps: 0.916044\n",
      "   93573/2000000: episode: 488, duration: 9.505s, episode steps: 189, steps per second: 20, episode reward: 115.200, mean reward: 0.610 [-1.000, 1.000], mean action: 3.048 [0.000, 6.000], mean observation: 171.721 [25.000, 255.000], loss: 0.008292, mean_absolute_error: 2.194947, mean_q: 2.574153, mean_eps: 0.915870\n",
      "   93793/2000000: episode: 489, duration: 11.215s, episode steps: 220, steps per second: 20, episode reward: 137.200, mean reward: 0.624 [-1.000, 1.000], mean action: 3.305 [0.000, 6.000], mean observation: 172.741 [25.000, 255.000], loss: 0.007087, mean_absolute_error: 2.223108, mean_q: 2.599914, mean_eps: 0.915684\n",
      "   93926/2000000: episode: 490, duration: 6.177s, episode steps: 133, steps per second: 22, episode reward: 46.200, mean reward: 0.347 [-1.000, 0.500], mean action: 2.684 [0.000, 6.000], mean observation: 172.960 [25.000, 255.000], loss: 0.006672, mean_absolute_error: 2.247768, mean_q: 2.634852, mean_eps: 0.915526\n",
      "   94125/2000000: episode: 491, duration: 10.028s, episode steps: 199, steps per second: 20, episode reward: 145.900, mean reward: 0.733 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 172.315 [24.000, 255.000], loss: 0.006357, mean_absolute_error: 2.211476, mean_q: 2.592895, mean_eps: 0.915377\n",
      "   94283/2000000: episode: 492, duration: 7.547s, episode steps: 158, steps per second: 21, episode reward: 63.900, mean reward: 0.404 [-1.000, 0.500], mean action: 3.032 [0.000, 6.000], mean observation: 171.892 [24.000, 255.000], loss: 0.007207, mean_absolute_error: 2.250945, mean_q: 2.644046, mean_eps: 0.915216\n",
      "   94499/2000000: episode: 493, duration: 11.072s, episode steps: 216, steps per second: 20, episode reward: 142.700, mean reward: 0.661 [-1.000, 1.000], mean action: 3.144 [0.000, 6.000], mean observation: 173.175 [24.000, 255.000], loss: 0.007053, mean_absolute_error: 2.167044, mean_q: 2.535550, mean_eps: 0.915049\n",
      "   94682/2000000: episode: 494, duration: 9.048s, episode steps: 183, steps per second: 20, episode reward: 77.600, mean reward: 0.424 [-1.000, 0.500], mean action: 2.918 [0.000, 6.000], mean observation: 172.414 [23.000, 255.000], loss: 0.006779, mean_absolute_error: 2.114206, mean_q: 2.469382, mean_eps: 0.914869\n",
      "   94870/2000000: episode: 495, duration: 9.441s, episode steps: 188, steps per second: 20, episode reward: 88.200, mean reward: 0.469 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 172.314 [23.000, 255.000], loss: 0.006159, mean_absolute_error: 2.162148, mean_q: 2.533451, mean_eps: 0.914702\n",
      "   95080/2000000: episode: 496, duration: 10.634s, episode steps: 210, steps per second: 20, episode reward: 140.600, mean reward: 0.670 [-1.000, 1.000], mean action: 3.276 [0.000, 6.000], mean observation: 172.789 [24.000, 255.000], loss: 0.006238, mean_absolute_error: 2.151860, mean_q: 2.518857, mean_eps: 0.914523\n",
      "   95266/2000000: episode: 497, duration: 9.231s, episode steps: 186, steps per second: 20, episode reward: 103.400, mean reward: 0.556 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 172.082 [24.000, 255.000], loss: 0.006395, mean_absolute_error: 2.163780, mean_q: 2.539983, mean_eps: 0.914345\n",
      "   95454/2000000: episode: 498, duration: 9.312s, episode steps: 188, steps per second: 20, episode reward: 94.600, mean reward: 0.503 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 172.139 [24.000, 255.000], loss: 0.006704, mean_absolute_error: 2.195779, mean_q: 2.562816, mean_eps: 0.914176\n",
      "   95650/2000000: episode: 499, duration: 9.831s, episode steps: 196, steps per second: 20, episode reward: 113.300, mean reward: 0.578 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 172.525 [23.000, 255.000], loss: 0.005833, mean_absolute_error: 2.167001, mean_q: 2.533975, mean_eps: 0.914003\n",
      "   95799/2000000: episode: 500, duration: 7.859s, episode steps: 149, steps per second: 19, episode reward: 79.100, mean reward: 0.531 [-1.000, 1.000], mean action: 3.336 [0.000, 6.000], mean observation: 172.700 [23.000, 255.000], loss: 0.005925, mean_absolute_error: 2.177694, mean_q: 2.544214, mean_eps: 0.913848\n",
      "   95996/2000000: episode: 501, duration: 9.975s, episode steps: 197, steps per second: 20, episode reward: 145.100, mean reward: 0.737 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 172.155 [24.000, 255.000], loss: 0.009362, mean_absolute_error: 2.239129, mean_q: 2.623226, mean_eps: 0.913694\n",
      "   96167/2000000: episode: 502, duration: 8.235s, episode steps: 171, steps per second: 21, episode reward: 65.200, mean reward: 0.381 [-1.000, 0.500], mean action: 3.076 [0.000, 6.000], mean observation: 172.249 [24.000, 255.000], loss: 0.007129, mean_absolute_error: 2.180436, mean_q: 2.547990, mean_eps: 0.913528\n",
      "   96306/2000000: episode: 503, duration: 6.502s, episode steps: 139, steps per second: 21, episode reward: 50.400, mean reward: 0.363 [-1.000, 0.500], mean action: 2.748 [0.000, 6.000], mean observation: 172.661 [25.000, 255.000], loss: 0.006820, mean_absolute_error: 2.188548, mean_q: 2.565055, mean_eps: 0.913388\n",
      "   96486/2000000: episode: 504, duration: 8.745s, episode steps: 180, steps per second: 21, episode reward: 110.600, mean reward: 0.614 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 171.784 [23.000, 255.000], loss: 0.006189, mean_absolute_error: 2.202482, mean_q: 2.579523, mean_eps: 0.913244\n",
      "   96647/2000000: episode: 505, duration: 7.679s, episode steps: 161, steps per second: 21, episode reward: 76.700, mean reward: 0.476 [-1.000, 1.000], mean action: 2.994 [0.000, 6.000], mean observation: 171.263 [24.000, 255.000], loss: 0.006005, mean_absolute_error: 2.189195, mean_q: 2.558920, mean_eps: 0.913091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   96808/2000000: episode: 506, duration: 7.772s, episode steps: 161, steps per second: 21, episode reward: 87.800, mean reward: 0.545 [-1.000, 1.000], mean action: 2.689 [0.000, 6.000], mean observation: 171.214 [24.000, 255.000], loss: 0.006911, mean_absolute_error: 2.215865, mean_q: 2.587366, mean_eps: 0.912947\n",
      "   96959/2000000: episode: 507, duration: 7.096s, episode steps: 151, steps per second: 21, episode reward: 56.800, mean reward: 0.376 [-1.000, 0.500], mean action: 3.013 [0.000, 6.000], mean observation: 171.599 [24.000, 255.000], loss: 0.007396, mean_absolute_error: 2.152564, mean_q: 2.507237, mean_eps: 0.912806\n",
      "   97099/2000000: episode: 508, duration: 6.483s, episode steps: 140, steps per second: 22, episode reward: 48.900, mean reward: 0.349 [-1.000, 0.500], mean action: 2.729 [0.000, 6.000], mean observation: 172.445 [25.000, 255.000], loss: 0.009010, mean_absolute_error: 2.210826, mean_q: 2.581924, mean_eps: 0.912675\n",
      "   97230/2000000: episode: 509, duration: 6.197s, episode steps: 131, steps per second: 21, episode reward: 51.200, mean reward: 0.391 [-1.000, 0.500], mean action: 2.794 [0.000, 6.000], mean observation: 172.327 [24.000, 255.000], loss: 0.006317, mean_absolute_error: 2.146998, mean_q: 2.507092, mean_eps: 0.912552\n",
      "   97391/2000000: episode: 510, duration: 7.645s, episode steps: 161, steps per second: 21, episode reward: 65.400, mean reward: 0.406 [-1.000, 0.500], mean action: 2.932 [0.000, 6.000], mean observation: 171.376 [24.000, 255.000], loss: 0.008922, mean_absolute_error: 2.196459, mean_q: 2.566389, mean_eps: 0.912421\n",
      "   97587/2000000: episode: 511, duration: 9.918s, episode steps: 196, steps per second: 20, episode reward: 98.100, mean reward: 0.501 [-1.000, 1.000], mean action: 3.230 [0.000, 6.000], mean observation: 172.026 [23.000, 255.000], loss: 0.007918, mean_absolute_error: 2.158677, mean_q: 2.518959, mean_eps: 0.912261\n",
      "   97820/2000000: episode: 512, duration: 12.003s, episode steps: 233, steps per second: 19, episode reward: 122.600, mean reward: 0.526 [-1.000, 1.000], mean action: 3.189 [0.000, 6.000], mean observation: 172.634 [24.000, 255.000], loss: 0.006829, mean_absolute_error: 2.154630, mean_q: 2.523723, mean_eps: 0.912068\n",
      "   98002/2000000: episode: 513, duration: 8.676s, episode steps: 182, steps per second: 21, episode reward: 53.900, mean reward: 0.296 [-1.000, 0.500], mean action: 3.275 [0.000, 6.000], mean observation: 172.651 [25.000, 255.000], loss: 0.006566, mean_absolute_error: 2.215519, mean_q: 2.595474, mean_eps: 0.911881\n",
      "   98154/2000000: episode: 514, duration: 7.175s, episode steps: 152, steps per second: 21, episode reward: 52.500, mean reward: 0.345 [-1.000, 0.500], mean action: 2.941 [0.000, 6.000], mean observation: 172.265 [24.000, 255.000], loss: 0.007415, mean_absolute_error: 2.108418, mean_q: 2.458830, mean_eps: 0.911730\n",
      "   98342/2000000: episode: 515, duration: 9.528s, episode steps: 188, steps per second: 20, episode reward: 120.700, mean reward: 0.642 [-1.000, 1.000], mean action: 3.021 [0.000, 6.000], mean observation: 171.049 [23.000, 255.000], loss: 0.007249, mean_absolute_error: 2.143846, mean_q: 2.510071, mean_eps: 0.911577\n",
      "   98560/2000000: episode: 516, duration: 11.190s, episode steps: 218, steps per second: 19, episode reward: 120.100, mean reward: 0.551 [-1.000, 1.000], mean action: 3.147 [0.000, 6.000], mean observation: 172.080 [24.000, 255.000], loss: 0.005997, mean_absolute_error: 2.147242, mean_q: 2.505148, mean_eps: 0.911395\n",
      "   98744/2000000: episode: 517, duration: 9.005s, episode steps: 184, steps per second: 20, episode reward: 76.500, mean reward: 0.416 [-1.000, 0.500], mean action: 3.087 [0.000, 6.000], mean observation: 171.893 [24.000, 255.000], loss: 0.006299, mean_absolute_error: 2.157391, mean_q: 2.513922, mean_eps: 0.911215\n",
      "   98925/2000000: episode: 518, duration: 8.852s, episode steps: 181, steps per second: 20, episode reward: 90.100, mean reward: 0.498 [-1.000, 1.000], mean action: 2.906 [0.000, 6.000], mean observation: 171.561 [21.000, 255.000], loss: 0.006076, mean_absolute_error: 2.169283, mean_q: 2.535825, mean_eps: 0.911049\n",
      "   99133/2000000: episode: 519, duration: 10.607s, episode steps: 208, steps per second: 20, episode reward: 138.200, mean reward: 0.664 [-1.000, 1.000], mean action: 3.212 [0.000, 6.000], mean observation: 172.234 [24.000, 255.000], loss: 0.006493, mean_absolute_error: 2.183363, mean_q: 2.549541, mean_eps: 0.910873\n",
      "   99305/2000000: episode: 520, duration: 8.239s, episode steps: 172, steps per second: 21, episode reward: 70.500, mean reward: 0.410 [-1.000, 0.500], mean action: 2.907 [0.000, 6.000], mean observation: 171.444 [24.000, 255.000], loss: 0.006499, mean_absolute_error: 2.162047, mean_q: 2.526320, mean_eps: 0.910702\n",
      "   99524/2000000: episode: 521, duration: 11.272s, episode steps: 219, steps per second: 19, episode reward: 108.800, mean reward: 0.497 [-1.000, 1.000], mean action: 3.091 [0.000, 6.000], mean observation: 172.593 [24.000, 255.000], loss: 0.006153, mean_absolute_error: 2.154189, mean_q: 2.523751, mean_eps: 0.910527\n",
      "   99718/2000000: episode: 522, duration: 9.662s, episode steps: 194, steps per second: 20, episode reward: 125.900, mean reward: 0.649 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 171.261 [22.000, 255.000], loss: 0.006721, mean_absolute_error: 2.179909, mean_q: 2.538499, mean_eps: 0.910342\n",
      "   99955/2000000: episode: 523, duration: 12.272s, episode steps: 237, steps per second: 19, episode reward: 138.600, mean reward: 0.585 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 172.570 [22.000, 255.000], loss: 0.006447, mean_absolute_error: 2.158843, mean_q: 2.520737, mean_eps: 0.910148\n",
      "  100085/2000000: episode: 524, duration: 6.909s, episode steps: 130, steps per second: 19, episode reward: 73.200, mean reward: 0.563 [-1.000, 1.000], mean action: 3.454 [0.000, 6.000], mean observation: 172.740 [24.000, 255.000], loss: 0.036986, mean_absolute_error: 2.463063, mean_q: 2.884723, mean_eps: 0.909982\n",
      "  100239/2000000: episode: 525, duration: 7.258s, episode steps: 154, steps per second: 21, episode reward: 60.700, mean reward: 0.394 [-1.000, 0.500], mean action: 2.721 [0.000, 6.000], mean observation: 171.607 [23.000, 255.000], loss: 0.010775, mean_absolute_error: 2.523584, mean_q: 2.957982, mean_eps: 0.909854\n",
      "  100455/2000000: episode: 526, duration: 11.049s, episode steps: 216, steps per second: 20, episode reward: 112.400, mean reward: 0.520 [-1.000, 1.000], mean action: 3.014 [0.000, 6.000], mean observation: 172.580 [23.000, 255.000], loss: 0.010270, mean_absolute_error: 2.601987, mean_q: 3.054954, mean_eps: 0.909689\n",
      "  100645/2000000: episode: 527, duration: 9.593s, episode steps: 190, steps per second: 20, episode reward: 143.400, mean reward: 0.755 [-1.000, 1.000], mean action: 2.932 [0.000, 6.000], mean observation: 170.970 [23.000, 255.000], loss: 0.009405, mean_absolute_error: 2.554850, mean_q: 2.980588, mean_eps: 0.909505\n",
      "  100826/2000000: episode: 528, duration: 8.697s, episode steps: 181, steps per second: 21, episode reward: 69.800, mean reward: 0.386 [-1.000, 0.500], mean action: 3.039 [0.000, 6.000], mean observation: 171.709 [23.000, 255.000], loss: 0.008614, mean_absolute_error: 2.577618, mean_q: 3.013811, mean_eps: 0.909338\n",
      "  101023/2000000: episode: 529, duration: 9.978s, episode steps: 197, steps per second: 20, episode reward: 141.200, mean reward: 0.717 [-1.000, 1.000], mean action: 3.127 [0.000, 6.000], mean observation: 171.013 [24.000, 255.000], loss: 0.009888, mean_absolute_error: 2.582953, mean_q: 3.033631, mean_eps: 0.909168\n",
      "  101191/2000000: episode: 530, duration: 8.113s, episode steps: 168, steps per second: 21, episode reward: 71.800, mean reward: 0.427 [-1.000, 1.000], mean action: 2.940 [0.000, 6.000], mean observation: 170.797 [23.000, 255.000], loss: 0.008502, mean_absolute_error: 2.561043, mean_q: 2.989982, mean_eps: 0.909005\n",
      "  101346/2000000: episode: 531, duration: 8.153s, episode steps: 155, steps per second: 19, episode reward: 86.100, mean reward: 0.555 [-1.000, 1.000], mean action: 3.348 [0.000, 6.000], mean observation: 172.420 [24.000, 255.000], loss: 0.007949, mean_absolute_error: 2.530067, mean_q: 2.964378, mean_eps: 0.908859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  101522/2000000: episode: 532, duration: 8.777s, episode steps: 176, steps per second: 20, episode reward: 78.600, mean reward: 0.447 [-1.000, 1.000], mean action: 2.824 [0.000, 6.000], mean observation: 171.097 [23.000, 255.000], loss: 0.008582, mean_absolute_error: 2.593964, mean_q: 3.033739, mean_eps: 0.908709\n",
      "  101738/2000000: episode: 533, duration: 11.106s, episode steps: 216, steps per second: 19, episode reward: 156.600, mean reward: 0.725 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 171.945 [24.000, 255.000], loss: 0.007682, mean_absolute_error: 2.530787, mean_q: 2.962162, mean_eps: 0.908533\n",
      "  101948/2000000: episode: 534, duration: 10.622s, episode steps: 210, steps per second: 20, episode reward: 123.200, mean reward: 0.587 [-1.000, 1.000], mean action: 3.014 [0.000, 6.000], mean observation: 172.158 [24.000, 255.000], loss: 0.007324, mean_absolute_error: 2.509413, mean_q: 2.929239, mean_eps: 0.908342\n",
      "  102154/2000000: episode: 535, duration: 10.368s, episode steps: 206, steps per second: 20, episode reward: 127.300, mean reward: 0.618 [-1.000, 1.000], mean action: 3.257 [0.000, 6.000], mean observation: 171.978 [24.000, 255.000], loss: 0.008020, mean_absolute_error: 2.525505, mean_q: 2.954065, mean_eps: 0.908155\n",
      "  102343/2000000: episode: 536, duration: 9.456s, episode steps: 189, steps per second: 20, episode reward: 128.200, mean reward: 0.678 [-1.000, 1.000], mean action: 2.942 [0.000, 6.000], mean observation: 171.206 [23.000, 255.000], loss: 0.007426, mean_absolute_error: 2.597454, mean_q: 3.034957, mean_eps: 0.907977\n",
      "  102569/2000000: episode: 537, duration: 11.682s, episode steps: 226, steps per second: 19, episode reward: 125.600, mean reward: 0.556 [-1.000, 1.000], mean action: 3.049 [0.000, 6.000], mean observation: 172.215 [24.000, 255.000], loss: 0.007729, mean_absolute_error: 2.596713, mean_q: 3.042723, mean_eps: 0.907790\n",
      "  102702/2000000: episode: 538, duration: 6.132s, episode steps: 133, steps per second: 22, episode reward: 44.600, mean reward: 0.335 [-1.000, 0.500], mean action: 2.774 [0.000, 6.000], mean observation: 172.905 [24.000, 255.000], loss: 0.009488, mean_absolute_error: 2.604770, mean_q: 3.050106, mean_eps: 0.907628\n",
      "  102900/2000000: episode: 539, duration: 9.921s, episode steps: 198, steps per second: 20, episode reward: 141.600, mean reward: 0.715 [-1.000, 1.000], mean action: 3.096 [0.000, 6.000], mean observation: 171.191 [23.000, 255.000], loss: 0.008595, mean_absolute_error: 2.596138, mean_q: 3.031583, mean_eps: 0.907480\n",
      "  103042/2000000: episode: 540, duration: 6.687s, episode steps: 142, steps per second: 21, episode reward: 49.500, mean reward: 0.349 [-1.000, 0.500], mean action: 2.683 [0.000, 6.000], mean observation: 172.462 [23.000, 255.000], loss: 0.007565, mean_absolute_error: 2.538389, mean_q: 2.952501, mean_eps: 0.907327\n",
      "  103290/2000000: episode: 541, duration: 12.957s, episode steps: 248, steps per second: 19, episode reward: 123.300, mean reward: 0.497 [-1.000, 1.000], mean action: 3.210 [0.000, 6.000], mean observation: 173.191 [23.000, 255.000], loss: 0.008670, mean_absolute_error: 2.553194, mean_q: 2.973989, mean_eps: 0.907151\n",
      "  103482/2000000: episode: 542, duration: 9.432s, episode steps: 192, steps per second: 20, episode reward: 70.900, mean reward: 0.369 [-1.000, 0.500], mean action: 3.109 [0.000, 6.000], mean observation: 172.423 [23.000, 255.000], loss: 0.008332, mean_absolute_error: 2.549814, mean_q: 2.973212, mean_eps: 0.906953\n",
      "  103675/2000000: episode: 543, duration: 9.664s, episode steps: 193, steps per second: 20, episode reward: 140.400, mean reward: 0.727 [-1.000, 1.000], mean action: 2.927 [0.000, 6.000], mean observation: 171.675 [24.000, 255.000], loss: 0.007730, mean_absolute_error: 2.583801, mean_q: 3.027964, mean_eps: 0.906780\n",
      "  103820/2000000: episode: 544, duration: 6.832s, episode steps: 145, steps per second: 21, episode reward: 54.600, mean reward: 0.377 [-1.000, 0.500], mean action: 2.738 [0.000, 6.000], mean observation: 172.400 [24.000, 255.000], loss: 0.008616, mean_absolute_error: 2.598444, mean_q: 3.045936, mean_eps: 0.906629\n",
      "  104011/2000000: episode: 545, duration: 9.559s, episode steps: 191, steps per second: 20, episode reward: 137.200, mean reward: 0.718 [-1.000, 1.000], mean action: 3.225 [0.000, 6.000], mean observation: 171.311 [23.000, 255.000], loss: 0.008711, mean_absolute_error: 2.648626, mean_q: 3.094763, mean_eps: 0.906477\n",
      "  104177/2000000: episode: 546, duration: 7.947s, episode steps: 166, steps per second: 21, episode reward: 68.300, mean reward: 0.411 [-1.000, 0.500], mean action: 2.873 [0.000, 6.000], mean observation: 171.861 [23.000, 255.000], loss: 0.007525, mean_absolute_error: 2.644097, mean_q: 3.102100, mean_eps: 0.906315\n",
      "  104333/2000000: episode: 547, duration: 7.412s, episode steps: 156, steps per second: 21, episode reward: 54.500, mean reward: 0.349 [-1.000, 0.500], mean action: 2.885 [0.000, 6.000], mean observation: 172.437 [23.000, 255.000], loss: 0.007011, mean_absolute_error: 2.522000, mean_q: 2.946522, mean_eps: 0.906170\n",
      "  104549/2000000: episode: 548, duration: 11.060s, episode steps: 216, steps per second: 20, episode reward: 137.500, mean reward: 0.637 [-1.000, 1.000], mean action: 3.236 [0.000, 6.000], mean observation: 172.398 [23.000, 255.000], loss: 0.006810, mean_absolute_error: 2.571324, mean_q: 3.003743, mean_eps: 0.906002\n",
      "  104730/2000000: episode: 549, duration: 8.664s, episode steps: 181, steps per second: 21, episode reward: 72.600, mean reward: 0.401 [-1.000, 0.500], mean action: 3.033 [0.000, 6.000], mean observation: 172.337 [25.000, 255.000], loss: 0.006680, mean_absolute_error: 2.579508, mean_q: 3.007906, mean_eps: 0.905824\n",
      "  104912/2000000: episode: 550, duration: 8.786s, episode steps: 182, steps per second: 21, episode reward: 77.200, mean reward: 0.424 [-1.000, 1.000], mean action: 2.945 [0.000, 6.000], mean observation: 171.984 [24.000, 255.000], loss: 0.007179, mean_absolute_error: 2.578005, mean_q: 3.009682, mean_eps: 0.905662\n",
      "  105096/2000000: episode: 551, duration: 9.037s, episode steps: 184, steps per second: 20, episode reward: 114.300, mean reward: 0.621 [-1.000, 1.000], mean action: 3.016 [0.000, 6.000], mean observation: 171.552 [24.000, 255.000], loss: 0.007615, mean_absolute_error: 2.552691, mean_q: 2.977990, mean_eps: 0.905498\n",
      "  105239/2000000: episode: 552, duration: 6.711s, episode steps: 143, steps per second: 21, episode reward: 53.600, mean reward: 0.375 [-1.000, 0.500], mean action: 2.762 [0.000, 6.000], mean observation: 172.717 [24.000, 255.000], loss: 0.008267, mean_absolute_error: 2.613433, mean_q: 3.057230, mean_eps: 0.905351\n",
      "  105425/2000000: episode: 553, duration: 9.229s, episode steps: 186, steps per second: 20, episode reward: 105.000, mean reward: 0.565 [-1.000, 1.000], mean action: 3.102 [0.000, 6.000], mean observation: 172.032 [24.000, 255.000], loss: 0.008963, mean_absolute_error: 2.590905, mean_q: 3.017246, mean_eps: 0.905201\n",
      "  105632/2000000: episode: 554, duration: 10.620s, episode steps: 207, steps per second: 19, episode reward: 132.600, mean reward: 0.641 [-1.000, 1.000], mean action: 3.130 [0.000, 6.000], mean observation: 172.769 [23.000, 255.000], loss: 0.007284, mean_absolute_error: 2.624960, mean_q: 3.072266, mean_eps: 0.905025\n",
      "  105778/2000000: episode: 555, duration: 6.880s, episode steps: 146, steps per second: 21, episode reward: 51.500, mean reward: 0.353 [-1.000, 0.500], mean action: 2.767 [0.000, 6.000], mean observation: 173.034 [24.000, 255.000], loss: 0.006839, mean_absolute_error: 2.505459, mean_q: 2.927362, mean_eps: 0.904866\n",
      "  105925/2000000: episode: 556, duration: 6.874s, episode steps: 147, steps per second: 21, episode reward: 51.200, mean reward: 0.348 [-1.000, 0.500], mean action: 2.735 [0.000, 6.000], mean observation: 173.040 [23.000, 255.000], loss: 0.006275, mean_absolute_error: 2.528462, mean_q: 2.953960, mean_eps: 0.904733\n",
      "  106087/2000000: episode: 557, duration: 7.712s, episode steps: 162, steps per second: 21, episode reward: 66.700, mean reward: 0.412 [-1.000, 0.500], mean action: 2.821 [0.000, 6.000], mean observation: 172.464 [23.000, 255.000], loss: 0.008142, mean_absolute_error: 2.605646, mean_q: 3.052483, mean_eps: 0.904595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  106274/2000000: episode: 558, duration: 9.333s, episode steps: 187, steps per second: 20, episode reward: 102.000, mean reward: 0.545 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 172.104 [24.000, 255.000], loss: 0.006993, mean_absolute_error: 2.616875, mean_q: 3.059485, mean_eps: 0.904438\n",
      "  106480/2000000: episode: 559, duration: 10.490s, episode steps: 206, steps per second: 20, episode reward: 145.200, mean reward: 0.705 [-1.000, 1.000], mean action: 3.150 [0.000, 6.000], mean observation: 172.968 [23.000, 255.000], loss: 0.006776, mean_absolute_error: 2.551682, mean_q: 2.972273, mean_eps: 0.904262\n",
      "  106671/2000000: episode: 560, duration: 9.575s, episode steps: 191, steps per second: 20, episode reward: 127.800, mean reward: 0.669 [-1.000, 1.000], mean action: 2.895 [0.000, 6.000], mean observation: 171.670 [24.000, 255.000], loss: 0.006802, mean_absolute_error: 2.592561, mean_q: 3.027784, mean_eps: 0.904083\n",
      "  106830/2000000: episode: 561, duration: 7.642s, episode steps: 159, steps per second: 21, episode reward: 91.300, mean reward: 0.574 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 171.697 [23.000, 255.000], loss: 0.007568, mean_absolute_error: 2.568814, mean_q: 3.004135, mean_eps: 0.903925\n",
      "  106989/2000000: episode: 562, duration: 7.538s, episode steps: 159, steps per second: 21, episode reward: 55.600, mean reward: 0.350 [-1.000, 0.500], mean action: 2.975 [0.000, 6.000], mean observation: 172.416 [24.000, 255.000], loss: 0.007400, mean_absolute_error: 2.634304, mean_q: 3.082805, mean_eps: 0.903781\n",
      "  107158/2000000: episode: 563, duration: 8.063s, episode steps: 169, steps per second: 21, episode reward: 68.600, mean reward: 0.406 [-1.000, 0.500], mean action: 2.846 [0.000, 6.000], mean observation: 172.209 [24.000, 255.000], loss: 0.007983, mean_absolute_error: 2.605137, mean_q: 3.034297, mean_eps: 0.903633\n",
      "  107374/2000000: episode: 564, duration: 11.061s, episode steps: 216, steps per second: 20, episode reward: 120.600, mean reward: 0.558 [-1.000, 1.000], mean action: 3.051 [0.000, 6.000], mean observation: 173.300 [24.000, 255.000], loss: 0.007164, mean_absolute_error: 2.588797, mean_q: 3.027525, mean_eps: 0.903461\n",
      "  107554/2000000: episode: 565, duration: 8.690s, episode steps: 180, steps per second: 21, episode reward: 68.900, mean reward: 0.383 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 172.688 [24.000, 255.000], loss: 0.007471, mean_absolute_error: 2.592187, mean_q: 3.033008, mean_eps: 0.903282\n",
      "  107748/2000000: episode: 566, duration: 9.744s, episode steps: 194, steps per second: 20, episode reward: 136.800, mean reward: 0.705 [-1.000, 1.000], mean action: 2.907 [0.000, 6.000], mean observation: 171.954 [24.000, 255.000], loss: 0.008255, mean_absolute_error: 2.574102, mean_q: 2.997451, mean_eps: 0.903115\n",
      "  107930/2000000: episode: 567, duration: 9.088s, episode steps: 182, steps per second: 20, episode reward: 119.500, mean reward: 0.657 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 172.291 [23.000, 255.000], loss: 0.006245, mean_absolute_error: 2.566783, mean_q: 2.997969, mean_eps: 0.902946\n",
      "  108146/2000000: episode: 568, duration: 11.045s, episode steps: 216, steps per second: 20, episode reward: 143.700, mean reward: 0.665 [-1.000, 1.000], mean action: 3.116 [0.000, 6.000], mean observation: 173.005 [24.000, 255.000], loss: 0.007229, mean_absolute_error: 2.563924, mean_q: 2.987210, mean_eps: 0.902766\n",
      "  108354/2000000: episode: 569, duration: 10.615s, episode steps: 208, steps per second: 20, episode reward: 128.200, mean reward: 0.616 [-1.000, 1.000], mean action: 3.163 [0.000, 6.000], mean observation: 172.664 [24.000, 255.000], loss: 0.007638, mean_absolute_error: 2.560893, mean_q: 2.985221, mean_eps: 0.902575\n",
      "  108513/2000000: episode: 570, duration: 7.543s, episode steps: 159, steps per second: 21, episode reward: 56.800, mean reward: 0.357 [-1.000, 0.500], mean action: 2.962 [0.000, 6.000], mean observation: 172.530 [24.000, 255.000], loss: 0.007290, mean_absolute_error: 2.575204, mean_q: 3.010779, mean_eps: 0.902409\n",
      "  108726/2000000: episode: 571, duration: 10.662s, episode steps: 213, steps per second: 20, episode reward: 111.400, mean reward: 0.523 [-1.000, 1.000], mean action: 3.094 [0.000, 6.000], mean observation: 173.463 [23.000, 255.000], loss: 0.006711, mean_absolute_error: 2.596786, mean_q: 3.037060, mean_eps: 0.902242\n",
      "  108952/2000000: episode: 572, duration: 11.580s, episode steps: 226, steps per second: 20, episode reward: 135.600, mean reward: 0.600 [-1.000, 1.000], mean action: 3.283 [0.000, 6.000], mean observation: 173.080 [24.000, 255.000], loss: 0.007154, mean_absolute_error: 2.591898, mean_q: 3.019366, mean_eps: 0.902046\n",
      "  109162/2000000: episode: 573, duration: 10.673s, episode steps: 210, steps per second: 20, episode reward: 143.000, mean reward: 0.681 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 173.295 [24.000, 255.000], loss: 0.006783, mean_absolute_error: 2.588328, mean_q: 3.018717, mean_eps: 0.901850\n",
      "  109340/2000000: episode: 574, duration: 8.591s, episode steps: 178, steps per second: 21, episode reward: 78.900, mean reward: 0.443 [-1.000, 1.000], mean action: 2.848 [0.000, 6.000], mean observation: 172.314 [23.000, 255.000], loss: 0.007457, mean_absolute_error: 2.611229, mean_q: 3.050006, mean_eps: 0.901675\n",
      "  109493/2000000: episode: 575, duration: 7.255s, episode steps: 153, steps per second: 21, episode reward: 61.400, mean reward: 0.401 [-1.000, 0.500], mean action: 2.745 [0.000, 6.000], mean observation: 172.559 [24.000, 255.000], loss: 0.006161, mean_absolute_error: 2.517312, mean_q: 2.940253, mean_eps: 0.901526\n",
      "  109711/2000000: episode: 576, duration: 11.169s, episode steps: 218, steps per second: 20, episode reward: 126.200, mean reward: 0.579 [-1.000, 1.000], mean action: 2.977 [0.000, 6.000], mean observation: 173.439 [24.000, 255.000], loss: 0.008004, mean_absolute_error: 2.565191, mean_q: 2.985798, mean_eps: 0.901358\n",
      "  109896/2000000: episode: 577, duration: 9.145s, episode steps: 185, steps per second: 20, episode reward: 84.100, mean reward: 0.455 [-1.000, 1.000], mean action: 2.919 [0.000, 6.000], mean observation: 171.893 [22.000, 255.000], loss: 0.007598, mean_absolute_error: 2.621632, mean_q: 3.065696, mean_eps: 0.901178\n",
      "  110084/2000000: episode: 578, duration: 9.323s, episode steps: 188, steps per second: 20, episode reward: 103.000, mean reward: 0.548 [-1.000, 1.000], mean action: 3.149 [0.000, 6.000], mean observation: 172.581 [23.000, 255.000], loss: 0.024647, mean_absolute_error: 2.803174, mean_q: 3.268638, mean_eps: 0.901011\n",
      "  110250/2000000: episode: 579, duration: 8.026s, episode steps: 166, steps per second: 21, episode reward: 70.300, mean reward: 0.423 [-1.000, 0.500], mean action: 2.789 [0.000, 6.000], mean observation: 171.747 [24.000, 255.000], loss: 0.013016, mean_absolute_error: 3.107142, mean_q: 3.650047, mean_eps: 0.900851\n",
      "  110415/2000000: episode: 580, duration: 7.850s, episode steps: 165, steps per second: 21, episode reward: 56.600, mean reward: 0.343 [-1.000, 0.500], mean action: 2.818 [0.000, 6.000], mean observation: 172.425 [24.000, 255.000], loss: 0.013105, mean_absolute_error: 3.000035, mean_q: 3.510227, mean_eps: 0.900701\n",
      "  110598/2000000: episode: 581, duration: 9.056s, episode steps: 183, steps per second: 20, episode reward: 79.200, mean reward: 0.433 [-1.000, 1.000], mean action: 3.060 [0.000, 6.000], mean observation: 171.871 [24.000, 255.000], loss: 0.010438, mean_absolute_error: 2.981368, mean_q: 3.460324, mean_eps: 0.900545\n",
      "  110809/2000000: episode: 582, duration: 10.577s, episode steps: 211, steps per second: 20, episode reward: 89.600, mean reward: 0.425 [-1.000, 0.500], mean action: 3.185 [0.000, 6.000], mean observation: 173.242 [24.000, 255.000], loss: 0.010282, mean_absolute_error: 3.047680, mean_q: 3.562684, mean_eps: 0.900366\n",
      "  110976/2000000: episode: 583, duration: 7.999s, episode steps: 167, steps per second: 21, episode reward: 66.800, mean reward: 0.400 [-1.000, 0.500], mean action: 2.934 [0.000, 6.000], mean observation: 172.113 [24.000, 255.000], loss: 0.008679, mean_absolute_error: 2.955513, mean_q: 3.444696, mean_eps: 0.900197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  111154/2000000: episode: 584, duration: 8.684s, episode steps: 178, steps per second: 20, episode reward: 115.400, mean reward: 0.648 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 171.323 [23.000, 255.000], loss: 0.008341, mean_absolute_error: 2.987540, mean_q: 3.484544, mean_eps: 0.900042\n",
      "  111313/2000000: episode: 585, duration: 7.582s, episode steps: 159, steps per second: 21, episode reward: 64.400, mean reward: 0.405 [-1.000, 0.500], mean action: 2.868 [0.000, 6.000], mean observation: 171.826 [23.000, 255.000], loss: 0.008683, mean_absolute_error: 2.985031, mean_q: 3.493663, mean_eps: 0.899889\n",
      "  111498/2000000: episode: 586, duration: 9.004s, episode steps: 185, steps per second: 21, episode reward: 71.800, mean reward: 0.388 [-1.000, 0.500], mean action: 3.054 [0.000, 6.000], mean observation: 172.128 [24.000, 255.000], loss: 0.009958, mean_absolute_error: 3.066476, mean_q: 3.572796, mean_eps: 0.899735\n",
      "  111720/2000000: episode: 587, duration: 11.399s, episode steps: 222, steps per second: 19, episode reward: 127.400, mean reward: 0.574 [-1.000, 1.000], mean action: 3.140 [0.000, 6.000], mean observation: 172.540 [23.000, 255.000], loss: 0.010551, mean_absolute_error: 3.052793, mean_q: 3.566145, mean_eps: 0.899553\n",
      "  111919/2000000: episode: 588, duration: 10.046s, episode steps: 199, steps per second: 20, episode reward: 126.500, mean reward: 0.636 [-1.000, 1.000], mean action: 3.126 [0.000, 6.000], mean observation: 171.977 [24.000, 255.000], loss: 0.009536, mean_absolute_error: 3.004933, mean_q: 3.498776, mean_eps: 0.899364\n",
      "  112126/2000000: episode: 589, duration: 10.489s, episode steps: 207, steps per second: 20, episode reward: 146.900, mean reward: 0.710 [-1.000, 1.000], mean action: 3.198 [0.000, 6.000], mean observation: 172.665 [25.000, 255.000], loss: 0.010498, mean_absolute_error: 3.027869, mean_q: 3.534485, mean_eps: 0.899180\n",
      "  112321/2000000: episode: 590, duration: 9.791s, episode steps: 195, steps per second: 20, episode reward: 90.400, mean reward: 0.464 [-1.000, 1.000], mean action: 3.036 [0.000, 6.000], mean observation: 172.291 [22.000, 255.000], loss: 0.009151, mean_absolute_error: 3.019665, mean_q: 3.525584, mean_eps: 0.898998\n",
      "  112528/2000000: episode: 591, duration: 10.557s, episode steps: 207, steps per second: 20, episode reward: 140.500, mean reward: 0.679 [-1.000, 1.000], mean action: 3.198 [0.000, 6.000], mean observation: 172.259 [24.000, 255.000], loss: 0.008546, mean_absolute_error: 3.045960, mean_q: 3.569898, mean_eps: 0.898818\n",
      "  112696/2000000: episode: 592, duration: 8.125s, episode steps: 168, steps per second: 21, episode reward: 71.400, mean reward: 0.425 [-1.000, 1.000], mean action: 3.012 [0.000, 6.000], mean observation: 171.469 [23.000, 255.000], loss: 0.008711, mean_absolute_error: 3.010627, mean_q: 3.508173, mean_eps: 0.898651\n",
      "  112918/2000000: episode: 593, duration: 11.394s, episode steps: 222, steps per second: 19, episode reward: 127.600, mean reward: 0.575 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 172.529 [22.000, 255.000], loss: 0.008752, mean_absolute_error: 3.042176, mean_q: 3.548316, mean_eps: 0.898475\n",
      "  113102/2000000: episode: 594, duration: 9.118s, episode steps: 184, steps per second: 20, episode reward: 75.700, mean reward: 0.411 [-1.000, 0.500], mean action: 3.049 [0.000, 6.000], mean observation: 171.556 [23.000, 255.000], loss: 0.008967, mean_absolute_error: 2.988516, mean_q: 3.482657, mean_eps: 0.898291\n",
      "  113303/2000000: episode: 595, duration: 10.042s, episode steps: 201, steps per second: 20, episode reward: 90.600, mean reward: 0.451 [-1.000, 0.500], mean action: 3.095 [0.000, 6.000], mean observation: 172.340 [24.000, 255.000], loss: 0.007999, mean_absolute_error: 2.947623, mean_q: 3.445416, mean_eps: 0.898118\n",
      "  113483/2000000: episode: 596, duration: 8.725s, episode steps: 180, steps per second: 21, episode reward: 64.100, mean reward: 0.356 [-1.000, 0.500], mean action: 2.972 [0.000, 6.000], mean observation: 172.121 [24.000, 255.000], loss: 0.008059, mean_absolute_error: 3.007871, mean_q: 3.514725, mean_eps: 0.897947\n",
      "  113660/2000000: episode: 597, duration: 8.684s, episode steps: 177, steps per second: 20, episode reward: 74.900, mean reward: 0.423 [-1.000, 1.000], mean action: 2.944 [0.000, 6.000], mean observation: 171.702 [22.000, 255.000], loss: 0.008469, mean_absolute_error: 3.006450, mean_q: 3.501317, mean_eps: 0.897787\n",
      "  113870/2000000: episode: 598, duration: 10.805s, episode steps: 210, steps per second: 19, episode reward: 129.000, mean reward: 0.614 [-1.000, 1.000], mean action: 2.919 [0.000, 6.000], mean observation: 172.688 [24.000, 255.000], loss: 0.009064, mean_absolute_error: 3.043234, mean_q: 3.555717, mean_eps: 0.897612\n",
      "  114054/2000000: episode: 599, duration: 9.024s, episode steps: 184, steps per second: 20, episode reward: 94.400, mean reward: 0.513 [-1.000, 1.000], mean action: 2.989 [0.000, 6.000], mean observation: 171.720 [23.000, 255.000], loss: 0.009025, mean_absolute_error: 3.051531, mean_q: 3.559585, mean_eps: 0.897434\n",
      "  114197/2000000: episode: 600, duration: 6.693s, episode steps: 143, steps per second: 21, episode reward: 51.200, mean reward: 0.358 [-1.000, 0.500], mean action: 2.846 [0.000, 6.000], mean observation: 172.511 [24.000, 255.000], loss: 0.007626, mean_absolute_error: 3.030337, mean_q: 3.535414, mean_eps: 0.897287\n",
      "  114322/2000000: episode: 601, duration: 5.823s, episode steps: 125, steps per second: 21, episode reward: 45.000, mean reward: 0.360 [-1.000, 0.500], mean action: 2.856 [0.000, 6.000], mean observation: 172.670 [24.000, 255.000], loss: 0.009514, mean_absolute_error: 2.977949, mean_q: 3.467198, mean_eps: 0.897166\n",
      "  114463/2000000: episode: 602, duration: 6.640s, episode steps: 141, steps per second: 21, episode reward: 54.200, mean reward: 0.384 [-1.000, 0.500], mean action: 2.603 [0.000, 6.000], mean observation: 172.605 [24.000, 255.000], loss: 0.007020, mean_absolute_error: 2.946214, mean_q: 3.429422, mean_eps: 0.897047\n",
      "  114645/2000000: episode: 603, duration: 8.690s, episode steps: 182, steps per second: 21, episode reward: 63.900, mean reward: 0.351 [-1.000, 0.500], mean action: 2.962 [0.000, 6.000], mean observation: 172.987 [23.000, 255.000], loss: 0.007533, mean_absolute_error: 2.986969, mean_q: 3.475241, mean_eps: 0.896901\n",
      "  114841/2000000: episode: 604, duration: 9.950s, episode steps: 196, steps per second: 20, episode reward: 127.300, mean reward: 0.649 [-1.000, 1.000], mean action: 3.107 [0.000, 6.000], mean observation: 172.310 [24.000, 255.000], loss: 0.008330, mean_absolute_error: 3.024054, mean_q: 3.526980, mean_eps: 0.896730\n",
      "  115023/2000000: episode: 605, duration: 8.869s, episode steps: 182, steps per second: 21, episode reward: 112.600, mean reward: 0.619 [-1.000, 1.000], mean action: 2.835 [0.000, 6.000], mean observation: 171.436 [24.000, 255.000], loss: 0.009315, mean_absolute_error: 3.127172, mean_q: 3.652702, mean_eps: 0.896561\n",
      "  115176/2000000: episode: 606, duration: 7.351s, episode steps: 153, steps per second: 21, episode reward: 65.400, mean reward: 0.427 [-1.000, 0.500], mean action: 2.863 [0.000, 6.000], mean observation: 172.306 [24.000, 255.000], loss: 0.008300, mean_absolute_error: 3.048832, mean_q: 3.564201, mean_eps: 0.896412\n",
      "  115385/2000000: episode: 607, duration: 10.761s, episode steps: 209, steps per second: 19, episode reward: 130.200, mean reward: 0.623 [-1.000, 1.000], mean action: 3.258 [0.000, 6.000], mean observation: 172.661 [24.000, 255.000], loss: 0.007663, mean_absolute_error: 3.037994, mean_q: 3.549524, mean_eps: 0.896248\n",
      "  115577/2000000: episode: 608, duration: 9.511s, episode steps: 192, steps per second: 20, episode reward: 78.100, mean reward: 0.407 [-1.000, 0.500], mean action: 3.146 [0.000, 6.000], mean observation: 172.362 [25.000, 255.000], loss: 0.007872, mean_absolute_error: 2.957941, mean_q: 3.450432, mean_eps: 0.896066\n",
      "  115807/2000000: episode: 609, duration: 11.849s, episode steps: 230, steps per second: 19, episode reward: 138.900, mean reward: 0.604 [-1.000, 1.000], mean action: 3.161 [0.000, 6.000], mean observation: 172.925 [22.000, 255.000], loss: 0.007745, mean_absolute_error: 3.025827, mean_q: 3.536953, mean_eps: 0.895877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  116027/2000000: episode: 610, duration: 11.297s, episode steps: 220, steps per second: 19, episode reward: 110.600, mean reward: 0.503 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 172.977 [24.000, 255.000], loss: 0.008868, mean_absolute_error: 3.052273, mean_q: 3.564263, mean_eps: 0.895676\n",
      "  116222/2000000: episode: 611, duration: 9.761s, episode steps: 195, steps per second: 20, episode reward: 126.100, mean reward: 0.647 [-1.000, 1.000], mean action: 3.026 [0.000, 6.000], mean observation: 172.445 [23.000, 255.000], loss: 0.007755, mean_absolute_error: 3.070752, mean_q: 3.590712, mean_eps: 0.895488\n",
      "  116386/2000000: episode: 612, duration: 7.848s, episode steps: 164, steps per second: 21, episode reward: 67.700, mean reward: 0.413 [-1.000, 0.500], mean action: 2.915 [0.000, 6.000], mean observation: 172.225 [24.000, 255.000], loss: 0.008358, mean_absolute_error: 2.994324, mean_q: 3.494152, mean_eps: 0.895326\n",
      "  116574/2000000: episode: 613, duration: 9.445s, episode steps: 188, steps per second: 20, episode reward: 89.800, mean reward: 0.478 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 172.483 [24.000, 255.000], loss: 0.007854, mean_absolute_error: 3.021070, mean_q: 3.536412, mean_eps: 0.895168\n",
      "  116802/2000000: episode: 614, duration: 11.917s, episode steps: 228, steps per second: 19, episode reward: 102.900, mean reward: 0.451 [-1.000, 1.000], mean action: 3.105 [0.000, 6.000], mean observation: 173.663 [25.000, 255.000], loss: 0.008342, mean_absolute_error: 3.008649, mean_q: 3.522445, mean_eps: 0.894981\n",
      "  116954/2000000: episode: 615, duration: 7.076s, episode steps: 152, steps per second: 21, episode reward: 49.300, mean reward: 0.324 [-1.000, 0.500], mean action: 2.888 [0.000, 6.000], mean observation: 173.205 [23.000, 255.000], loss: 0.009829, mean_absolute_error: 2.995801, mean_q: 3.511868, mean_eps: 0.894810\n",
      "  117136/2000000: episode: 616, duration: 8.902s, episode steps: 182, steps per second: 20, episode reward: 83.900, mean reward: 0.461 [-1.000, 1.000], mean action: 2.857 [0.000, 6.000], mean observation: 172.624 [23.000, 255.000], loss: 0.007194, mean_absolute_error: 2.998522, mean_q: 3.487568, mean_eps: 0.894660\n",
      "  117305/2000000: episode: 617, duration: 8.040s, episode steps: 169, steps per second: 21, episode reward: 68.600, mean reward: 0.406 [-1.000, 0.500], mean action: 2.988 [0.000, 6.000], mean observation: 172.613 [24.000, 255.000], loss: 0.008209, mean_absolute_error: 3.029339, mean_q: 3.524044, mean_eps: 0.894502\n",
      "  117479/2000000: episode: 618, duration: 8.420s, episode steps: 174, steps per second: 21, episode reward: 73.800, mean reward: 0.424 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 172.403 [23.000, 255.000], loss: 0.007625, mean_absolute_error: 2.978524, mean_q: 3.474380, mean_eps: 0.894347\n",
      "  117686/2000000: episode: 619, duration: 10.514s, episode steps: 207, steps per second: 20, episode reward: 126.000, mean reward: 0.609 [-1.000, 1.000], mean action: 3.039 [0.000, 6.000], mean observation: 173.969 [18.000, 255.000], loss: 0.008022, mean_absolute_error: 3.033855, mean_q: 3.544831, mean_eps: 0.894176\n",
      "  117864/2000000: episode: 620, duration: 8.701s, episode steps: 178, steps per second: 20, episode reward: 110.700, mean reward: 0.622 [-1.000, 1.000], mean action: 2.888 [0.000, 6.000], mean observation: 171.999 [25.000, 255.000], loss: 0.007509, mean_absolute_error: 3.029086, mean_q: 3.547496, mean_eps: 0.894003\n",
      "  118070/2000000: episode: 621, duration: 10.360s, episode steps: 206, steps per second: 20, episode reward: 131.800, mean reward: 0.640 [-1.000, 1.000], mean action: 2.937 [0.000, 6.000], mean observation: 173.363 [24.000, 255.000], loss: 0.007465, mean_absolute_error: 3.056319, mean_q: 3.570260, mean_eps: 0.893831\n",
      "  118223/2000000: episode: 622, duration: 8.052s, episode steps: 153, steps per second: 19, episode reward: 92.800, mean reward: 0.607 [-1.000, 1.000], mean action: 3.242 [0.000, 6.000], mean observation: 172.805 [24.000, 255.000], loss: 0.008775, mean_absolute_error: 3.035146, mean_q: 3.542563, mean_eps: 0.893669\n",
      "  118440/2000000: episode: 623, duration: 11.116s, episode steps: 217, steps per second: 20, episode reward: 152.800, mean reward: 0.704 [-1.000, 1.000], mean action: 3.235 [0.000, 6.000], mean observation: 173.353 [24.000, 255.000], loss: 0.006776, mean_absolute_error: 3.049694, mean_q: 3.563691, mean_eps: 0.893503\n",
      "  118592/2000000: episode: 624, duration: 7.196s, episode steps: 152, steps per second: 21, episode reward: 53.300, mean reward: 0.351 [-1.000, 0.500], mean action: 2.855 [0.000, 6.000], mean observation: 172.813 [25.000, 255.000], loss: 0.011240, mean_absolute_error: 3.053271, mean_q: 3.567309, mean_eps: 0.893337\n",
      "  118752/2000000: episode: 625, duration: 7.592s, episode steps: 160, steps per second: 21, episode reward: 63.300, mean reward: 0.396 [-1.000, 0.500], mean action: 3.062 [0.000, 6.000], mean observation: 172.149 [23.000, 255.000], loss: 0.009277, mean_absolute_error: 3.015973, mean_q: 3.527383, mean_eps: 0.893197\n",
      "  118899/2000000: episode: 626, duration: 6.907s, episode steps: 147, steps per second: 21, episode reward: 56.800, mean reward: 0.386 [-1.000, 0.500], mean action: 2.748 [0.000, 6.000], mean observation: 172.590 [25.000, 255.000], loss: 0.008874, mean_absolute_error: 3.060334, mean_q: 3.577883, mean_eps: 0.893058\n",
      "  119057/2000000: episode: 627, duration: 7.532s, episode steps: 158, steps per second: 21, episode reward: 71.300, mean reward: 0.451 [-1.000, 1.000], mean action: 2.835 [0.000, 6.000], mean observation: 171.965 [24.000, 255.000], loss: 0.007927, mean_absolute_error: 3.023178, mean_q: 3.525117, mean_eps: 0.892920\n",
      "  119257/2000000: episode: 628, duration: 10.131s, episode steps: 200, steps per second: 20, episode reward: 94.800, mean reward: 0.474 [-1.000, 1.000], mean action: 3.195 [0.000, 6.000], mean observation: 173.043 [25.000, 255.000], loss: 0.008283, mean_absolute_error: 3.003769, mean_q: 3.496446, mean_eps: 0.892758\n",
      "  119462/2000000: episode: 629, duration: 10.389s, episode steps: 205, steps per second: 20, episode reward: 144.600, mean reward: 0.705 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 172.958 [24.000, 255.000], loss: 0.006897, mean_absolute_error: 2.946160, mean_q: 3.435849, mean_eps: 0.892576\n",
      "  119617/2000000: episode: 630, duration: 7.439s, episode steps: 155, steps per second: 21, episode reward: 65.200, mean reward: 0.421 [-1.000, 0.500], mean action: 2.852 [0.000, 6.000], mean observation: 172.144 [24.000, 255.000], loss: 0.007030, mean_absolute_error: 2.986591, mean_q: 3.480646, mean_eps: 0.892414\n",
      "  119813/2000000: episode: 631, duration: 9.876s, episode steps: 196, steps per second: 20, episode reward: 92.900, mean reward: 0.474 [-1.000, 1.000], mean action: 3.031 [0.000, 6.000], mean observation: 172.716 [23.000, 255.000], loss: 0.007800, mean_absolute_error: 3.027248, mean_q: 3.534572, mean_eps: 0.892256\n",
      "  119974/2000000: episode: 632, duration: 7.708s, episode steps: 161, steps per second: 21, episode reward: 67.800, mean reward: 0.421 [-1.000, 0.500], mean action: 2.888 [0.000, 6.000], mean observation: 172.183 [23.000, 255.000], loss: 0.010818, mean_absolute_error: 2.992464, mean_q: 3.492061, mean_eps: 0.892095\n",
      "  120168/2000000: episode: 633, duration: 9.478s, episode steps: 194, steps per second: 20, episode reward: 55.100, mean reward: 0.284 [-1.000, 0.500], mean action: 3.144 [0.000, 6.000], mean observation: 173.145 [24.000, 255.000], loss: 0.036906, mean_absolute_error: 3.374677, mean_q: 3.957834, mean_eps: 0.891937\n",
      "  120352/2000000: episode: 634, duration: 9.024s, episode steps: 184, steps per second: 20, episode reward: 75.700, mean reward: 0.411 [-1.000, 0.500], mean action: 2.978 [0.000, 6.000], mean observation: 171.998 [24.000, 255.000], loss: 0.013108, mean_absolute_error: 3.398256, mean_q: 3.969481, mean_eps: 0.891768\n",
      "  120533/2000000: episode: 635, duration: 8.825s, episode steps: 181, steps per second: 21, episode reward: 81.700, mean reward: 0.451 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 172.494 [23.000, 255.000], loss: 0.013359, mean_absolute_error: 3.442545, mean_q: 4.014570, mean_eps: 0.891602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  120755/2000000: episode: 636, duration: 11.427s, episode steps: 222, steps per second: 19, episode reward: 118.200, mean reward: 0.532 [-1.000, 1.000], mean action: 3.041 [0.000, 6.000], mean observation: 172.980 [25.000, 255.000], loss: 0.010725, mean_absolute_error: 3.416398, mean_q: 3.983134, mean_eps: 0.891420\n",
      "  120942/2000000: episode: 637, duration: 9.258s, episode steps: 187, steps per second: 20, episode reward: 108.000, mean reward: 0.578 [-1.000, 1.000], mean action: 2.968 [0.000, 6.000], mean observation: 171.826 [24.000, 255.000], loss: 0.011916, mean_absolute_error: 3.479252, mean_q: 4.063767, mean_eps: 0.891237\n",
      "  121115/2000000: episode: 638, duration: 8.320s, episode steps: 173, steps per second: 21, episode reward: 75.000, mean reward: 0.434 [-1.000, 1.000], mean action: 3.006 [0.000, 6.000], mean observation: 171.744 [23.000, 255.000], loss: 0.009526, mean_absolute_error: 3.409912, mean_q: 3.980652, mean_eps: 0.891075\n",
      "  121321/2000000: episode: 639, duration: 10.495s, episode steps: 206, steps per second: 20, episode reward: 136.300, mean reward: 0.662 [-1.000, 1.000], mean action: 3.238 [0.000, 6.000], mean observation: 172.803 [24.000, 255.000], loss: 0.011928, mean_absolute_error: 3.348250, mean_q: 3.907043, mean_eps: 0.890904\n",
      "  121540/2000000: episode: 640, duration: 11.270s, episode steps: 219, steps per second: 19, episode reward: 147.000, mean reward: 0.671 [-1.000, 1.000], mean action: 3.297 [0.000, 6.000], mean observation: 172.711 [24.000, 255.000], loss: 0.009974, mean_absolute_error: 3.367682, mean_q: 3.927632, mean_eps: 0.890713\n",
      "  121700/2000000: episode: 641, duration: 8.460s, episode steps: 160, steps per second: 19, episode reward: 88.800, mean reward: 0.555 [-1.000, 1.000], mean action: 3.300 [0.000, 6.000], mean observation: 172.970 [24.000, 255.000], loss: 0.010754, mean_absolute_error: 3.336160, mean_q: 3.889592, mean_eps: 0.890544\n",
      "  121882/2000000: episode: 642, duration: 8.816s, episode steps: 182, steps per second: 21, episode reward: 67.900, mean reward: 0.373 [-1.000, 0.500], mean action: 3.049 [0.000, 6.000], mean observation: 172.794 [24.000, 255.000], loss: 0.010008, mean_absolute_error: 3.439875, mean_q: 4.010523, mean_eps: 0.890389\n",
      "  122082/2000000: episode: 643, duration: 10.090s, episode steps: 200, steps per second: 20, episode reward: 135.100, mean reward: 0.675 [-1.000, 1.000], mean action: 3.060 [0.000, 6.000], mean observation: 172.207 [24.000, 255.000], loss: 0.009752, mean_absolute_error: 3.408263, mean_q: 3.976539, mean_eps: 0.890216\n",
      "  122290/2000000: episode: 644, duration: 10.475s, episode steps: 208, steps per second: 20, episode reward: 107.800, mean reward: 0.518 [-1.000, 1.000], mean action: 2.976 [0.000, 6.000], mean observation: 172.772 [24.000, 255.000], loss: 0.009341, mean_absolute_error: 3.370462, mean_q: 3.928258, mean_eps: 0.890033\n",
      "  122506/2000000: episode: 645, duration: 11.127s, episode steps: 216, steps per second: 19, episode reward: 154.700, mean reward: 0.716 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 172.940 [23.000, 255.000], loss: 0.013042, mean_absolute_error: 3.408099, mean_q: 3.974924, mean_eps: 0.889842\n",
      "  122663/2000000: episode: 646, duration: 7.433s, episode steps: 157, steps per second: 21, episode reward: 53.000, mean reward: 0.338 [-1.000, 0.500], mean action: 3.166 [0.000, 6.000], mean observation: 172.429 [24.000, 255.000], loss: 0.009634, mean_absolute_error: 3.336832, mean_q: 3.891656, mean_eps: 0.889674\n",
      "  122832/2000000: episode: 647, duration: 8.097s, episode steps: 169, steps per second: 21, episode reward: 65.000, mean reward: 0.385 [-1.000, 0.500], mean action: 3.130 [0.000, 6.000], mean observation: 172.189 [24.000, 255.000], loss: 0.009586, mean_absolute_error: 3.379808, mean_q: 3.945742, mean_eps: 0.889529\n",
      "  123053/2000000: episode: 648, duration: 11.381s, episode steps: 221, steps per second: 19, episode reward: 121.200, mean reward: 0.548 [-1.000, 1.000], mean action: 3.167 [0.000, 6.000], mean observation: 172.547 [24.000, 255.000], loss: 0.009568, mean_absolute_error: 3.464424, mean_q: 4.044914, mean_eps: 0.889352\n",
      "  123245/2000000: episode: 649, duration: 9.657s, episode steps: 192, steps per second: 20, episode reward: 101.300, mean reward: 0.528 [-1.000, 1.000], mean action: 2.885 [0.000, 6.000], mean observation: 172.318 [23.000, 255.000], loss: 0.009769, mean_absolute_error: 3.326379, mean_q: 3.872411, mean_eps: 0.889165\n",
      "  123434/2000000: episode: 650, duration: 9.400s, episode steps: 189, steps per second: 20, episode reward: 96.600, mean reward: 0.511 [-1.000, 1.000], mean action: 3.079 [0.000, 6.000], mean observation: 172.252 [24.000, 255.000], loss: 0.009254, mean_absolute_error: 3.373221, mean_q: 3.924965, mean_eps: 0.888994\n",
      "  123616/2000000: episode: 651, duration: 8.969s, episode steps: 182, steps per second: 20, episode reward: 83.100, mean reward: 0.457 [-1.000, 1.000], mean action: 2.720 [0.000, 6.000], mean observation: 172.068 [23.000, 255.000], loss: 0.011016, mean_absolute_error: 3.344231, mean_q: 3.890971, mean_eps: 0.888828\n",
      "  123795/2000000: episode: 652, duration: 8.693s, episode steps: 179, steps per second: 21, episode reward: 86.900, mean reward: 0.485 [-1.000, 1.000], mean action: 2.927 [0.000, 6.000], mean observation: 171.991 [24.000, 255.000], loss: 0.008959, mean_absolute_error: 3.440660, mean_q: 4.016187, mean_eps: 0.888666\n",
      "  123946/2000000: episode: 653, duration: 7.101s, episode steps: 151, steps per second: 21, episode reward: 53.200, mean reward: 0.352 [-1.000, 0.500], mean action: 2.881 [0.000, 6.000], mean observation: 172.012 [24.000, 255.000], loss: 0.009594, mean_absolute_error: 3.407501, mean_q: 3.964996, mean_eps: 0.888517\n",
      "  124153/2000000: episode: 654, duration: 10.628s, episode steps: 207, steps per second: 19, episode reward: 148.400, mean reward: 0.717 [-1.000, 1.000], mean action: 3.053 [0.000, 6.000], mean observation: 172.469 [24.000, 255.000], loss: 0.009327, mean_absolute_error: 3.308947, mean_q: 3.858178, mean_eps: 0.888355\n",
      "  124287/2000000: episode: 655, duration: 6.307s, episode steps: 134, steps per second: 21, episode reward: 50.300, mean reward: 0.375 [-1.000, 0.500], mean action: 2.866 [0.000, 6.000], mean observation: 172.240 [23.000, 255.000], loss: 0.008658, mean_absolute_error: 3.456090, mean_q: 4.035666, mean_eps: 0.888202\n",
      "  124533/2000000: episode: 656, duration: 12.824s, episode steps: 246, steps per second: 19, episode reward: 171.700, mean reward: 0.698 [-1.000, 1.000], mean action: 3.183 [0.000, 6.000], mean observation: 173.013 [24.000, 255.000], loss: 0.008954, mean_absolute_error: 3.387626, mean_q: 3.954679, mean_eps: 0.888031\n",
      "  124677/2000000: episode: 657, duration: 6.872s, episode steps: 144, steps per second: 21, episode reward: 60.100, mean reward: 0.417 [-1.000, 0.500], mean action: 2.861 [0.000, 6.000], mean observation: 172.142 [23.000, 255.000], loss: 0.009966, mean_absolute_error: 3.342332, mean_q: 3.900323, mean_eps: 0.887855\n",
      "  124874/2000000: episode: 658, duration: 9.947s, episode steps: 197, steps per second: 20, episode reward: 129.500, mean reward: 0.657 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 171.953 [25.000, 255.000], loss: 0.009708, mean_absolute_error: 3.396596, mean_q: 3.963416, mean_eps: 0.887702\n",
      "  125078/2000000: episode: 659, duration: 10.158s, episode steps: 204, steps per second: 20, episode reward: 113.100, mean reward: 0.554 [-1.000, 1.000], mean action: 2.985 [0.000, 6.000], mean observation: 172.969 [24.000, 255.000], loss: 0.009840, mean_absolute_error: 3.456740, mean_q: 4.019726, mean_eps: 0.887522\n",
      "  125258/2000000: episode: 660, duration: 8.671s, episode steps: 180, steps per second: 21, episode reward: 103.200, mean reward: 0.573 [-1.000, 1.000], mean action: 2.933 [0.000, 6.000], mean observation: 171.302 [23.000, 255.000], loss: 0.010226, mean_absolute_error: 3.466468, mean_q: 4.038405, mean_eps: 0.887349\n",
      "  125489/2000000: episode: 661, duration: 11.985s, episode steps: 231, steps per second: 19, episode reward: 111.900, mean reward: 0.484 [-1.000, 1.000], mean action: 3.268 [0.000, 6.000], mean observation: 172.699 [24.000, 255.000], loss: 0.011282, mean_absolute_error: 3.426446, mean_q: 3.994238, mean_eps: 0.887163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  125703/2000000: episode: 662, duration: 10.927s, episode steps: 214, steps per second: 20, episode reward: 144.300, mean reward: 0.674 [-1.000, 1.000], mean action: 3.173 [0.000, 6.000], mean observation: 172.332 [24.000, 255.000], loss: 0.010715, mean_absolute_error: 3.455494, mean_q: 4.017558, mean_eps: 0.886964\n",
      "  125894/2000000: episode: 663, duration: 9.635s, episode steps: 191, steps per second: 20, episode reward: 106.200, mean reward: 0.556 [-1.000, 1.000], mean action: 3.031 [0.000, 6.000], mean observation: 171.764 [22.000, 255.000], loss: 0.009522, mean_absolute_error: 3.496448, mean_q: 4.075122, mean_eps: 0.886782\n",
      "  126076/2000000: episode: 664, duration: 8.990s, episode steps: 182, steps per second: 20, episode reward: 103.000, mean reward: 0.566 [-1.000, 1.000], mean action: 3.264 [0.000, 6.000], mean observation: 171.133 [23.000, 255.000], loss: 0.010797, mean_absolute_error: 3.435136, mean_q: 3.999583, mean_eps: 0.886614\n",
      "  126256/2000000: episode: 665, duration: 8.846s, episode steps: 180, steps per second: 20, episode reward: 87.800, mean reward: 0.488 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 171.079 [23.000, 255.000], loss: 0.008932, mean_absolute_error: 3.424604, mean_q: 3.980750, mean_eps: 0.886452\n",
      "  126466/2000000: episode: 666, duration: 10.452s, episode steps: 210, steps per second: 20, episode reward: 82.700, mean reward: 0.394 [-1.000, 0.500], mean action: 3.057 [0.000, 6.000], mean observation: 172.924 [24.000, 255.000], loss: 0.009228, mean_absolute_error: 3.421936, mean_q: 3.975005, mean_eps: 0.886276\n",
      "  126661/2000000: episode: 667, duration: 9.760s, episode steps: 195, steps per second: 20, episode reward: 128.600, mean reward: 0.659 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 171.486 [24.000, 255.000], loss: 0.009551, mean_absolute_error: 3.403026, mean_q: 3.952519, mean_eps: 0.886092\n",
      "  126842/2000000: episode: 668, duration: 8.755s, episode steps: 181, steps per second: 21, episode reward: 72.200, mean reward: 0.399 [-1.000, 0.500], mean action: 3.243 [0.000, 6.000], mean observation: 171.485 [24.000, 255.000], loss: 0.009350, mean_absolute_error: 3.374583, mean_q: 3.931430, mean_eps: 0.885923\n",
      "  127044/2000000: episode: 669, duration: 10.196s, episode steps: 202, steps per second: 20, episode reward: 123.700, mean reward: 0.612 [-1.000, 1.000], mean action: 2.946 [0.000, 6.000], mean observation: 171.496 [23.000, 255.000], loss: 0.009058, mean_absolute_error: 3.452057, mean_q: 4.026218, mean_eps: 0.885752\n",
      "  127223/2000000: episode: 670, duration: 8.677s, episode steps: 179, steps per second: 21, episode reward: 70.800, mean reward: 0.396 [-1.000, 0.500], mean action: 3.061 [0.000, 6.000], mean observation: 171.438 [23.000, 255.000], loss: 0.008338, mean_absolute_error: 3.355420, mean_q: 3.914506, mean_eps: 0.885581\n",
      "  127404/2000000: episode: 671, duration: 8.719s, episode steps: 181, steps per second: 21, episode reward: 69.000, mean reward: 0.381 [-1.000, 0.500], mean action: 2.983 [0.000, 6.000], mean observation: 172.082 [24.000, 255.000], loss: 0.010020, mean_absolute_error: 3.414576, mean_q: 3.975530, mean_eps: 0.885419\n",
      "  127590/2000000: episode: 672, duration: 9.185s, episode steps: 186, steps per second: 20, episode reward: 106.500, mean reward: 0.573 [-1.000, 1.000], mean action: 2.887 [0.000, 6.000], mean observation: 170.833 [23.000, 255.000], loss: 0.008714, mean_absolute_error: 3.355959, mean_q: 3.921024, mean_eps: 0.885254\n",
      "  127782/2000000: episode: 673, duration: 9.525s, episode steps: 192, steps per second: 20, episode reward: 95.200, mean reward: 0.496 [-1.000, 1.000], mean action: 3.062 [0.000, 6.000], mean observation: 171.533 [24.000, 255.000], loss: 0.010344, mean_absolute_error: 3.488296, mean_q: 4.060747, mean_eps: 0.885083\n",
      "  127924/2000000: episode: 674, duration: 6.668s, episode steps: 142, steps per second: 21, episode reward: 51.100, mean reward: 0.360 [-1.000, 0.500], mean action: 2.923 [0.000, 6.000], mean observation: 171.994 [23.000, 255.000], loss: 0.010327, mean_absolute_error: 3.466176, mean_q: 4.048299, mean_eps: 0.884933\n",
      "  128075/2000000: episode: 675, duration: 7.064s, episode steps: 151, steps per second: 21, episode reward: 54.400, mean reward: 0.360 [-1.000, 0.500], mean action: 2.960 [0.000, 6.000], mean observation: 171.546 [23.000, 255.000], loss: 0.009549, mean_absolute_error: 3.382161, mean_q: 3.940583, mean_eps: 0.884802\n",
      "  128280/2000000: episode: 676, duration: 10.488s, episode steps: 205, steps per second: 20, episode reward: 152.200, mean reward: 0.742 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 172.318 [24.000, 255.000], loss: 0.008699, mean_absolute_error: 3.396255, mean_q: 3.950656, mean_eps: 0.884642\n",
      "  128422/2000000: episode: 677, duration: 6.668s, episode steps: 142, steps per second: 21, episode reward: 51.900, mean reward: 0.365 [-1.000, 0.500], mean action: 2.859 [0.000, 6.000], mean observation: 171.873 [24.000, 255.000], loss: 0.011382, mean_absolute_error: 3.300297, mean_q: 3.817605, mean_eps: 0.884485\n",
      "  128608/2000000: episode: 678, duration: 9.204s, episode steps: 186, steps per second: 20, episode reward: 107.400, mean reward: 0.577 [-1.000, 1.000], mean action: 2.968 [0.000, 6.000], mean observation: 171.179 [24.000, 255.000], loss: 0.009728, mean_absolute_error: 3.539235, mean_q: 4.126741, mean_eps: 0.884337\n",
      "  128788/2000000: episode: 679, duration: 8.787s, episode steps: 180, steps per second: 20, episode reward: 108.000, mean reward: 0.600 [-1.000, 1.000], mean action: 3.089 [0.000, 6.000], mean observation: 170.512 [25.000, 255.000], loss: 0.008558, mean_absolute_error: 3.402648, mean_q: 3.966892, mean_eps: 0.884174\n",
      "  128927/2000000: episode: 680, duration: 6.570s, episode steps: 139, steps per second: 21, episode reward: 48.800, mean reward: 0.351 [-1.000, 0.500], mean action: 2.655 [0.000, 6.000], mean observation: 172.098 [24.000, 255.000], loss: 0.007820, mean_absolute_error: 3.390407, mean_q: 3.949677, mean_eps: 0.884030\n",
      "  129076/2000000: episode: 681, duration: 7.052s, episode steps: 149, steps per second: 21, episode reward: 57.800, mean reward: 0.388 [-1.000, 0.500], mean action: 2.933 [0.000, 6.000], mean observation: 171.496 [24.000, 255.000], loss: 0.010022, mean_absolute_error: 3.340899, mean_q: 3.893573, mean_eps: 0.883900\n",
      "  129252/2000000: episode: 682, duration: 8.517s, episode steps: 176, steps per second: 21, episode reward: 71.300, mean reward: 0.405 [-1.000, 0.500], mean action: 2.898 [0.000, 6.000], mean observation: 171.586 [24.000, 255.000], loss: 0.009550, mean_absolute_error: 3.479481, mean_q: 4.059323, mean_eps: 0.883754\n",
      "  129433/2000000: episode: 683, duration: 8.752s, episode steps: 181, steps per second: 21, episode reward: 107.300, mean reward: 0.593 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 170.764 [23.000, 255.000], loss: 0.008207, mean_absolute_error: 3.366606, mean_q: 3.927228, mean_eps: 0.883592\n",
      "  129642/2000000: episode: 684, duration: 10.517s, episode steps: 209, steps per second: 20, episode reward: 108.700, mean reward: 0.520 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 172.926 [23.000, 255.000], loss: 0.008688, mean_absolute_error: 3.377866, mean_q: 3.927780, mean_eps: 0.883416\n",
      "  129810/2000000: episode: 685, duration: 8.169s, episode steps: 168, steps per second: 21, episode reward: 112.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.798 [0.000, 6.000], mean observation: 170.362 [23.000, 255.000], loss: 0.008253, mean_absolute_error: 3.485381, mean_q: 4.071222, mean_eps: 0.883247\n",
      "  130024/2000000: episode: 686, duration: 11.030s, episode steps: 214, steps per second: 19, episode reward: 136.000, mean reward: 0.636 [-1.000, 1.000], mean action: 3.047 [0.000, 6.000], mean observation: 171.924 [24.000, 255.000], loss: 0.015174, mean_absolute_error: 3.380459, mean_q: 3.925041, mean_eps: 0.883076\n",
      "  130213/2000000: episode: 687, duration: 9.410s, episode steps: 189, steps per second: 20, episode reward: 138.300, mean reward: 0.732 [-1.000, 1.000], mean action: 2.868 [0.000, 6.000], mean observation: 171.066 [24.000, 255.000], loss: 0.024198, mean_absolute_error: 3.805974, mean_q: 4.444707, mean_eps: 0.882894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  130370/2000000: episode: 688, duration: 7.495s, episode steps: 157, steps per second: 21, episode reward: 80.000, mean reward: 0.510 [-1.000, 1.000], mean action: 2.847 [0.000, 6.000], mean observation: 171.217 [23.000, 255.000], loss: 0.013626, mean_absolute_error: 3.723365, mean_q: 4.339189, mean_eps: 0.882737\n",
      "  130536/2000000: episode: 689, duration: 7.973s, episode steps: 166, steps per second: 21, episode reward: 70.300, mean reward: 0.423 [-1.000, 0.500], mean action: 2.705 [0.000, 6.000], mean observation: 171.263 [24.000, 255.000], loss: 0.012601, mean_absolute_error: 3.815317, mean_q: 4.447087, mean_eps: 0.882593\n",
      "  130702/2000000: episode: 690, duration: 7.922s, episode steps: 166, steps per second: 21, episode reward: 67.100, mean reward: 0.404 [-1.000, 0.500], mean action: 2.922 [0.000, 6.000], mean observation: 172.166 [25.000, 255.000], loss: 0.011635, mean_absolute_error: 3.806749, mean_q: 4.437352, mean_eps: 0.882444\n",
      "  130872/2000000: episode: 691, duration: 8.169s, episode steps: 170, steps per second: 21, episode reward: 70.700, mean reward: 0.416 [-1.000, 0.500], mean action: 2.888 [0.000, 6.000], mean observation: 171.766 [24.000, 255.000], loss: 0.009996, mean_absolute_error: 3.773750, mean_q: 4.391860, mean_eps: 0.882293\n",
      "  131053/2000000: episode: 692, duration: 8.706s, episode steps: 181, steps per second: 21, episode reward: 71.800, mean reward: 0.397 [-1.000, 0.500], mean action: 2.934 [0.000, 6.000], mean observation: 172.232 [24.000, 255.000], loss: 0.011526, mean_absolute_error: 3.748378, mean_q: 4.364603, mean_eps: 0.882134\n",
      "  131237/2000000: episode: 693, duration: 9.080s, episode steps: 184, steps per second: 20, episode reward: 94.200, mean reward: 0.512 [-1.000, 1.000], mean action: 2.886 [0.000, 6.000], mean observation: 171.999 [24.000, 255.000], loss: 0.011216, mean_absolute_error: 3.774882, mean_q: 4.398298, mean_eps: 0.881969\n",
      "  131372/2000000: episode: 694, duration: 6.291s, episode steps: 135, steps per second: 21, episode reward: 47.200, mean reward: 0.350 [-1.000, 0.500], mean action: 2.689 [0.000, 6.000], mean observation: 173.076 [24.000, 255.000], loss: 0.010532, mean_absolute_error: 3.794939, mean_q: 4.433618, mean_eps: 0.881826\n",
      "  131550/2000000: episode: 695, duration: 8.666s, episode steps: 178, steps per second: 21, episode reward: 94.100, mean reward: 0.529 [-1.000, 1.000], mean action: 2.888 [0.000, 6.000], mean observation: 171.824 [24.000, 255.000], loss: 0.010404, mean_absolute_error: 3.811363, mean_q: 4.455621, mean_eps: 0.881686\n",
      "  131751/2000000: episode: 696, duration: 10.124s, episode steps: 201, steps per second: 20, episode reward: 138.200, mean reward: 0.688 [-1.000, 1.000], mean action: 3.139 [0.000, 6.000], mean observation: 172.407 [23.000, 255.000], loss: 0.010092, mean_absolute_error: 3.786570, mean_q: 4.411709, mean_eps: 0.881515\n",
      "  131923/2000000: episode: 697, duration: 8.264s, episode steps: 172, steps per second: 21, episode reward: 68.100, mean reward: 0.396 [-1.000, 0.500], mean action: 2.884 [0.000, 6.000], mean observation: 172.348 [24.000, 255.000], loss: 0.011780, mean_absolute_error: 3.916291, mean_q: 4.573732, mean_eps: 0.881348\n",
      "  132102/2000000: episode: 698, duration: 8.873s, episode steps: 179, steps per second: 20, episode reward: 89.200, mean reward: 0.498 [-1.000, 1.000], mean action: 2.771 [0.000, 6.000], mean observation: 172.141 [23.000, 255.000], loss: 0.010175, mean_absolute_error: 3.898939, mean_q: 4.536319, mean_eps: 0.881189\n",
      "  132290/2000000: episode: 699, duration: 9.327s, episode steps: 188, steps per second: 20, episode reward: 107.600, mean reward: 0.572 [-1.000, 1.000], mean action: 2.840 [0.000, 6.000], mean observation: 171.641 [24.000, 255.000], loss: 0.011990, mean_absolute_error: 3.855876, mean_q: 4.502273, mean_eps: 0.881024\n",
      "  132497/2000000: episode: 700, duration: 10.446s, episode steps: 207, steps per second: 20, episode reward: 102.900, mean reward: 0.497 [-1.000, 1.000], mean action: 3.058 [0.000, 6.000], mean observation: 173.609 [23.000, 255.000], loss: 0.009528, mean_absolute_error: 3.875107, mean_q: 4.515092, mean_eps: 0.880845\n",
      "  132682/2000000: episode: 701, duration: 9.060s, episode steps: 185, steps per second: 20, episode reward: 72.600, mean reward: 0.392 [-1.000, 0.500], mean action: 3.027 [0.000, 6.000], mean observation: 172.582 [23.000, 255.000], loss: 0.010679, mean_absolute_error: 3.793538, mean_q: 4.405605, mean_eps: 0.880669\n",
      "  132862/2000000: episode: 702, duration: 8.672s, episode steps: 180, steps per second: 21, episode reward: 91.900, mean reward: 0.511 [-1.000, 1.000], mean action: 2.861 [0.000, 6.000], mean observation: 172.017 [24.000, 255.000], loss: 0.011772, mean_absolute_error: 3.829046, mean_q: 4.460609, mean_eps: 0.880505\n",
      "  133032/2000000: episode: 703, duration: 8.138s, episode steps: 170, steps per second: 21, episode reward: 82.800, mean reward: 0.487 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 171.892 [24.000, 255.000], loss: 0.009748, mean_absolute_error: 3.815787, mean_q: 4.433895, mean_eps: 0.880349\n",
      "  133185/2000000: episode: 704, duration: 7.213s, episode steps: 153, steps per second: 21, episode reward: 55.400, mean reward: 0.362 [-1.000, 0.500], mean action: 2.765 [0.000, 6.000], mean observation: 172.827 [23.000, 255.000], loss: 0.010041, mean_absolute_error: 3.732624, mean_q: 4.347913, mean_eps: 0.880203\n",
      "  133398/2000000: episode: 705, duration: 10.794s, episode steps: 213, steps per second: 20, episode reward: 106.800, mean reward: 0.501 [-1.000, 1.000], mean action: 3.202 [0.000, 6.000], mean observation: 173.607 [24.000, 255.000], loss: 0.011232, mean_absolute_error: 3.756229, mean_q: 4.372363, mean_eps: 0.880037\n",
      "  133621/2000000: episode: 706, duration: 11.421s, episode steps: 223, steps per second: 20, episode reward: 114.600, mean reward: 0.514 [-1.000, 1.000], mean action: 3.193 [0.000, 6.000], mean observation: 173.070 [24.000, 255.000], loss: 0.011006, mean_absolute_error: 3.787311, mean_q: 4.423881, mean_eps: 0.879841\n",
      "  133831/2000000: episode: 707, duration: 10.619s, episode steps: 210, steps per second: 20, episode reward: 126.800, mean reward: 0.604 [-1.000, 1.000], mean action: 3.157 [0.000, 6.000], mean observation: 173.404 [24.000, 255.000], loss: 0.010449, mean_absolute_error: 3.718189, mean_q: 4.320352, mean_eps: 0.879647\n",
      "  134024/2000000: episode: 708, duration: 9.645s, episode steps: 193, steps per second: 20, episode reward: 91.900, mean reward: 0.476 [-1.000, 1.000], mean action: 2.943 [0.000, 6.000], mean observation: 173.074 [24.000, 255.000], loss: 0.011098, mean_absolute_error: 3.794844, mean_q: 4.432728, mean_eps: 0.879467\n",
      "  134207/2000000: episode: 709, duration: 8.971s, episode steps: 183, steps per second: 20, episode reward: 105.200, mean reward: 0.575 [-1.000, 1.000], mean action: 3.038 [0.000, 6.000], mean observation: 172.244 [24.000, 255.000], loss: 0.013355, mean_absolute_error: 3.725116, mean_q: 4.315601, mean_eps: 0.879297\n",
      "  134347/2000000: episode: 710, duration: 6.526s, episode steps: 140, steps per second: 21, episode reward: 51.300, mean reward: 0.366 [-1.000, 0.500], mean action: 2.686 [0.000, 6.000], mean observation: 172.904 [24.000, 255.000], loss: 0.011369, mean_absolute_error: 3.811815, mean_q: 4.441599, mean_eps: 0.879152\n",
      "  134569/2000000: episode: 711, duration: 11.380s, episode steps: 222, steps per second: 20, episode reward: 162.300, mean reward: 0.731 [-1.000, 1.000], mean action: 3.149 [0.000, 6.000], mean observation: 172.900 [24.000, 255.000], loss: 0.010091, mean_absolute_error: 3.856915, mean_q: 4.496249, mean_eps: 0.878988\n",
      "  134765/2000000: episode: 712, duration: 9.847s, episode steps: 196, steps per second: 20, episode reward: 136.000, mean reward: 0.694 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 172.459 [23.000, 255.000], loss: 0.010794, mean_absolute_error: 3.766062, mean_q: 4.376836, mean_eps: 0.878799\n",
      "  134951/2000000: episode: 713, duration: 9.130s, episode steps: 186, steps per second: 20, episode reward: 114.800, mean reward: 0.617 [-1.000, 1.000], mean action: 2.715 [0.000, 6.000], mean observation: 172.232 [21.000, 255.000], loss: 0.010257, mean_absolute_error: 3.826485, mean_q: 4.451885, mean_eps: 0.878628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  135162/2000000: episode: 714, duration: 10.960s, episode steps: 211, steps per second: 19, episode reward: 132.600, mean reward: 0.628 [-1.000, 1.000], mean action: 3.204 [0.000, 6.000], mean observation: 173.037 [24.000, 255.000], loss: 0.012328, mean_absolute_error: 3.752980, mean_q: 4.382629, mean_eps: 0.878450\n",
      "  135328/2000000: episode: 715, duration: 7.935s, episode steps: 166, steps per second: 21, episode reward: 66.700, mean reward: 0.402 [-1.000, 0.500], mean action: 2.940 [0.000, 6.000], mean observation: 172.357 [24.000, 255.000], loss: 0.011575, mean_absolute_error: 3.791648, mean_q: 4.411172, mean_eps: 0.878280\n",
      "  135502/2000000: episode: 716, duration: 8.190s, episode steps: 174, steps per second: 21, episode reward: 56.300, mean reward: 0.324 [-1.000, 0.500], mean action: 2.649 [0.000, 6.000], mean observation: 173.012 [23.000, 255.000], loss: 0.010388, mean_absolute_error: 3.836309, mean_q: 4.482739, mean_eps: 0.878127\n",
      "  135639/2000000: episode: 717, duration: 6.451s, episode steps: 137, steps per second: 21, episode reward: 49.800, mean reward: 0.364 [-1.000, 0.500], mean action: 2.861 [0.000, 6.000], mean observation: 172.939 [24.000, 255.000], loss: 0.011988, mean_absolute_error: 3.828742, mean_q: 4.454653, mean_eps: 0.877987\n",
      "  135798/2000000: episode: 718, duration: 7.599s, episode steps: 159, steps per second: 21, episode reward: 60.400, mean reward: 0.380 [-1.000, 0.500], mean action: 3.038 [0.000, 6.000], mean observation: 172.455 [25.000, 255.000], loss: 0.009573, mean_absolute_error: 3.851359, mean_q: 4.492440, mean_eps: 0.877854\n",
      "  135989/2000000: episode: 719, duration: 9.493s, episode steps: 191, steps per second: 20, episode reward: 86.800, mean reward: 0.454 [-1.000, 0.500], mean action: 3.241 [0.000, 6.000], mean observation: 172.674 [23.000, 255.000], loss: 0.010239, mean_absolute_error: 3.838577, mean_q: 4.460289, mean_eps: 0.877695\n",
      "  136177/2000000: episode: 720, duration: 9.368s, episode steps: 188, steps per second: 20, episode reward: 117.500, mean reward: 0.625 [-1.000, 1.000], mean action: 2.920 [0.000, 6.000], mean observation: 172.094 [24.000, 255.000], loss: 0.010470, mean_absolute_error: 3.842025, mean_q: 4.469909, mean_eps: 0.877524\n",
      "  136350/2000000: episode: 721, duration: 8.280s, episode steps: 173, steps per second: 21, episode reward: 70.600, mean reward: 0.408 [-1.000, 0.500], mean action: 2.908 [0.000, 6.000], mean observation: 172.247 [24.000, 255.000], loss: 0.009810, mean_absolute_error: 3.799126, mean_q: 4.432246, mean_eps: 0.877362\n",
      "  136494/2000000: episode: 722, duration: 6.767s, episode steps: 144, steps per second: 21, episode reward: 54.500, mean reward: 0.378 [-1.000, 0.500], mean action: 2.764 [0.000, 6.000], mean observation: 172.655 [23.000, 255.000], loss: 0.008951, mean_absolute_error: 3.823281, mean_q: 4.460897, mean_eps: 0.877220\n",
      "  136640/2000000: episode: 723, duration: 7.736s, episode steps: 146, steps per second: 19, episode reward: 80.200, mean reward: 0.549 [-1.000, 1.000], mean action: 3.219 [0.000, 6.000], mean observation: 172.756 [24.000, 255.000], loss: 0.009554, mean_absolute_error: 3.711941, mean_q: 4.323401, mean_eps: 0.877091\n",
      "  136821/2000000: episode: 724, duration: 8.792s, episode steps: 181, steps per second: 21, episode reward: 73.400, mean reward: 0.406 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 172.565 [23.000, 255.000], loss: 0.010907, mean_absolute_error: 3.738389, mean_q: 4.359520, mean_eps: 0.876943\n",
      "  137026/2000000: episode: 725, duration: 10.362s, episode steps: 205, steps per second: 20, episode reward: 136.500, mean reward: 0.666 [-1.000, 1.000], mean action: 3.068 [0.000, 6.000], mean observation: 173.348 [24.000, 255.000], loss: 0.010799, mean_absolute_error: 3.794631, mean_q: 4.411296, mean_eps: 0.876768\n",
      "  137215/2000000: episode: 726, duration: 9.299s, episode steps: 189, steps per second: 20, episode reward: 121.400, mean reward: 0.642 [-1.000, 1.000], mean action: 2.868 [0.000, 6.000], mean observation: 171.819 [24.000, 255.000], loss: 0.010935, mean_absolute_error: 3.788738, mean_q: 4.419954, mean_eps: 0.876592\n",
      "  137402/2000000: episode: 727, duration: 9.190s, episode steps: 187, steps per second: 20, episode reward: 78.000, mean reward: 0.417 [-1.000, 0.500], mean action: 3.091 [0.000, 6.000], mean observation: 172.546 [23.000, 255.000], loss: 0.010663, mean_absolute_error: 3.796551, mean_q: 4.420704, mean_eps: 0.876423\n",
      "  137617/2000000: episode: 728, duration: 10.986s, episode steps: 215, steps per second: 20, episode reward: 128.300, mean reward: 0.597 [-1.000, 1.000], mean action: 3.098 [0.000, 6.000], mean observation: 173.316 [23.000, 255.000], loss: 0.009449, mean_absolute_error: 3.689121, mean_q: 4.296858, mean_eps: 0.876241\n",
      "  137798/2000000: episode: 729, duration: 8.825s, episode steps: 181, steps per second: 21, episode reward: 64.600, mean reward: 0.357 [-1.000, 0.500], mean action: 3.055 [0.000, 6.000], mean observation: 172.671 [24.000, 255.000], loss: 0.009100, mean_absolute_error: 3.711091, mean_q: 4.309637, mean_eps: 0.876063\n",
      "  138033/2000000: episode: 730, duration: 12.119s, episode steps: 235, steps per second: 19, episode reward: 104.200, mean reward: 0.443 [-1.000, 1.000], mean action: 3.174 [0.000, 6.000], mean observation: 173.709 [24.000, 255.000], loss: 0.010094, mean_absolute_error: 3.819876, mean_q: 4.449981, mean_eps: 0.875876\n",
      "  138169/2000000: episode: 731, duration: 7.189s, episode steps: 136, steps per second: 19, episode reward: 73.700, mean reward: 0.542 [-1.000, 1.000], mean action: 3.287 [0.000, 6.000], mean observation: 173.443 [23.000, 255.000], loss: 0.009959, mean_absolute_error: 3.745139, mean_q: 4.345874, mean_eps: 0.875708\n",
      "  138350/2000000: episode: 732, duration: 8.693s, episode steps: 181, steps per second: 21, episode reward: 70.200, mean reward: 0.388 [-1.000, 0.500], mean action: 3.039 [0.000, 6.000], mean observation: 172.371 [24.000, 255.000], loss: 0.009162, mean_absolute_error: 3.713600, mean_q: 4.322445, mean_eps: 0.875566\n",
      "  138548/2000000: episode: 733, duration: 10.038s, episode steps: 198, steps per second: 20, episode reward: 140.500, mean reward: 0.710 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 172.036 [24.000, 255.000], loss: 0.010620, mean_absolute_error: 3.820664, mean_q: 4.460301, mean_eps: 0.875397\n",
      "  138736/2000000: episode: 734, duration: 9.407s, episode steps: 188, steps per second: 20, episode reward: 124.500, mean reward: 0.662 [-1.000, 1.000], mean action: 2.952 [0.000, 6.000], mean observation: 171.517 [24.000, 255.000], loss: 0.009395, mean_absolute_error: 3.751003, mean_q: 4.362405, mean_eps: 0.875224\n",
      "  138853/2000000: episode: 735, duration: 5.402s, episode steps: 117, steps per second: 22, episode reward: 42.200, mean reward: 0.361 [-1.000, 0.500], mean action: 2.513 [0.000, 6.000], mean observation: 172.500 [24.000, 255.000], loss: 0.009196, mean_absolute_error: 3.791484, mean_q: 4.414146, mean_eps: 0.875085\n",
      "  139066/2000000: episode: 736, duration: 10.871s, episode steps: 213, steps per second: 20, episode reward: 131.600, mean reward: 0.618 [-1.000, 1.000], mean action: 3.131 [0.000, 6.000], mean observation: 172.569 [24.000, 255.000], loss: 0.010673, mean_absolute_error: 3.764534, mean_q: 4.382289, mean_eps: 0.874936\n",
      "  139269/2000000: episode: 737, duration: 10.151s, episode steps: 203, steps per second: 20, episode reward: 102.600, mean reward: 0.505 [-1.000, 1.000], mean action: 2.892 [0.000, 6.000], mean observation: 172.362 [24.000, 255.000], loss: 0.009709, mean_absolute_error: 3.708112, mean_q: 4.322488, mean_eps: 0.874749\n",
      "  139428/2000000: episode: 738, duration: 7.720s, episode steps: 159, steps per second: 21, episode reward: 67.600, mean reward: 0.425 [-1.000, 0.500], mean action: 2.805 [0.000, 6.000], mean observation: 171.663 [23.000, 255.000], loss: 0.010620, mean_absolute_error: 3.825042, mean_q: 4.451490, mean_eps: 0.874587\n",
      "  139560/2000000: episode: 739, duration: 6.178s, episode steps: 132, steps per second: 21, episode reward: 46.100, mean reward: 0.349 [-1.000, 0.500], mean action: 2.977 [0.000, 6.000], mean observation: 172.541 [24.000, 255.000], loss: 0.008888, mean_absolute_error: 3.734358, mean_q: 4.357138, mean_eps: 0.874457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  139727/2000000: episode: 740, duration: 7.986s, episode steps: 167, steps per second: 21, episode reward: 70.000, mean reward: 0.419 [-1.000, 0.500], mean action: 2.892 [0.000, 6.000], mean observation: 171.324 [23.000, 255.000], loss: 0.009416, mean_absolute_error: 3.674945, mean_q: 4.276043, mean_eps: 0.874322\n",
      "  139916/2000000: episode: 741, duration: 9.284s, episode steps: 189, steps per second: 20, episode reward: 110.000, mean reward: 0.582 [-1.000, 1.000], mean action: 2.931 [0.000, 6.000], mean observation: 171.347 [24.000, 255.000], loss: 0.008847, mean_absolute_error: 3.663839, mean_q: 4.255414, mean_eps: 0.874162\n",
      "  140096/2000000: episode: 742, duration: 8.692s, episode steps: 180, steps per second: 21, episode reward: 72.100, mean reward: 0.401 [-1.000, 0.500], mean action: 2.922 [0.000, 6.000], mean observation: 172.200 [24.000, 255.000], loss: 0.036197, mean_absolute_error: 3.989647, mean_q: 4.635869, mean_eps: 0.873996\n",
      "  140250/2000000: episode: 743, duration: 7.233s, episode steps: 154, steps per second: 21, episode reward: 55.100, mean reward: 0.358 [-1.000, 0.500], mean action: 2.773 [0.000, 6.000], mean observation: 172.188 [24.000, 255.000], loss: 0.021755, mean_absolute_error: 4.165710, mean_q: 4.877031, mean_eps: 0.873845\n",
      "  140473/2000000: episode: 744, duration: 11.417s, episode steps: 223, steps per second: 20, episode reward: 160.300, mean reward: 0.719 [-1.000, 1.000], mean action: 3.135 [0.000, 6.000], mean observation: 172.210 [24.000, 255.000], loss: 0.014448, mean_absolute_error: 4.219455, mean_q: 4.931231, mean_eps: 0.873674\n",
      "  140608/2000000: episode: 745, duration: 6.322s, episode steps: 135, steps per second: 21, episode reward: 45.200, mean reward: 0.335 [-1.000, 0.500], mean action: 2.659 [0.000, 6.000], mean observation: 172.688 [24.000, 255.000], loss: 0.012928, mean_absolute_error: 4.018687, mean_q: 4.678419, mean_eps: 0.873514\n",
      "  140830/2000000: episode: 746, duration: 11.538s, episode steps: 222, steps per second: 19, episode reward: 104.000, mean reward: 0.468 [-1.000, 1.000], mean action: 3.104 [0.000, 6.000], mean observation: 173.445 [23.000, 255.000], loss: 0.014406, mean_absolute_error: 4.099196, mean_q: 4.791591, mean_eps: 0.873354\n",
      "  140985/2000000: episode: 747, duration: 7.317s, episode steps: 155, steps per second: 21, episode reward: 55.200, mean reward: 0.356 [-1.000, 0.500], mean action: 3.052 [0.000, 6.000], mean observation: 172.212 [23.000, 255.000], loss: 0.013911, mean_absolute_error: 4.215951, mean_q: 4.909568, mean_eps: 0.873183\n",
      "  141207/2000000: episode: 748, duration: 11.429s, episode steps: 222, steps per second: 19, episode reward: 118.700, mean reward: 0.535 [-1.000, 1.000], mean action: 3.257 [0.000, 6.000], mean observation: 172.630 [23.000, 255.000], loss: 0.013041, mean_absolute_error: 4.144035, mean_q: 4.834411, mean_eps: 0.873014\n",
      "  141418/2000000: episode: 749, duration: 10.679s, episode steps: 211, steps per second: 20, episode reward: 105.200, mean reward: 0.499 [-1.000, 1.000], mean action: 3.014 [0.000, 6.000], mean observation: 173.111 [24.000, 255.000], loss: 0.012676, mean_absolute_error: 4.102381, mean_q: 4.777288, mean_eps: 0.872819\n",
      "  141626/2000000: episode: 750, duration: 10.412s, episode steps: 208, steps per second: 20, episode reward: 91.300, mean reward: 0.439 [-1.000, 0.500], mean action: 3.077 [0.000, 6.000], mean observation: 173.026 [24.000, 255.000], loss: 0.013156, mean_absolute_error: 4.071930, mean_q: 4.749856, mean_eps: 0.872630\n",
      "  141811/2000000: episode: 751, duration: 9.131s, episode steps: 185, steps per second: 20, episode reward: 128.200, mean reward: 0.693 [-1.000, 1.000], mean action: 3.065 [0.000, 6.000], mean observation: 170.875 [24.000, 255.000], loss: 0.014331, mean_absolute_error: 4.074815, mean_q: 4.755315, mean_eps: 0.872454\n",
      "  141994/2000000: episode: 752, duration: 8.943s, episode steps: 183, steps per second: 20, episode reward: 114.600, mean reward: 0.626 [-1.000, 1.000], mean action: 3.120 [0.000, 6.000], mean observation: 171.222 [24.000, 255.000], loss: 0.014034, mean_absolute_error: 4.181931, mean_q: 4.886923, mean_eps: 0.872288\n",
      "  142187/2000000: episode: 753, duration: 9.541s, episode steps: 193, steps per second: 20, episode reward: 88.200, mean reward: 0.457 [-1.000, 0.500], mean action: 3.005 [0.000, 6.000], mean observation: 172.543 [23.000, 255.000], loss: 0.014243, mean_absolute_error: 4.232978, mean_q: 4.940951, mean_eps: 0.872119\n",
      "  142342/2000000: episode: 754, duration: 7.381s, episode steps: 155, steps per second: 21, episode reward: 61.200, mean reward: 0.395 [-1.000, 0.500], mean action: 2.806 [0.000, 6.000], mean observation: 172.210 [24.000, 255.000], loss: 0.013563, mean_absolute_error: 4.201300, mean_q: 4.884282, mean_eps: 0.871962\n",
      "  142518/2000000: episode: 755, duration: 8.454s, episode steps: 176, steps per second: 21, episode reward: 70.100, mean reward: 0.398 [-1.000, 0.500], mean action: 3.102 [0.000, 6.000], mean observation: 172.612 [22.000, 255.000], loss: 0.013252, mean_absolute_error: 4.174617, mean_q: 4.850449, mean_eps: 0.871813\n",
      "  142709/2000000: episode: 756, duration: 9.388s, episode steps: 191, steps per second: 20, episode reward: 71.200, mean reward: 0.373 [-1.000, 0.500], mean action: 3.110 [0.000, 6.000], mean observation: 172.367 [23.000, 255.000], loss: 0.013043, mean_absolute_error: 4.214053, mean_q: 4.921985, mean_eps: 0.871647\n",
      "  142916/2000000: episode: 757, duration: 10.542s, episode steps: 207, steps per second: 20, episode reward: 144.100, mean reward: 0.696 [-1.000, 1.000], mean action: 3.193 [0.000, 6.000], mean observation: 172.655 [24.000, 255.000], loss: 0.012457, mean_absolute_error: 4.197871, mean_q: 4.904653, mean_eps: 0.871469\n",
      "  143066/2000000: episode: 758, duration: 7.953s, episode steps: 150, steps per second: 19, episode reward: 78.900, mean reward: 0.526 [-1.000, 1.000], mean action: 3.333 [0.000, 6.000], mean observation: 173.695 [23.000, 255.000], loss: 0.012502, mean_absolute_error: 4.196485, mean_q: 4.886813, mean_eps: 0.871309\n",
      "  143245/2000000: episode: 759, duration: 8.529s, episode steps: 179, steps per second: 21, episode reward: 53.200, mean reward: 0.297 [-1.000, 0.500], mean action: 2.771 [0.000, 6.000], mean observation: 173.208 [23.000, 255.000], loss: 0.011398, mean_absolute_error: 4.149980, mean_q: 4.840024, mean_eps: 0.871160\n",
      "  143465/2000000: episode: 760, duration: 11.210s, episode steps: 220, steps per second: 20, episode reward: 128.700, mean reward: 0.585 [-1.000, 1.000], mean action: 3.132 [0.000, 6.000], mean observation: 173.505 [23.000, 255.000], loss: 0.013466, mean_absolute_error: 4.197699, mean_q: 4.894068, mean_eps: 0.870980\n",
      "  143633/2000000: episode: 761, duration: 8.093s, episode steps: 168, steps per second: 21, episode reward: 70.100, mean reward: 0.417 [-1.000, 0.500], mean action: 2.798 [0.000, 6.000], mean observation: 172.522 [23.000, 255.000], loss: 0.011786, mean_absolute_error: 4.160155, mean_q: 4.847784, mean_eps: 0.870805\n",
      "  143866/2000000: episode: 762, duration: 11.987s, episode steps: 233, steps per second: 19, episode reward: 127.700, mean reward: 0.548 [-1.000, 1.000], mean action: 3.129 [0.000, 6.000], mean observation: 173.537 [23.000, 255.000], loss: 0.014307, mean_absolute_error: 4.161912, mean_q: 4.870277, mean_eps: 0.870625\n",
      "  144051/2000000: episode: 763, duration: 9.155s, episode steps: 185, steps per second: 20, episode reward: 92.400, mean reward: 0.499 [-1.000, 1.000], mean action: 3.178 [0.000, 6.000], mean observation: 172.003 [23.000, 255.000], loss: 0.011651, mean_absolute_error: 4.090055, mean_q: 4.766084, mean_eps: 0.870438\n",
      "  144237/2000000: episode: 764, duration: 9.251s, episode steps: 186, steps per second: 20, episode reward: 77.900, mean reward: 0.419 [-1.000, 0.500], mean action: 2.957 [0.000, 6.000], mean observation: 172.806 [25.000, 255.000], loss: 0.012709, mean_absolute_error: 4.192622, mean_q: 4.891292, mean_eps: 0.870270\n",
      "  144455/2000000: episode: 765, duration: 11.208s, episode steps: 218, steps per second: 19, episode reward: 108.200, mean reward: 0.496 [-1.000, 1.000], mean action: 3.092 [0.000, 6.000], mean observation: 173.796 [23.000, 255.000], loss: 0.014127, mean_absolute_error: 4.155265, mean_q: 4.833518, mean_eps: 0.870089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  144644/2000000: episode: 766, duration: 9.395s, episode steps: 189, steps per second: 20, episode reward: 135.300, mean reward: 0.716 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 171.947 [23.000, 255.000], loss: 0.012417, mean_absolute_error: 4.132474, mean_q: 4.827106, mean_eps: 0.869907\n",
      "  144857/2000000: episode: 767, duration: 10.798s, episode steps: 213, steps per second: 20, episode reward: 155.700, mean reward: 0.731 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 173.661 [24.000, 255.000], loss: 0.011571, mean_absolute_error: 4.254891, mean_q: 4.965096, mean_eps: 0.869725\n",
      "  145049/2000000: episode: 768, duration: 9.573s, episode steps: 192, steps per second: 20, episode reward: 118.500, mean reward: 0.617 [-1.000, 1.000], mean action: 2.969 [0.000, 6.000], mean observation: 172.688 [20.000, 255.000], loss: 0.013645, mean_absolute_error: 4.097077, mean_q: 4.768700, mean_eps: 0.869541\n",
      "  145242/2000000: episode: 769, duration: 9.637s, episode steps: 193, steps per second: 20, episode reward: 118.600, mean reward: 0.615 [-1.000, 1.000], mean action: 3.078 [0.000, 6.000], mean observation: 172.630 [23.000, 255.000], loss: 0.012346, mean_absolute_error: 4.199574, mean_q: 4.881186, mean_eps: 0.869369\n",
      "  145427/2000000: episode: 770, duration: 9.092s, episode steps: 185, steps per second: 20, episode reward: 131.500, mean reward: 0.711 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 171.785 [23.000, 255.000], loss: 0.012410, mean_absolute_error: 4.176921, mean_q: 4.868699, mean_eps: 0.869199\n",
      "  145614/2000000: episode: 771, duration: 9.193s, episode steps: 187, steps per second: 20, episode reward: 80.000, mean reward: 0.428 [-1.000, 0.500], mean action: 3.118 [0.000, 6.000], mean observation: 172.918 [22.000, 255.000], loss: 0.013359, mean_absolute_error: 4.195911, mean_q: 4.884709, mean_eps: 0.869032\n",
      "  145816/2000000: episode: 772, duration: 10.218s, episode steps: 202, steps per second: 20, episode reward: 132.700, mean reward: 0.657 [-1.000, 1.000], mean action: 3.248 [0.000, 6.000], mean observation: 173.182 [23.000, 255.000], loss: 0.011960, mean_absolute_error: 4.231697, mean_q: 4.928982, mean_eps: 0.868857\n",
      "  145994/2000000: episode: 773, duration: 8.626s, episode steps: 178, steps per second: 21, episode reward: 72.000, mean reward: 0.404 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 172.580 [24.000, 255.000], loss: 0.013151, mean_absolute_error: 4.154867, mean_q: 4.847292, mean_eps: 0.868686\n",
      "  146200/2000000: episode: 774, duration: 10.519s, episode steps: 206, steps per second: 20, episode reward: 136.400, mean reward: 0.662 [-1.000, 1.000], mean action: 3.170 [0.000, 6.000], mean observation: 172.793 [24.000, 255.000], loss: 0.011381, mean_absolute_error: 4.170349, mean_q: 4.846505, mean_eps: 0.868514\n",
      "  146390/2000000: episode: 775, duration: 9.528s, episode steps: 190, steps per second: 20, episode reward: 95.800, mean reward: 0.504 [-1.000, 1.000], mean action: 3.116 [0.000, 6.000], mean observation: 172.652 [24.000, 255.000], loss: 0.014245, mean_absolute_error: 4.331784, mean_q: 5.045113, mean_eps: 0.868335\n",
      "  146562/2000000: episode: 776, duration: 8.360s, episode steps: 172, steps per second: 21, episode reward: 69.300, mean reward: 0.403 [-1.000, 0.500], mean action: 2.924 [0.000, 6.000], mean observation: 172.411 [23.000, 255.000], loss: 0.015412, mean_absolute_error: 4.226876, mean_q: 4.938345, mean_eps: 0.868172\n",
      "  146748/2000000: episode: 777, duration: 9.230s, episode steps: 186, steps per second: 20, episode reward: 102.900, mean reward: 0.553 [-1.000, 1.000], mean action: 2.930 [0.000, 6.000], mean observation: 172.647 [23.000, 255.000], loss: 0.012022, mean_absolute_error: 4.204525, mean_q: 4.908454, mean_eps: 0.868011\n",
      "  146952/2000000: episode: 778, duration: 10.346s, episode steps: 204, steps per second: 20, episode reward: 147.100, mean reward: 0.721 [-1.000, 1.000], mean action: 3.029 [0.000, 6.000], mean observation: 173.057 [23.000, 255.000], loss: 0.012139, mean_absolute_error: 4.137583, mean_q: 4.819040, mean_eps: 0.867837\n",
      "  147151/2000000: episode: 779, duration: 9.922s, episode steps: 199, steps per second: 20, episode reward: 121.500, mean reward: 0.611 [-1.000, 1.000], mean action: 2.910 [0.000, 6.000], mean observation: 173.215 [24.000, 255.000], loss: 0.010399, mean_absolute_error: 4.156881, mean_q: 4.851032, mean_eps: 0.867655\n",
      "  147315/2000000: episode: 780, duration: 7.927s, episode steps: 164, steps per second: 21, episode reward: 81.800, mean reward: 0.499 [-1.000, 1.000], mean action: 2.768 [0.000, 6.000], mean observation: 171.479 [23.000, 255.000], loss: 0.012354, mean_absolute_error: 4.274769, mean_q: 4.989495, mean_eps: 0.867491\n",
      "  147535/2000000: episode: 781, duration: 11.225s, episode steps: 220, steps per second: 20, episode reward: 164.300, mean reward: 0.747 [-1.000, 1.000], mean action: 3.186 [0.000, 6.000], mean observation: 173.130 [23.000, 255.000], loss: 0.011397, mean_absolute_error: 4.139602, mean_q: 4.823205, mean_eps: 0.867318\n",
      "  147743/2000000: episode: 782, duration: 10.578s, episode steps: 208, steps per second: 20, episode reward: 137.300, mean reward: 0.660 [-1.000, 1.000], mean action: 3.024 [0.000, 6.000], mean observation: 172.990 [23.000, 255.000], loss: 0.014119, mean_absolute_error: 4.269377, mean_q: 4.976063, mean_eps: 0.867126\n",
      "  147929/2000000: episode: 783, duration: 8.961s, episode steps: 186, steps per second: 21, episode reward: 55.100, mean reward: 0.296 [-1.000, 0.500], mean action: 3.016 [0.000, 6.000], mean observation: 172.801 [22.000, 255.000], loss: 0.011750, mean_absolute_error: 4.201261, mean_q: 4.920177, mean_eps: 0.866948\n",
      "  148136/2000000: episode: 784, duration: 10.396s, episode steps: 207, steps per second: 20, episode reward: 94.600, mean reward: 0.457 [-1.000, 1.000], mean action: 3.087 [0.000, 6.000], mean observation: 173.193 [24.000, 255.000], loss: 0.014056, mean_absolute_error: 4.124142, mean_q: 4.820116, mean_eps: 0.866771\n",
      "  148275/2000000: episode: 785, duration: 6.560s, episode steps: 139, steps per second: 21, episode reward: 52.000, mean reward: 0.374 [-1.000, 0.500], mean action: 2.719 [0.000, 6.000], mean observation: 172.352 [23.000, 255.000], loss: 0.011258, mean_absolute_error: 4.308795, mean_q: 5.042310, mean_eps: 0.866616\n",
      "  148416/2000000: episode: 786, duration: 6.520s, episode steps: 141, steps per second: 22, episode reward: 47.000, mean reward: 0.333 [-1.000, 0.500], mean action: 2.752 [0.000, 6.000], mean observation: 172.933 [24.000, 255.000], loss: 0.018197, mean_absolute_error: 4.198966, mean_q: 4.893449, mean_eps: 0.866490\n",
      "  148597/2000000: episode: 787, duration: 8.823s, episode steps: 181, steps per second: 21, episode reward: 73.800, mean reward: 0.408 [-1.000, 0.500], mean action: 2.978 [0.000, 6.000], mean observation: 172.290 [21.000, 255.000], loss: 0.012362, mean_absolute_error: 4.152844, mean_q: 4.839357, mean_eps: 0.866345\n",
      "  148783/2000000: episode: 788, duration: 9.202s, episode steps: 186, steps per second: 20, episode reward: 92.100, mean reward: 0.495 [-1.000, 1.000], mean action: 3.032 [0.000, 6.000], mean observation: 172.087 [25.000, 255.000], loss: 0.012996, mean_absolute_error: 4.189251, mean_q: 4.877985, mean_eps: 0.866179\n",
      "  149003/2000000: episode: 789, duration: 11.288s, episode steps: 220, steps per second: 19, episode reward: 122.200, mean reward: 0.555 [-1.000, 1.000], mean action: 3.114 [0.000, 6.000], mean observation: 173.132 [24.000, 255.000], loss: 0.009942, mean_absolute_error: 4.137220, mean_q: 4.822811, mean_eps: 0.865997\n",
      "  149138/2000000: episode: 790, duration: 6.345s, episode steps: 135, steps per second: 21, episode reward: 50.000, mean reward: 0.370 [-1.000, 0.500], mean action: 2.785 [0.000, 6.000], mean observation: 172.692 [24.000, 255.000], loss: 0.011483, mean_absolute_error: 4.172403, mean_q: 4.872165, mean_eps: 0.865837\n",
      "  149349/2000000: episode: 791, duration: 10.781s, episode steps: 211, steps per second: 20, episode reward: 148.100, mean reward: 0.702 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 172.693 [24.000, 255.000], loss: 0.012629, mean_absolute_error: 4.325074, mean_q: 5.052497, mean_eps: 0.865680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  149546/2000000: episode: 792, duration: 9.902s, episode steps: 197, steps per second: 20, episode reward: 125.100, mean reward: 0.635 [-1.000, 1.000], mean action: 3.041 [0.000, 6.000], mean observation: 172.454 [24.000, 255.000], loss: 0.011011, mean_absolute_error: 4.268613, mean_q: 4.977381, mean_eps: 0.865497\n",
      "  149730/2000000: episode: 793, duration: 9.119s, episode steps: 184, steps per second: 20, episode reward: 104.200, mean reward: 0.566 [-1.000, 1.000], mean action: 2.897 [0.000, 6.000], mean observation: 171.852 [23.000, 255.000], loss: 0.012374, mean_absolute_error: 4.192930, mean_q: 4.893481, mean_eps: 0.865326\n",
      "  149855/2000000: episode: 794, duration: 5.864s, episode steps: 125, steps per second: 21, episode reward: 49.000, mean reward: 0.392 [-1.000, 0.500], mean action: 2.576 [0.000, 6.000], mean observation: 172.396 [23.000, 255.000], loss: 0.011615, mean_absolute_error: 4.168934, mean_q: 4.857097, mean_eps: 0.865187\n",
      "  150061/2000000: episode: 795, duration: 10.442s, episode steps: 206, steps per second: 20, episode reward: 148.400, mean reward: 0.720 [-1.000, 1.000], mean action: 3.102 [0.000, 6.000], mean observation: 173.049 [24.000, 255.000], loss: 0.025799, mean_absolute_error: 4.389658, mean_q: 5.115547, mean_eps: 0.865038\n",
      "  150194/2000000: episode: 796, duration: 6.168s, episode steps: 133, steps per second: 22, episode reward: 47.800, mean reward: 0.359 [-1.000, 0.500], mean action: 2.556 [0.000, 6.000], mean observation: 173.097 [23.000, 255.000], loss: 0.026319, mean_absolute_error: 4.617420, mean_q: 5.410555, mean_eps: 0.864885\n",
      "  150403/2000000: episode: 797, duration: 10.686s, episode steps: 209, steps per second: 20, episode reward: 146.000, mean reward: 0.699 [-1.000, 1.000], mean action: 3.273 [0.000, 6.000], mean observation: 172.851 [24.000, 255.000], loss: 0.018956, mean_absolute_error: 4.645576, mean_q: 5.437412, mean_eps: 0.864732\n",
      "  150582/2000000: episode: 798, duration: 8.718s, episode steps: 179, steps per second: 21, episode reward: 102.600, mean reward: 0.573 [-1.000, 1.000], mean action: 3.006 [0.000, 6.000], mean observation: 171.879 [24.000, 255.000], loss: 0.015877, mean_absolute_error: 4.576666, mean_q: 5.334630, mean_eps: 0.864557\n",
      "  150788/2000000: episode: 799, duration: 10.368s, episode steps: 206, steps per second: 20, episode reward: 141.000, mean reward: 0.684 [-1.000, 1.000], mean action: 2.981 [0.000, 6.000], mean observation: 172.749 [23.000, 255.000], loss: 0.017080, mean_absolute_error: 4.674681, mean_q: 5.465830, mean_eps: 0.864384\n",
      "  150983/2000000: episode: 800, duration: 9.189s, episode steps: 195, steps per second: 21, episode reward: 100.300, mean reward: 0.514 [-1.000, 1.000], mean action: 2.815 [0.000, 6.000], mean observation: 172.173 [23.000, 255.000], loss: 0.016730, mean_absolute_error: 4.718846, mean_q: 5.515227, mean_eps: 0.864204\n",
      "  151187/2000000: episode: 801, duration: 10.604s, episode steps: 204, steps per second: 19, episode reward: 140.200, mean reward: 0.687 [-1.000, 1.000], mean action: 3.015 [0.000, 6.000], mean observation: 172.697 [24.000, 255.000], loss: 0.014930, mean_absolute_error: 4.688860, mean_q: 5.477249, mean_eps: 0.864024\n",
      "  151363/2000000: episode: 802, duration: 8.748s, episode steps: 176, steps per second: 20, episode reward: 72.500, mean reward: 0.412 [-1.000, 0.500], mean action: 2.858 [0.000, 6.000], mean observation: 172.385 [24.000, 255.000], loss: 0.014098, mean_absolute_error: 4.461211, mean_q: 5.200451, mean_eps: 0.863853\n",
      "  151584/2000000: episode: 803, duration: 11.658s, episode steps: 221, steps per second: 19, episode reward: 122.100, mean reward: 0.552 [-1.000, 1.000], mean action: 3.181 [0.000, 6.000], mean observation: 173.113 [23.000, 255.000], loss: 0.016828, mean_absolute_error: 4.634514, mean_q: 5.396892, mean_eps: 0.863675\n",
      "  151815/2000000: episode: 804, duration: 12.230s, episode steps: 231, steps per second: 19, episode reward: 121.400, mean reward: 0.526 [-1.000, 1.000], mean action: 3.186 [0.000, 6.000], mean observation: 173.118 [23.000, 255.000], loss: 0.017911, mean_absolute_error: 4.522500, mean_q: 5.273800, mean_eps: 0.863472\n",
      "  152001/2000000: episode: 805, duration: 9.457s, episode steps: 186, steps per second: 20, episode reward: 92.200, mean reward: 0.496 [-1.000, 1.000], mean action: 3.048 [0.000, 6.000], mean observation: 171.741 [24.000, 255.000], loss: 0.016864, mean_absolute_error: 4.632178, mean_q: 5.413549, mean_eps: 0.863283\n",
      "  152161/2000000: episode: 806, duration: 7.872s, episode steps: 160, steps per second: 20, episode reward: 67.700, mean reward: 0.423 [-1.000, 0.500], mean action: 2.769 [0.000, 6.000], mean observation: 171.413 [23.000, 255.000], loss: 0.016980, mean_absolute_error: 4.722293, mean_q: 5.530377, mean_eps: 0.863126\n",
      "  152350/2000000: episode: 807, duration: 9.580s, episode steps: 189, steps per second: 20, episode reward: 133.400, mean reward: 0.706 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 171.304 [23.000, 255.000], loss: 0.016319, mean_absolute_error: 4.563952, mean_q: 5.346460, mean_eps: 0.862970\n",
      "  152522/2000000: episode: 808, duration: 8.365s, episode steps: 172, steps per second: 21, episode reward: 70.100, mean reward: 0.408 [-1.000, 0.500], mean action: 2.843 [0.000, 6.000], mean observation: 171.876 [23.000, 255.000], loss: 0.015264, mean_absolute_error: 4.617408, mean_q: 5.421491, mean_eps: 0.862808\n",
      "  152736/2000000: episode: 809, duration: 11.289s, episode steps: 214, steps per second: 19, episode reward: 122.100, mean reward: 0.571 [-1.000, 1.000], mean action: 3.154 [0.000, 6.000], mean observation: 172.457 [22.000, 255.000], loss: 0.016202, mean_absolute_error: 4.623653, mean_q: 5.396881, mean_eps: 0.862635\n",
      "  152884/2000000: episode: 810, duration: 7.976s, episode steps: 148, steps per second: 19, episode reward: 74.700, mean reward: 0.505 [-1.000, 1.000], mean action: 3.486 [0.000, 6.000], mean observation: 171.894 [24.000, 255.000], loss: 0.017493, mean_absolute_error: 4.586016, mean_q: 5.357833, mean_eps: 0.862473\n",
      "  153093/2000000: episode: 811, duration: 10.868s, episode steps: 209, steps per second: 19, episode reward: 126.100, mean reward: 0.603 [-1.000, 1.000], mean action: 3.115 [0.000, 6.000], mean observation: 173.006 [24.000, 255.000], loss: 0.015088, mean_absolute_error: 4.610685, mean_q: 5.384576, mean_eps: 0.862311\n",
      "  153270/2000000: episode: 812, duration: 8.561s, episode steps: 177, steps per second: 21, episode reward: 61.400, mean reward: 0.347 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 172.553 [24.000, 255.000], loss: 0.013884, mean_absolute_error: 4.659592, mean_q: 5.438739, mean_eps: 0.862136\n",
      "  153454/2000000: episode: 813, duration: 9.365s, episode steps: 184, steps per second: 20, episode reward: 74.700, mean reward: 0.406 [-1.000, 1.000], mean action: 3.239 [0.000, 6.000], mean observation: 171.577 [23.000, 255.000], loss: 0.013785, mean_absolute_error: 4.634203, mean_q: 5.403922, mean_eps: 0.861974\n",
      "  153642/2000000: episode: 814, duration: 9.601s, episode steps: 188, steps per second: 20, episode reward: 140.900, mean reward: 0.749 [-1.000, 1.000], mean action: 3.122 [0.000, 6.000], mean observation: 171.148 [24.000, 255.000], loss: 0.015937, mean_absolute_error: 4.635628, mean_q: 5.415435, mean_eps: 0.861807\n",
      "  153828/2000000: episode: 815, duration: 9.457s, episode steps: 186, steps per second: 20, episode reward: 75.100, mean reward: 0.404 [-1.000, 0.500], mean action: 3.065 [0.000, 6.000], mean observation: 171.639 [23.000, 255.000], loss: 0.019328, mean_absolute_error: 4.556311, mean_q: 5.319564, mean_eps: 0.861639\n",
      "  154007/2000000: episode: 816, duration: 8.949s, episode steps: 179, steps per second: 20, episode reward: 118.300, mean reward: 0.661 [-1.000, 1.000], mean action: 2.721 [0.000, 6.000], mean observation: 170.542 [24.000, 255.000], loss: 0.013549, mean_absolute_error: 4.573969, mean_q: 5.334738, mean_eps: 0.861476\n",
      "  154187/2000000: episode: 817, duration: 8.757s, episode steps: 180, steps per second: 21, episode reward: 73.700, mean reward: 0.409 [-1.000, 0.500], mean action: 2.872 [0.000, 6.000], mean observation: 171.972 [22.000, 255.000], loss: 0.015499, mean_absolute_error: 4.711415, mean_q: 5.497694, mean_eps: 0.861314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  154402/2000000: episode: 818, duration: 11.265s, episode steps: 215, steps per second: 19, episode reward: 157.200, mean reward: 0.731 [-1.000, 1.000], mean action: 3.172 [0.000, 6.000], mean observation: 171.377 [24.000, 255.000], loss: 0.013502, mean_absolute_error: 4.504372, mean_q: 5.254290, mean_eps: 0.861135\n",
      "  154558/2000000: episode: 819, duration: 7.618s, episode steps: 156, steps per second: 20, episode reward: 66.900, mean reward: 0.429 [-1.000, 0.500], mean action: 2.654 [0.000, 6.000], mean observation: 171.140 [23.000, 255.000], loss: 0.013057, mean_absolute_error: 4.657640, mean_q: 5.430872, mean_eps: 0.860968\n",
      "  154760/2000000: episode: 820, duration: 10.446s, episode steps: 202, steps per second: 19, episode reward: 128.300, mean reward: 0.635 [-1.000, 1.000], mean action: 3.277 [0.000, 6.000], mean observation: 172.010 [24.000, 255.000], loss: 0.014533, mean_absolute_error: 4.531280, mean_q: 5.294454, mean_eps: 0.860808\n",
      "  154915/2000000: episode: 821, duration: 7.506s, episode steps: 155, steps per second: 21, episode reward: 59.600, mean reward: 0.385 [-1.000, 0.500], mean action: 2.903 [0.000, 6.000], mean observation: 171.487 [25.000, 255.000], loss: 0.015069, mean_absolute_error: 4.629383, mean_q: 5.411108, mean_eps: 0.860648\n",
      "  155146/2000000: episode: 822, duration: 12.183s, episode steps: 231, steps per second: 19, episode reward: 113.500, mean reward: 0.491 [-1.000, 1.000], mean action: 3.195 [0.000, 6.000], mean observation: 172.811 [24.000, 255.000], loss: 0.015998, mean_absolute_error: 4.527107, mean_q: 5.281428, mean_eps: 0.860473\n",
      "  155356/2000000: episode: 823, duration: 10.940s, episode steps: 210, steps per second: 19, episode reward: 101.600, mean reward: 0.484 [-1.000, 1.000], mean action: 3.162 [0.000, 6.000], mean observation: 172.864 [23.000, 255.000], loss: 0.016634, mean_absolute_error: 4.597779, mean_q: 5.379582, mean_eps: 0.860275\n",
      "  155563/2000000: episode: 824, duration: 10.476s, episode steps: 207, steps per second: 20, episode reward: 74.000, mean reward: 0.357 [-1.000, 0.500], mean action: 3.362 [0.000, 6.000], mean observation: 172.027 [23.000, 255.000], loss: 0.014140, mean_absolute_error: 4.603130, mean_q: 5.353452, mean_eps: 0.860088\n",
      "  155723/2000000: episode: 825, duration: 7.839s, episode steps: 160, steps per second: 20, episode reward: 63.300, mean reward: 0.396 [-1.000, 0.500], mean action: 2.894 [0.000, 6.000], mean observation: 171.266 [24.000, 255.000], loss: 0.015816, mean_absolute_error: 4.663757, mean_q: 5.448508, mean_eps: 0.859922\n",
      "  155882/2000000: episode: 826, duration: 7.754s, episode steps: 159, steps per second: 21, episode reward: 60.000, mean reward: 0.377 [-1.000, 0.500], mean action: 2.950 [0.000, 6.000], mean observation: 171.283 [23.000, 255.000], loss: 0.017790, mean_absolute_error: 4.617487, mean_q: 5.382597, mean_eps: 0.859778\n",
      "  156060/2000000: episode: 827, duration: 8.873s, episode steps: 178, steps per second: 20, episode reward: 116.800, mean reward: 0.656 [-1.000, 1.000], mean action: 3.067 [0.000, 6.000], mean observation: 170.361 [24.000, 255.000], loss: 0.016823, mean_absolute_error: 4.529667, mean_q: 5.278529, mean_eps: 0.859627\n",
      "  156248/2000000: episode: 828, duration: 9.672s, episode steps: 188, steps per second: 19, episode reward: 132.800, mean reward: 0.706 [-1.000, 1.000], mean action: 3.106 [0.000, 6.000], mean observation: 170.786 [24.000, 255.000], loss: 0.014836, mean_absolute_error: 4.601749, mean_q: 5.370563, mean_eps: 0.859463\n",
      "  156453/2000000: episode: 829, duration: 10.729s, episode steps: 205, steps per second: 19, episode reward: 151.500, mean reward: 0.739 [-1.000, 1.000], mean action: 3.059 [0.000, 6.000], mean observation: 171.931 [23.000, 255.000], loss: 0.014358, mean_absolute_error: 4.555522, mean_q: 5.321415, mean_eps: 0.859285\n",
      "  156646/2000000: episode: 830, duration: 9.839s, episode steps: 193, steps per second: 20, episode reward: 114.000, mean reward: 0.591 [-1.000, 1.000], mean action: 3.244 [0.000, 6.000], mean observation: 171.162 [23.000, 255.000], loss: 0.014157, mean_absolute_error: 4.511220, mean_q: 5.266170, mean_eps: 0.859105\n",
      "  156792/2000000: episode: 831, duration: 7.022s, episode steps: 146, steps per second: 21, episode reward: 52.700, mean reward: 0.361 [-1.000, 0.500], mean action: 2.836 [0.000, 6.000], mean observation: 171.977 [24.000, 255.000], loss: 0.010542, mean_absolute_error: 4.508739, mean_q: 5.250930, mean_eps: 0.858954\n",
      "  157000/2000000: episode: 832, duration: 10.860s, episode steps: 208, steps per second: 19, episode reward: 124.700, mean reward: 0.600 [-1.000, 1.000], mean action: 3.154 [0.000, 6.000], mean observation: 172.151 [23.000, 255.000], loss: 0.014544, mean_absolute_error: 4.606732, mean_q: 5.370749, mean_eps: 0.858795\n",
      "  157180/2000000: episode: 833, duration: 9.060s, episode steps: 180, steps per second: 20, episode reward: 108.800, mean reward: 0.604 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 170.883 [24.000, 255.000], loss: 0.014199, mean_absolute_error: 4.566100, mean_q: 5.320429, mean_eps: 0.858621\n",
      "  157423/2000000: episode: 834, duration: 12.990s, episode steps: 243, steps per second: 19, episode reward: 123.700, mean reward: 0.509 [-1.000, 1.000], mean action: 3.243 [0.000, 6.000], mean observation: 172.890 [23.000, 255.000], loss: 0.013912, mean_absolute_error: 4.652206, mean_q: 5.428442, mean_eps: 0.858430\n",
      "  157596/2000000: episode: 835, duration: 8.623s, episode steps: 173, steps per second: 20, episode reward: 88.200, mean reward: 0.510 [-1.000, 1.000], mean action: 2.827 [0.000, 6.000], mean observation: 171.496 [23.000, 255.000], loss: 0.013406, mean_absolute_error: 4.699186, mean_q: 5.484523, mean_eps: 0.858243\n",
      "  157821/2000000: episode: 836, duration: 11.847s, episode steps: 225, steps per second: 19, episode reward: 138.900, mean reward: 0.617 [-1.000, 1.000], mean action: 3.236 [0.000, 6.000], mean observation: 172.312 [24.000, 255.000], loss: 0.015251, mean_absolute_error: 4.532210, mean_q: 5.285642, mean_eps: 0.858063\n",
      "  158041/2000000: episode: 837, duration: 11.498s, episode steps: 220, steps per second: 19, episode reward: 156.900, mean reward: 0.713 [-1.000, 1.000], mean action: 3.282 [0.000, 6.000], mean observation: 172.420 [24.000, 255.000], loss: 0.014328, mean_absolute_error: 4.595100, mean_q: 5.380947, mean_eps: 0.857861\n",
      "  158179/2000000: episode: 838, duration: 6.597s, episode steps: 138, steps per second: 21, episode reward: 51.900, mean reward: 0.376 [-1.000, 0.500], mean action: 2.862 [0.000, 6.000], mean observation: 172.330 [24.000, 255.000], loss: 0.012858, mean_absolute_error: 4.707079, mean_q: 5.506142, mean_eps: 0.857701\n",
      "  158366/2000000: episode: 839, duration: 9.508s, episode steps: 187, steps per second: 20, episode reward: 128.100, mean reward: 0.685 [-1.000, 1.000], mean action: 3.037 [0.000, 6.000], mean observation: 171.311 [23.000, 255.000], loss: 0.014148, mean_absolute_error: 4.554123, mean_q: 5.311131, mean_eps: 0.857555\n",
      "  158555/2000000: episode: 840, duration: 9.696s, episode steps: 189, steps per second: 19, episode reward: 128.900, mean reward: 0.682 [-1.000, 1.000], mean action: 3.095 [0.000, 6.000], mean observation: 171.647 [24.000, 255.000], loss: 0.013814, mean_absolute_error: 4.604248, mean_q: 5.376518, mean_eps: 0.857386\n",
      "  158715/2000000: episode: 841, duration: 7.879s, episode steps: 160, steps per second: 20, episode reward: 104.300, mean reward: 0.652 [-1.000, 1.000], mean action: 2.956 [0.000, 6.000], mean observation: 171.335 [24.000, 255.000], loss: 0.015125, mean_absolute_error: 4.597445, mean_q: 5.366690, mean_eps: 0.857229\n",
      "  158910/2000000: episode: 842, duration: 10.095s, episode steps: 195, steps per second: 19, episode reward: 132.100, mean reward: 0.677 [-1.000, 1.000], mean action: 3.241 [0.000, 6.000], mean observation: 171.836 [24.000, 255.000], loss: 0.014602, mean_absolute_error: 4.657787, mean_q: 5.448472, mean_eps: 0.857069\n",
      "  159090/2000000: episode: 843, duration: 8.989s, episode steps: 180, steps per second: 20, episode reward: 119.000, mean reward: 0.661 [-1.000, 1.000], mean action: 2.989 [0.000, 6.000], mean observation: 171.444 [22.000, 255.000], loss: 0.013717, mean_absolute_error: 4.616196, mean_q: 5.382992, mean_eps: 0.856900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  159253/2000000: episode: 844, duration: 7.986s, episode steps: 163, steps per second: 20, episode reward: 66.400, mean reward: 0.407 [-1.000, 0.500], mean action: 2.914 [0.000, 6.000], mean observation: 172.116 [23.000, 255.000], loss: 0.012632, mean_absolute_error: 4.557160, mean_q: 5.302867, mean_eps: 0.856745\n",
      "  159480/2000000: episode: 845, duration: 12.047s, episode steps: 227, steps per second: 19, episode reward: 169.900, mean reward: 0.748 [-1.000, 1.000], mean action: 3.344 [0.000, 6.000], mean observation: 172.256 [24.000, 255.000], loss: 0.016404, mean_absolute_error: 4.617571, mean_q: 5.390272, mean_eps: 0.856571\n",
      "  159707/2000000: episode: 846, duration: 11.943s, episode steps: 227, steps per second: 19, episode reward: 105.800, mean reward: 0.466 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 173.114 [23.000, 255.000], loss: 0.015226, mean_absolute_error: 4.560160, mean_q: 5.324047, mean_eps: 0.856367\n",
      "  159844/2000000: episode: 847, duration: 7.397s, episode steps: 137, steps per second: 19, episode reward: 73.700, mean reward: 0.538 [-1.000, 1.000], mean action: 3.540 [0.000, 6.000], mean observation: 172.734 [24.000, 255.000], loss: 0.011548, mean_absolute_error: 4.659572, mean_q: 5.443911, mean_eps: 0.856203\n",
      "  160066/2000000: episode: 848, duration: 11.776s, episode steps: 222, steps per second: 19, episode reward: 108.400, mean reward: 0.488 [-1.000, 1.000], mean action: 3.108 [0.000, 6.000], mean observation: 172.905 [24.000, 255.000], loss: 0.028958, mean_absolute_error: 4.877570, mean_q: 5.687206, mean_eps: 0.856041\n",
      "  160282/2000000: episode: 849, duration: 11.426s, episode steps: 216, steps per second: 19, episode reward: 104.000, mean reward: 0.481 [-1.000, 1.000], mean action: 3.097 [0.000, 6.000], mean observation: 173.448 [23.000, 255.000], loss: 0.025882, mean_absolute_error: 5.027802, mean_q: 5.866324, mean_eps: 0.855843\n",
      "  160506/2000000: episode: 850, duration: 11.674s, episode steps: 224, steps per second: 19, episode reward: 146.600, mean reward: 0.654 [-1.000, 1.000], mean action: 3.277 [0.000, 6.000], mean observation: 173.253 [24.000, 255.000], loss: 0.021426, mean_absolute_error: 5.057671, mean_q: 5.906589, mean_eps: 0.855645\n",
      "  160697/2000000: episode: 851, duration: 9.771s, episode steps: 191, steps per second: 20, episode reward: 141.600, mean reward: 0.741 [-1.000, 1.000], mean action: 3.058 [0.000, 6.000], mean observation: 171.888 [24.000, 255.000], loss: 0.018494, mean_absolute_error: 5.001349, mean_q: 5.830648, mean_eps: 0.855458\n",
      "  160841/2000000: episode: 852, duration: 6.927s, episode steps: 144, steps per second: 21, episode reward: 52.500, mean reward: 0.365 [-1.000, 0.500], mean action: 2.882 [0.000, 6.000], mean observation: 172.814 [23.000, 255.000], loss: 0.020284, mean_absolute_error: 5.005485, mean_q: 5.851563, mean_eps: 0.855307\n",
      "  161032/2000000: episode: 853, duration: 9.798s, episode steps: 191, steps per second: 19, episode reward: 119.700, mean reward: 0.627 [-1.000, 1.000], mean action: 3.105 [0.000, 6.000], mean observation: 171.705 [24.000, 255.000], loss: 0.017837, mean_absolute_error: 5.099908, mean_q: 5.984753, mean_eps: 0.855158\n",
      "  161210/2000000: episode: 854, duration: 8.797s, episode steps: 178, steps per second: 20, episode reward: 69.100, mean reward: 0.388 [-1.000, 0.500], mean action: 2.949 [0.000, 6.000], mean observation: 172.699 [23.000, 255.000], loss: 0.019896, mean_absolute_error: 4.928746, mean_q: 5.763056, mean_eps: 0.854992\n",
      "  161358/2000000: episode: 855, duration: 7.146s, episode steps: 148, steps per second: 21, episode reward: 55.300, mean reward: 0.374 [-1.000, 0.500], mean action: 2.750 [0.000, 6.000], mean observation: 172.749 [23.000, 255.000], loss: 0.020456, mean_absolute_error: 5.141842, mean_q: 6.009271, mean_eps: 0.854844\n",
      "  161564/2000000: episode: 856, duration: 10.689s, episode steps: 206, steps per second: 19, episode reward: 129.900, mean reward: 0.631 [-1.000, 1.000], mean action: 3.092 [0.000, 6.000], mean observation: 173.457 [24.000, 255.000], loss: 0.017818, mean_absolute_error: 5.011887, mean_q: 5.852351, mean_eps: 0.854686\n",
      "  161793/2000000: episode: 857, duration: 12.079s, episode steps: 229, steps per second: 19, episode reward: 175.300, mean reward: 0.766 [-1.000, 1.000], mean action: 3.293 [0.000, 6.000], mean observation: 173.280 [23.000, 255.000], loss: 0.018030, mean_absolute_error: 4.983711, mean_q: 5.829833, mean_eps: 0.854490\n",
      "  161974/2000000: episode: 858, duration: 9.061s, episode steps: 181, steps per second: 20, episode reward: 74.600, mean reward: 0.412 [-1.000, 0.500], mean action: 2.890 [0.000, 6.000], mean observation: 172.748 [23.000, 255.000], loss: 0.019632, mean_absolute_error: 5.093122, mean_q: 5.956073, mean_eps: 0.854304\n",
      "  162118/2000000: episode: 859, duration: 7.742s, episode steps: 144, steps per second: 19, episode reward: 75.800, mean reward: 0.526 [-1.000, 1.000], mean action: 3.396 [0.000, 6.000], mean observation: 172.921 [24.000, 255.000], loss: 0.017205, mean_absolute_error: 4.918387, mean_q: 5.733864, mean_eps: 0.854159\n",
      "  162310/2000000: episode: 860, duration: 9.883s, episode steps: 192, steps per second: 19, episode reward: 133.600, mean reward: 0.696 [-1.000, 1.000], mean action: 3.052 [0.000, 6.000], mean observation: 172.217 [23.000, 255.000], loss: 0.017123, mean_absolute_error: 5.074924, mean_q: 5.934000, mean_eps: 0.854007\n",
      "  162516/2000000: episode: 861, duration: 10.753s, episode steps: 206, steps per second: 19, episode reward: 128.000, mean reward: 0.621 [-1.000, 1.000], mean action: 2.985 [0.000, 6.000], mean observation: 172.996 [22.000, 255.000], loss: 0.017803, mean_absolute_error: 4.996316, mean_q: 5.830766, mean_eps: 0.853829\n",
      "  162710/2000000: episode: 862, duration: 9.887s, episode steps: 194, steps per second: 20, episode reward: 110.200, mean reward: 0.568 [-1.000, 1.000], mean action: 2.861 [0.000, 6.000], mean observation: 173.278 [23.000, 255.000], loss: 0.019679, mean_absolute_error: 5.017939, mean_q: 5.866416, mean_eps: 0.853649\n",
      "  162840/2000000: episode: 863, duration: 6.172s, episode steps: 130, steps per second: 21, episode reward: 46.300, mean reward: 0.356 [-1.000, 0.500], mean action: 2.554 [0.000, 6.000], mean observation: 173.307 [24.000, 255.000], loss: 0.019002, mean_absolute_error: 5.008431, mean_q: 5.858183, mean_eps: 0.853503\n",
      "  163064/2000000: episode: 864, duration: 11.842s, episode steps: 224, steps per second: 19, episode reward: 102.700, mean reward: 0.458 [-1.000, 1.000], mean action: 3.232 [0.000, 6.000], mean observation: 174.208 [23.000, 255.000], loss: 0.020399, mean_absolute_error: 4.943436, mean_q: 5.781637, mean_eps: 0.853345\n",
      "  163234/2000000: episode: 865, duration: 8.400s, episode steps: 170, steps per second: 20, episode reward: 69.100, mean reward: 0.406 [-1.000, 0.500], mean action: 2.771 [0.000, 6.000], mean observation: 172.693 [23.000, 255.000], loss: 0.015047, mean_absolute_error: 4.967330, mean_q: 5.803154, mean_eps: 0.853167\n",
      "  163428/2000000: episode: 866, duration: 10.095s, episode steps: 194, steps per second: 19, episode reward: 138.700, mean reward: 0.715 [-1.000, 1.000], mean action: 3.129 [0.000, 6.000], mean observation: 172.403 [23.000, 255.000], loss: 0.017775, mean_absolute_error: 5.139630, mean_q: 6.002470, mean_eps: 0.853003\n",
      "  163637/2000000: episode: 867, duration: 10.908s, episode steps: 209, steps per second: 19, episode reward: 136.600, mean reward: 0.654 [-1.000, 1.000], mean action: 3.158 [0.000, 6.000], mean observation: 173.185 [23.000, 255.000], loss: 0.019250, mean_absolute_error: 4.965454, mean_q: 5.793073, mean_eps: 0.852821\n",
      "  163848/2000000: episode: 868, duration: 11.057s, episode steps: 211, steps per second: 19, episode reward: 104.600, mean reward: 0.496 [-1.000, 1.000], mean action: 3.166 [0.000, 6.000], mean observation: 173.054 [24.000, 255.000], loss: 0.021053, mean_absolute_error: 5.009694, mean_q: 5.848081, mean_eps: 0.852632\n",
      "  164012/2000000: episode: 869, duration: 8.007s, episode steps: 164, steps per second: 20, episode reward: 66.900, mean reward: 0.408 [-1.000, 0.500], mean action: 2.939 [0.000, 6.000], mean observation: 172.250 [24.000, 255.000], loss: 0.020027, mean_absolute_error: 5.126733, mean_q: 5.983940, mean_eps: 0.852465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  164193/2000000: episode: 870, duration: 9.059s, episode steps: 181, steps per second: 20, episode reward: 105.400, mean reward: 0.582 [-1.000, 1.000], mean action: 2.878 [0.000, 6.000], mean observation: 171.779 [23.000, 255.000], loss: 0.020572, mean_absolute_error: 5.061935, mean_q: 5.918398, mean_eps: 0.852308\n",
      "  164406/2000000: episode: 871, duration: 11.230s, episode steps: 213, steps per second: 19, episode reward: 127.300, mean reward: 0.598 [-1.000, 1.000], mean action: 3.258 [0.000, 6.000], mean observation: 173.072 [24.000, 255.000], loss: 0.017997, mean_absolute_error: 5.076589, mean_q: 5.950205, mean_eps: 0.852130\n",
      "  164604/2000000: episode: 872, duration: 10.200s, episode steps: 198, steps per second: 19, episode reward: 96.100, mean reward: 0.485 [-1.000, 1.000], mean action: 2.848 [0.000, 6.000], mean observation: 172.380 [24.000, 255.000], loss: 0.018140, mean_absolute_error: 5.070793, mean_q: 5.929813, mean_eps: 0.851946\n",
      "  164783/2000000: episode: 873, duration: 8.981s, episode steps: 179, steps per second: 20, episode reward: 80.600, mean reward: 0.450 [-1.000, 1.000], mean action: 2.821 [0.000, 6.000], mean observation: 172.314 [23.000, 255.000], loss: 0.019981, mean_absolute_error: 5.104619, mean_q: 5.959276, mean_eps: 0.851777\n",
      "  164975/2000000: episode: 874, duration: 9.745s, episode steps: 192, steps per second: 20, episode reward: 120.800, mean reward: 0.629 [-1.000, 1.000], mean action: 2.948 [0.000, 6.000], mean observation: 171.950 [23.000, 255.000], loss: 0.020790, mean_absolute_error: 5.179328, mean_q: 6.044643, mean_eps: 0.851610\n",
      "  165204/2000000: episode: 875, duration: 12.187s, episode steps: 229, steps per second: 19, episode reward: 136.400, mean reward: 0.596 [-1.000, 1.000], mean action: 3.253 [0.000, 6.000], mean observation: 173.058 [22.000, 255.000], loss: 0.018773, mean_absolute_error: 5.082716, mean_q: 5.926753, mean_eps: 0.851421\n",
      "  165414/2000000: episode: 876, duration: 10.983s, episode steps: 210, steps per second: 19, episode reward: 136.700, mean reward: 0.651 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 173.149 [24.000, 255.000], loss: 0.019149, mean_absolute_error: 5.143701, mean_q: 6.016602, mean_eps: 0.851223\n",
      "  165593/2000000: episode: 877, duration: 8.973s, episode steps: 179, steps per second: 20, episode reward: 115.000, mean reward: 0.642 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 170.905 [24.000, 255.000], loss: 0.019749, mean_absolute_error: 5.198497, mean_q: 6.077548, mean_eps: 0.851046\n",
      "  165774/2000000: episode: 878, duration: 9.065s, episode steps: 181, steps per second: 20, episode reward: 72.600, mean reward: 0.401 [-1.000, 0.500], mean action: 2.956 [0.000, 6.000], mean observation: 171.989 [23.000, 255.000], loss: 0.020844, mean_absolute_error: 5.143739, mean_q: 5.997037, mean_eps: 0.850884\n",
      "  166007/2000000: episode: 879, duration: 12.066s, episode steps: 233, steps per second: 19, episode reward: 138.900, mean reward: 0.596 [-1.000, 1.000], mean action: 3.245 [0.000, 6.000], mean observation: 173.523 [24.000, 255.000], loss: 0.021917, mean_absolute_error: 5.144526, mean_q: 6.006416, mean_eps: 0.850699\n",
      "  166177/2000000: episode: 880, duration: 8.310s, episode steps: 170, steps per second: 20, episode reward: 65.100, mean reward: 0.383 [-1.000, 0.500], mean action: 3.053 [0.000, 6.000], mean observation: 172.362 [24.000, 255.000], loss: 0.017693, mean_absolute_error: 5.132271, mean_q: 6.013110, mean_eps: 0.850517\n",
      "  166322/2000000: episode: 881, duration: 7.833s, episode steps: 145, steps per second: 19, episode reward: 80.800, mean reward: 0.557 [-1.000, 1.000], mean action: 3.572 [0.000, 6.000], mean observation: 173.181 [24.000, 255.000], loss: 0.022566, mean_absolute_error: 5.156314, mean_q: 6.019116, mean_eps: 0.850375\n",
      "  166482/2000000: episode: 882, duration: 7.781s, episode steps: 160, steps per second: 21, episode reward: 58.500, mean reward: 0.366 [-1.000, 0.500], mean action: 2.888 [0.000, 6.000], mean observation: 171.694 [23.000, 255.000], loss: 0.019832, mean_absolute_error: 5.256111, mean_q: 6.147763, mean_eps: 0.850238\n",
      "  166673/2000000: episode: 883, duration: 9.741s, episode steps: 191, steps per second: 20, episode reward: 121.300, mean reward: 0.635 [-1.000, 1.000], mean action: 2.848 [0.000, 6.000], mean observation: 171.442 [23.000, 255.000], loss: 0.019308, mean_absolute_error: 5.126845, mean_q: 5.989736, mean_eps: 0.850080\n",
      "  166856/2000000: episode: 884, duration: 9.036s, episode steps: 183, steps per second: 20, episode reward: 73.200, mean reward: 0.400 [-1.000, 0.500], mean action: 2.787 [0.000, 6.000], mean observation: 172.145 [23.000, 255.000], loss: 0.020073, mean_absolute_error: 5.169808, mean_q: 6.048402, mean_eps: 0.849912\n",
      "  167064/2000000: episode: 885, duration: 10.542s, episode steps: 208, steps per second: 20, episode reward: 72.100, mean reward: 0.347 [-1.000, 0.500], mean action: 3.067 [0.000, 6.000], mean observation: 173.069 [24.000, 255.000], loss: 0.019636, mean_absolute_error: 5.106626, mean_q: 5.962038, mean_eps: 0.849738\n",
      "  167243/2000000: episode: 886, duration: 8.920s, episode steps: 179, steps per second: 20, episode reward: 76.800, mean reward: 0.429 [-1.000, 1.000], mean action: 3.117 [0.000, 6.000], mean observation: 171.742 [24.000, 255.000], loss: 0.019094, mean_absolute_error: 5.026414, mean_q: 5.860627, mean_eps: 0.849563\n",
      "  167397/2000000: episode: 887, duration: 7.407s, episode steps: 154, steps per second: 21, episode reward: 53.900, mean reward: 0.350 [-1.000, 0.500], mean action: 2.870 [0.000, 6.000], mean observation: 171.938 [24.000, 255.000], loss: 0.018909, mean_absolute_error: 5.154429, mean_q: 6.019786, mean_eps: 0.849412\n",
      "  167581/2000000: episode: 888, duration: 9.278s, episode steps: 184, steps per second: 20, episode reward: 85.600, mean reward: 0.465 [-1.000, 1.000], mean action: 3.201 [0.000, 6.000], mean observation: 171.325 [24.000, 255.000], loss: 0.017685, mean_absolute_error: 5.076315, mean_q: 5.914350, mean_eps: 0.849259\n",
      "  167733/2000000: episode: 889, duration: 7.456s, episode steps: 152, steps per second: 20, episode reward: 66.500, mean reward: 0.438 [-1.000, 0.500], mean action: 2.776 [0.000, 6.000], mean observation: 171.654 [23.000, 255.000], loss: 0.015728, mean_absolute_error: 4.946906, mean_q: 5.763512, mean_eps: 0.849108\n",
      "  167924/2000000: episode: 890, duration: 9.778s, episode steps: 191, steps per second: 20, episode reward: 90.100, mean reward: 0.472 [-1.000, 1.000], mean action: 2.895 [0.000, 6.000], mean observation: 171.890 [23.000, 255.000], loss: 0.017234, mean_absolute_error: 5.030045, mean_q: 5.862138, mean_eps: 0.848955\n",
      "  168118/2000000: episode: 891, duration: 9.991s, episode steps: 194, steps per second: 19, episode reward: 135.000, mean reward: 0.696 [-1.000, 1.000], mean action: 3.052 [0.000, 6.000], mean observation: 171.581 [23.000, 255.000], loss: 0.020671, mean_absolute_error: 5.071822, mean_q: 5.914118, mean_eps: 0.848782\n",
      "  168304/2000000: episode: 892, duration: 9.261s, episode steps: 186, steps per second: 20, episode reward: 65.500, mean reward: 0.352 [-1.000, 0.500], mean action: 3.129 [0.000, 6.000], mean observation: 172.421 [24.000, 255.000], loss: 0.021445, mean_absolute_error: 5.117818, mean_q: 5.978640, mean_eps: 0.848611\n",
      "  168520/2000000: episode: 893, duration: 11.340s, episode steps: 216, steps per second: 19, episode reward: 111.500, mean reward: 0.516 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 172.950 [23.000, 255.000], loss: 0.019912, mean_absolute_error: 5.153114, mean_q: 6.020751, mean_eps: 0.848431\n",
      "  168656/2000000: episode: 894, duration: 6.508s, episode steps: 136, steps per second: 21, episode reward: 48.500, mean reward: 0.357 [-1.000, 0.500], mean action: 2.801 [0.000, 6.000], mean observation: 172.966 [23.000, 255.000], loss: 0.017642, mean_absolute_error: 5.126498, mean_q: 5.982647, mean_eps: 0.848273\n",
      "  168866/2000000: episode: 895, duration: 10.780s, episode steps: 210, steps per second: 19, episode reward: 96.200, mean reward: 0.458 [-1.000, 1.000], mean action: 3.129 [0.000, 6.000], mean observation: 172.810 [23.000, 255.000], loss: 0.016499, mean_absolute_error: 5.114053, mean_q: 5.967561, mean_eps: 0.848116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  169003/2000000: episode: 896, duration: 6.416s, episode steps: 137, steps per second: 21, episode reward: 43.800, mean reward: 0.320 [-1.000, 0.500], mean action: 2.628 [0.000, 6.000], mean observation: 173.293 [24.000, 255.000], loss: 0.021046, mean_absolute_error: 5.173168, mean_q: 6.039774, mean_eps: 0.847959\n",
      "  169152/2000000: episode: 897, duration: 7.273s, episode steps: 149, steps per second: 20, episode reward: 61.400, mean reward: 0.412 [-1.000, 0.500], mean action: 2.933 [0.000, 6.000], mean observation: 172.379 [24.000, 255.000], loss: 0.020754, mean_absolute_error: 4.981658, mean_q: 5.811157, mean_eps: 0.847832\n",
      "  169338/2000000: episode: 898, duration: 9.490s, episode steps: 186, steps per second: 20, episode reward: 123.100, mean reward: 0.662 [-1.000, 1.000], mean action: 3.188 [0.000, 6.000], mean observation: 171.861 [22.000, 255.000], loss: 0.018803, mean_absolute_error: 4.935635, mean_q: 5.768061, mean_eps: 0.847680\n",
      "  169531/2000000: episode: 899, duration: 9.904s, episode steps: 193, steps per second: 19, episode reward: 90.300, mean reward: 0.468 [-1.000, 1.000], mean action: 3.021 [0.000, 6.000], mean observation: 172.424 [23.000, 255.000], loss: 0.018580, mean_absolute_error: 5.042350, mean_q: 5.891988, mean_eps: 0.847509\n",
      "  169713/2000000: episode: 900, duration: 9.047s, episode steps: 182, steps per second: 20, episode reward: 76.300, mean reward: 0.419 [-1.000, 0.500], mean action: 3.038 [0.000, 6.000], mean observation: 172.768 [24.000, 255.000], loss: 0.016868, mean_absolute_error: 5.096975, mean_q: 5.948073, mean_eps: 0.847340\n",
      "  169898/2000000: episode: 901, duration: 9.386s, episode steps: 185, steps per second: 20, episode reward: 85.000, mean reward: 0.459 [-1.000, 0.500], mean action: 2.870 [0.000, 6.000], mean observation: 172.302 [24.000, 255.000], loss: 0.016435, mean_absolute_error: 5.094479, mean_q: 5.950185, mean_eps: 0.847175\n",
      "  170040/2000000: episode: 902, duration: 7.673s, episode steps: 142, steps per second: 19, episode reward: 74.700, mean reward: 0.526 [-1.000, 1.000], mean action: 3.458 [0.000, 6.000], mean observation: 173.108 [24.000, 255.000], loss: 0.038471, mean_absolute_error: 5.255304, mean_q: 6.135107, mean_eps: 0.847029\n",
      "  170243/2000000: episode: 903, duration: 10.457s, episode steps: 203, steps per second: 19, episode reward: 133.200, mean reward: 0.656 [-1.000, 1.000], mean action: 3.123 [0.000, 6.000], mean observation: 172.935 [25.000, 255.000], loss: 0.032491, mean_absolute_error: 5.353543, mean_q: 6.257311, mean_eps: 0.846874\n",
      "  170383/2000000: episode: 904, duration: 6.880s, episode steps: 140, steps per second: 20, episode reward: 57.300, mean reward: 0.409 [-1.000, 0.500], mean action: 2.693 [0.000, 6.000], mean observation: 172.565 [23.000, 255.000], loss: 0.024384, mean_absolute_error: 5.544622, mean_q: 6.476238, mean_eps: 0.846719\n",
      "  170587/2000000: episode: 905, duration: 10.428s, episode steps: 204, steps per second: 20, episode reward: 121.900, mean reward: 0.598 [-1.000, 1.000], mean action: 3.123 [0.000, 6.000], mean observation: 172.589 [23.000, 255.000], loss: 0.023385, mean_absolute_error: 5.430971, mean_q: 6.346026, mean_eps: 0.846564\n",
      "  170722/2000000: episode: 906, duration: 6.464s, episode steps: 135, steps per second: 21, episode reward: 49.600, mean reward: 0.367 [-1.000, 0.500], mean action: 2.933 [0.000, 6.000], mean observation: 172.999 [23.000, 255.000], loss: 0.020616, mean_absolute_error: 5.413405, mean_q: 6.327660, mean_eps: 0.846411\n",
      "  170858/2000000: episode: 907, duration: 6.498s, episode steps: 136, steps per second: 21, episode reward: 48.900, mean reward: 0.360 [-1.000, 0.500], mean action: 2.772 [0.000, 6.000], mean observation: 173.434 [24.000, 255.000], loss: 0.019671, mean_absolute_error: 5.348288, mean_q: 6.238594, mean_eps: 0.846289\n",
      "  170995/2000000: episode: 908, duration: 6.559s, episode steps: 137, steps per second: 21, episode reward: 47.800, mean reward: 0.349 [-1.000, 0.500], mean action: 2.650 [0.000, 6.000], mean observation: 173.374 [23.000, 255.000], loss: 0.022342, mean_absolute_error: 5.577078, mean_q: 6.512724, mean_eps: 0.846167\n",
      "  171198/2000000: episode: 909, duration: 10.529s, episode steps: 203, steps per second: 19, episode reward: 122.000, mean reward: 0.601 [-1.000, 1.000], mean action: 3.123 [0.000, 6.000], mean observation: 172.763 [23.000, 255.000], loss: 0.022934, mean_absolute_error: 5.314299, mean_q: 6.201150, mean_eps: 0.846014\n",
      "  171386/2000000: episode: 910, duration: 9.639s, episode steps: 188, steps per second: 20, episode reward: 120.500, mean reward: 0.641 [-1.000, 1.000], mean action: 2.798 [0.000, 6.000], mean observation: 172.615 [24.000, 255.000], loss: 0.024351, mean_absolute_error: 5.429672, mean_q: 6.355265, mean_eps: 0.845837\n",
      "  171592/2000000: episode: 911, duration: 10.743s, episode steps: 206, steps per second: 19, episode reward: 129.500, mean reward: 0.629 [-1.000, 1.000], mean action: 3.136 [0.000, 6.000], mean observation: 173.356 [23.000, 255.000], loss: 0.023720, mean_absolute_error: 5.353135, mean_q: 6.252064, mean_eps: 0.845661\n",
      "  171752/2000000: episode: 912, duration: 7.928s, episode steps: 160, steps per second: 20, episode reward: 71.800, mean reward: 0.449 [-1.000, 1.000], mean action: 2.919 [0.000, 6.000], mean observation: 172.012 [24.000, 255.000], loss: 0.022737, mean_absolute_error: 5.530815, mean_q: 6.471581, mean_eps: 0.845497\n",
      "  171958/2000000: episode: 913, duration: 10.734s, episode steps: 206, steps per second: 19, episode reward: 97.800, mean reward: 0.475 [-1.000, 1.000], mean action: 3.238 [0.000, 6.000], mean observation: 173.375 [23.000, 255.000], loss: 0.021359, mean_absolute_error: 5.365693, mean_q: 6.262837, mean_eps: 0.845331\n",
      "  172152/2000000: episode: 914, duration: 9.923s, episode steps: 194, steps per second: 20, episode reward: 125.400, mean reward: 0.646 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 172.402 [23.000, 255.000], loss: 0.020413, mean_absolute_error: 5.484739, mean_q: 6.399961, mean_eps: 0.845151\n",
      "  172353/2000000: episode: 915, duration: 10.369s, episode steps: 201, steps per second: 19, episode reward: 136.200, mean reward: 0.678 [-1.000, 1.000], mean action: 2.876 [0.000, 6.000], mean observation: 173.379 [24.000, 255.000], loss: 0.022737, mean_absolute_error: 5.388366, mean_q: 6.299634, mean_eps: 0.844973\n",
      "  172538/2000000: episode: 916, duration: 9.094s, episode steps: 185, steps per second: 20, episode reward: 50.200, mean reward: 0.271 [-1.000, 0.500], mean action: 3.027 [0.000, 6.000], mean observation: 173.624 [23.000, 255.000], loss: 0.021049, mean_absolute_error: 5.432497, mean_q: 6.350021, mean_eps: 0.844799\n",
      "  172752/2000000: episode: 917, duration: 11.289s, episode steps: 214, steps per second: 19, episode reward: 131.000, mean reward: 0.612 [-1.000, 1.000], mean action: 3.093 [0.000, 6.000], mean observation: 173.541 [24.000, 255.000], loss: 0.023997, mean_absolute_error: 5.440316, mean_q: 6.354529, mean_eps: 0.844620\n",
      "  172893/2000000: episode: 918, duration: 6.878s, episode steps: 141, steps per second: 20, episode reward: 58.200, mean reward: 0.413 [-1.000, 0.500], mean action: 2.610 [0.000, 6.000], mean observation: 172.551 [25.000, 255.000], loss: 0.023067, mean_absolute_error: 5.470050, mean_q: 6.394438, mean_eps: 0.844460\n",
      "  173106/2000000: episode: 919, duration: 11.058s, episode steps: 213, steps per second: 19, episode reward: 130.000, mean reward: 0.610 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 173.705 [23.000, 255.000], loss: 0.022406, mean_absolute_error: 5.332182, mean_q: 6.218439, mean_eps: 0.844300\n",
      "  173298/2000000: episode: 920, duration: 9.948s, episode steps: 192, steps per second: 19, episode reward: 141.300, mean reward: 0.736 [-1.000, 1.000], mean action: 2.979 [0.000, 6.000], mean observation: 172.207 [24.000, 255.000], loss: 0.022284, mean_absolute_error: 5.352096, mean_q: 6.264176, mean_eps: 0.844118\n",
      "  173510/2000000: episode: 921, duration: 11.390s, episode steps: 212, steps per second: 19, episode reward: 137.700, mean reward: 0.650 [-1.000, 1.000], mean action: 3.085 [0.000, 6.000], mean observation: 173.405 [23.000, 255.000], loss: 0.021401, mean_absolute_error: 5.419544, mean_q: 6.329147, mean_eps: 0.843936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  173697/2000000: episode: 922, duration: 9.487s, episode steps: 187, steps per second: 20, episode reward: 106.600, mean reward: 0.570 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 172.235 [23.000, 255.000], loss: 0.020869, mean_absolute_error: 5.449778, mean_q: 6.354863, mean_eps: 0.843756\n",
      "  173885/2000000: episode: 923, duration: 9.650s, episode steps: 188, steps per second: 19, episode reward: 129.100, mean reward: 0.687 [-1.000, 1.000], mean action: 3.080 [0.000, 6.000], mean observation: 171.758 [23.000, 255.000], loss: 0.021248, mean_absolute_error: 5.548793, mean_q: 6.491547, mean_eps: 0.843587\n",
      "  174040/2000000: episode: 924, duration: 7.608s, episode steps: 155, steps per second: 20, episode reward: 76.400, mean reward: 0.493 [-1.000, 1.000], mean action: 3.006 [0.000, 6.000], mean observation: 171.522 [22.000, 255.000], loss: 0.022779, mean_absolute_error: 5.397482, mean_q: 6.301738, mean_eps: 0.843434\n",
      "  174209/2000000: episode: 925, duration: 8.238s, episode steps: 169, steps per second: 21, episode reward: 62.200, mean reward: 0.368 [-1.000, 0.500], mean action: 3.148 [0.000, 6.000], mean observation: 172.383 [23.000, 255.000], loss: 0.024212, mean_absolute_error: 5.630836, mean_q: 6.584490, mean_eps: 0.843288\n",
      "  174394/2000000: episode: 926, duration: 9.238s, episode steps: 185, steps per second: 20, episode reward: 65.800, mean reward: 0.356 [-1.000, 0.500], mean action: 2.919 [0.000, 6.000], mean observation: 172.884 [24.000, 255.000], loss: 0.020716, mean_absolute_error: 5.445052, mean_q: 6.367065, mean_eps: 0.843128\n",
      "  174618/2000000: episode: 927, duration: 11.816s, episode steps: 224, steps per second: 19, episode reward: 106.800, mean reward: 0.477 [-1.000, 1.000], mean action: 3.018 [0.000, 6.000], mean observation: 173.456 [23.000, 255.000], loss: 0.022238, mean_absolute_error: 5.451728, mean_q: 6.362860, mean_eps: 0.842945\n",
      "  174782/2000000: episode: 928, duration: 8.034s, episode steps: 164, steps per second: 20, episode reward: 66.500, mean reward: 0.405 [-1.000, 0.500], mean action: 2.988 [0.000, 6.000], mean observation: 171.908 [23.000, 255.000], loss: 0.019057, mean_absolute_error: 5.293294, mean_q: 6.178206, mean_eps: 0.842770\n",
      "  175009/2000000: episode: 929, duration: 12.001s, episode steps: 227, steps per second: 19, episode reward: 101.800, mean reward: 0.448 [-1.000, 1.000], mean action: 3.150 [0.000, 6.000], mean observation: 173.746 [22.000, 255.000], loss: 0.021325, mean_absolute_error: 5.364251, mean_q: 6.259442, mean_eps: 0.842594\n",
      "  175184/2000000: episode: 930, duration: 8.716s, episode steps: 175, steps per second: 20, episode reward: 107.300, mean reward: 0.613 [-1.000, 1.000], mean action: 2.846 [0.000, 6.000], mean observation: 171.162 [25.000, 255.000], loss: 0.019957, mean_absolute_error: 5.372746, mean_q: 6.265229, mean_eps: 0.842414\n",
      "  175366/2000000: episode: 931, duration: 9.172s, episode steps: 182, steps per second: 20, episode reward: 130.800, mean reward: 0.719 [-1.000, 1.000], mean action: 2.978 [0.000, 6.000], mean observation: 171.771 [24.000, 255.000], loss: 0.018643, mean_absolute_error: 5.316294, mean_q: 6.190571, mean_eps: 0.842253\n",
      "  175474/2000000: episode: 932, duration: 5.906s, episode steps: 108, steps per second: 18, episode reward: 63.800, mean reward: 0.591 [-1.000, 1.000], mean action: 3.759 [0.000, 6.000], mean observation: 174.090 [23.000, 255.000], loss: 0.021802, mean_absolute_error: 5.476877, mean_q: 6.394030, mean_eps: 0.842122\n",
      "  175660/2000000: episode: 933, duration: 9.437s, episode steps: 186, steps per second: 20, episode reward: 130.900, mean reward: 0.704 [-1.000, 1.000], mean action: 2.952 [0.000, 6.000], mean observation: 171.293 [23.000, 255.000], loss: 0.020918, mean_absolute_error: 5.314497, mean_q: 6.195382, mean_eps: 0.841991\n",
      "  175845/2000000: episode: 934, duration: 9.441s, episode steps: 185, steps per second: 20, episode reward: 127.500, mean reward: 0.689 [-1.000, 1.000], mean action: 3.027 [0.000, 6.000], mean observation: 171.980 [23.000, 255.000], loss: 0.020133, mean_absolute_error: 5.357248, mean_q: 6.253416, mean_eps: 0.841823\n",
      "  175980/2000000: episode: 935, duration: 7.313s, episode steps: 135, steps per second: 18, episode reward: 71.000, mean reward: 0.526 [-1.000, 1.000], mean action: 3.222 [0.000, 6.000], mean observation: 173.758 [24.000, 255.000], loss: 0.019582, mean_absolute_error: 5.289608, mean_q: 6.168676, mean_eps: 0.841679\n",
      "  176172/2000000: episode: 936, duration: 9.784s, episode steps: 192, steps per second: 20, episode reward: 120.000, mean reward: 0.625 [-1.000, 1.000], mean action: 3.016 [0.000, 6.000], mean observation: 172.496 [24.000, 255.000], loss: 0.019582, mean_absolute_error: 5.455154, mean_q: 6.357683, mean_eps: 0.841533\n",
      "  176365/2000000: episode: 937, duration: 9.880s, episode steps: 193, steps per second: 20, episode reward: 122.300, mean reward: 0.634 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 171.883 [23.000, 255.000], loss: 0.019540, mean_absolute_error: 5.220528, mean_q: 6.086881, mean_eps: 0.841359\n",
      "  176546/2000000: episode: 938, duration: 9.085s, episode steps: 181, steps per second: 20, episode reward: 85.400, mean reward: 0.472 [-1.000, 1.000], mean action: 2.834 [0.000, 6.000], mean observation: 172.210 [24.000, 255.000], loss: 0.024301, mean_absolute_error: 5.494946, mean_q: 6.413415, mean_eps: 0.841190\n",
      "  176744/2000000: episode: 939, duration: 10.213s, episode steps: 198, steps per second: 19, episode reward: 139.700, mean reward: 0.706 [-1.000, 1.000], mean action: 3.152 [0.000, 6.000], mean observation: 172.764 [23.000, 255.000], loss: 0.020967, mean_absolute_error: 5.458625, mean_q: 6.372684, mean_eps: 0.841020\n",
      "  176878/2000000: episode: 940, duration: 6.240s, episode steps: 134, steps per second: 21, episode reward: 44.700, mean reward: 0.334 [-1.000, 0.500], mean action: 2.627 [0.000, 6.000], mean observation: 173.409 [24.000, 255.000], loss: 0.019758, mean_absolute_error: 5.436023, mean_q: 6.362200, mean_eps: 0.840871\n",
      "  177069/2000000: episode: 941, duration: 9.736s, episode steps: 191, steps per second: 20, episode reward: 132.900, mean reward: 0.696 [-1.000, 1.000], mean action: 3.042 [0.000, 6.000], mean observation: 171.812 [23.000, 255.000], loss: 0.020982, mean_absolute_error: 5.466722, mean_q: 6.389752, mean_eps: 0.840723\n",
      "  177224/2000000: episode: 942, duration: 7.600s, episode steps: 155, steps per second: 20, episode reward: 61.600, mean reward: 0.397 [-1.000, 0.500], mean action: 2.935 [0.000, 6.000], mean observation: 172.168 [24.000, 255.000], loss: 0.025779, mean_absolute_error: 5.407356, mean_q: 6.320415, mean_eps: 0.840569\n",
      "  177406/2000000: episode: 943, duration: 9.232s, episode steps: 182, steps per second: 20, episode reward: 114.300, mean reward: 0.628 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 171.440 [24.000, 255.000], loss: 0.023405, mean_absolute_error: 5.357149, mean_q: 6.249847, mean_eps: 0.840417\n",
      "  177589/2000000: episode: 944, duration: 9.159s, episode steps: 183, steps per second: 20, episode reward: 77.400, mean reward: 0.423 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 172.046 [24.000, 255.000], loss: 0.022264, mean_absolute_error: 5.436409, mean_q: 6.361249, mean_eps: 0.840252\n",
      "  177792/2000000: episode: 945, duration: 10.367s, episode steps: 203, steps per second: 20, episode reward: 98.900, mean reward: 0.487 [-1.000, 1.000], mean action: 2.833 [0.000, 6.000], mean observation: 173.199 [23.000, 255.000], loss: 0.017770, mean_absolute_error: 5.387572, mean_q: 6.295172, mean_eps: 0.840079\n",
      "  177975/2000000: episode: 946, duration: 9.211s, episode steps: 183, steps per second: 20, episode reward: 125.500, mean reward: 0.686 [-1.000, 1.000], mean action: 3.038 [0.000, 6.000], mean observation: 171.475 [23.000, 255.000], loss: 0.018858, mean_absolute_error: 5.340668, mean_q: 6.231772, mean_eps: 0.839906\n",
      "  178158/2000000: episode: 947, duration: 9.256s, episode steps: 183, steps per second: 20, episode reward: 86.900, mean reward: 0.475 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 172.120 [23.000, 255.000], loss: 0.022318, mean_absolute_error: 5.444686, mean_q: 6.349823, mean_eps: 0.839741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  178322/2000000: episode: 948, duration: 8.012s, episode steps: 164, steps per second: 20, episode reward: 67.300, mean reward: 0.410 [-1.000, 0.500], mean action: 2.823 [0.000, 6.000], mean observation: 171.478 [24.000, 255.000], loss: 0.022692, mean_absolute_error: 5.289641, mean_q: 6.173274, mean_eps: 0.839584\n",
      "  178502/2000000: episode: 949, duration: 8.879s, episode steps: 180, steps per second: 20, episode reward: 75.500, mean reward: 0.419 [-1.000, 1.000], mean action: 3.094 [0.000, 6.000], mean observation: 171.805 [23.000, 255.000], loss: 0.021916, mean_absolute_error: 5.389863, mean_q: 6.287182, mean_eps: 0.839429\n",
      "  178654/2000000: episode: 950, duration: 7.358s, episode steps: 152, steps per second: 21, episode reward: 61.300, mean reward: 0.403 [-1.000, 0.500], mean action: 2.757 [0.000, 6.000], mean observation: 171.768 [23.000, 255.000], loss: 0.020160, mean_absolute_error: 5.349994, mean_q: 6.252641, mean_eps: 0.839280\n",
      "  178831/2000000: episode: 951, duration: 8.667s, episode steps: 177, steps per second: 20, episode reward: 70.200, mean reward: 0.397 [-1.000, 0.500], mean action: 3.085 [0.000, 6.000], mean observation: 172.143 [24.000, 255.000], loss: 0.021640, mean_absolute_error: 5.481103, mean_q: 6.421385, mean_eps: 0.839132\n",
      "  179028/2000000: episode: 952, duration: 10.109s, episode steps: 197, steps per second: 19, episode reward: 125.100, mean reward: 0.635 [-1.000, 1.000], mean action: 3.188 [0.000, 6.000], mean observation: 171.686 [23.000, 255.000], loss: 0.019002, mean_absolute_error: 5.290678, mean_q: 6.167210, mean_eps: 0.838965\n",
      "  179163/2000000: episode: 953, duration: 6.417s, episode steps: 135, steps per second: 21, episode reward: 45.600, mean reward: 0.338 [-1.000, 0.500], mean action: 2.556 [0.000, 6.000], mean observation: 172.455 [24.000, 255.000], loss: 0.019620, mean_absolute_error: 5.376921, mean_q: 6.266631, mean_eps: 0.838815\n",
      "  179368/2000000: episode: 954, duration: 10.664s, episode steps: 205, steps per second: 19, episode reward: 110.900, mean reward: 0.541 [-1.000, 1.000], mean action: 3.156 [0.000, 6.000], mean observation: 172.563 [23.000, 255.000], loss: 0.019844, mean_absolute_error: 5.339353, mean_q: 6.239219, mean_eps: 0.838662\n",
      "  179574/2000000: episode: 955, duration: 10.427s, episode steps: 206, steps per second: 20, episode reward: 89.900, mean reward: 0.436 [-1.000, 0.500], mean action: 3.092 [0.000, 6.000], mean observation: 173.310 [23.000, 255.000], loss: 0.020172, mean_absolute_error: 5.351520, mean_q: 6.248729, mean_eps: 0.838477\n",
      "  179760/2000000: episode: 956, duration: 9.453s, episode steps: 186, steps per second: 20, episode reward: 130.000, mean reward: 0.699 [-1.000, 1.000], mean action: 3.188 [0.000, 6.000], mean observation: 171.018 [23.000, 255.000], loss: 0.023791, mean_absolute_error: 5.459292, mean_q: 6.371311, mean_eps: 0.838301\n",
      "  179896/2000000: episode: 957, duration: 6.590s, episode steps: 136, steps per second: 21, episode reward: 51.700, mean reward: 0.380 [-1.000, 0.500], mean action: 2.551 [0.000, 6.000], mean observation: 171.933 [24.000, 255.000], loss: 0.017210, mean_absolute_error: 5.508202, mean_q: 6.433814, mean_eps: 0.838157\n",
      "  180082/2000000: episode: 958, duration: 9.498s, episode steps: 186, steps per second: 20, episode reward: 86.200, mean reward: 0.463 [-1.000, 1.000], mean action: 3.065 [0.000, 6.000], mean observation: 171.832 [24.000, 255.000], loss: 0.041815, mean_absolute_error: 5.593710, mean_q: 6.555780, mean_eps: 0.838011\n",
      "  180294/2000000: episode: 959, duration: 11.128s, episode steps: 212, steps per second: 19, episode reward: 135.700, mean reward: 0.640 [-1.000, 1.000], mean action: 3.151 [0.000, 6.000], mean observation: 172.397 [24.000, 255.000], loss: 0.026002, mean_absolute_error: 5.819166, mean_q: 6.822485, mean_eps: 0.837831\n",
      "  180490/2000000: episode: 960, duration: 10.084s, episode steps: 196, steps per second: 19, episode reward: 128.100, mean reward: 0.654 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 171.344 [24.000, 255.000], loss: 0.028062, mean_absolute_error: 5.790205, mean_q: 6.766266, mean_eps: 0.837647\n",
      "  180679/2000000: episode: 961, duration: 9.678s, episode steps: 189, steps per second: 20, episode reward: 143.700, mean reward: 0.760 [-1.000, 1.000], mean action: 2.963 [0.000, 6.000], mean observation: 171.013 [23.000, 255.000], loss: 0.022241, mean_absolute_error: 5.847068, mean_q: 6.825587, mean_eps: 0.837474\n",
      "  180891/2000000: episode: 962, duration: 11.078s, episode steps: 212, steps per second: 19, episode reward: 117.900, mean reward: 0.556 [-1.000, 1.000], mean action: 3.104 [0.000, 6.000], mean observation: 172.376 [22.000, 255.000], loss: 0.031287, mean_absolute_error: 5.775557, mean_q: 6.742205, mean_eps: 0.837294\n",
      "  181074/2000000: episode: 963, duration: 9.219s, episode steps: 183, steps per second: 20, episode reward: 114.000, mean reward: 0.623 [-1.000, 1.000], mean action: 2.913 [0.000, 6.000], mean observation: 170.569 [23.000, 255.000], loss: 0.026692, mean_absolute_error: 5.901127, mean_q: 6.893672, mean_eps: 0.837116\n",
      "  181213/2000000: episode: 964, duration: 7.494s, episode steps: 139, steps per second: 19, episode reward: 72.400, mean reward: 0.521 [-1.000, 1.000], mean action: 3.424 [0.000, 6.000], mean observation: 171.993 [22.000, 255.000], loss: 0.028187, mean_absolute_error: 5.776661, mean_q: 6.751507, mean_eps: 0.836970\n",
      "  181371/2000000: episode: 965, duration: 7.710s, episode steps: 158, steps per second: 20, episode reward: 65.100, mean reward: 0.412 [-1.000, 0.500], mean action: 2.994 [0.000, 6.000], mean observation: 171.216 [23.000, 255.000], loss: 0.030569, mean_absolute_error: 5.719116, mean_q: 6.684906, mean_eps: 0.836837\n",
      "  181550/2000000: episode: 966, duration: 8.936s, episode steps: 179, steps per second: 20, episode reward: 81.700, mean reward: 0.456 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 171.424 [24.000, 255.000], loss: 0.028338, mean_absolute_error: 5.857192, mean_q: 6.866864, mean_eps: 0.836686\n",
      "  181730/2000000: episode: 967, duration: 9.026s, episode steps: 180, steps per second: 20, episode reward: 93.700, mean reward: 0.521 [-1.000, 1.000], mean action: 2.933 [0.000, 6.000], mean observation: 171.041 [22.000, 255.000], loss: 0.031680, mean_absolute_error: 5.752635, mean_q: 6.721765, mean_eps: 0.836524\n",
      "  181918/2000000: episode: 968, duration: 9.650s, episode steps: 188, steps per second: 19, episode reward: 145.000, mean reward: 0.771 [-1.000, 1.000], mean action: 3.197 [0.000, 6.000], mean observation: 171.129 [24.000, 255.000], loss: 0.027964, mean_absolute_error: 5.895677, mean_q: 6.908694, mean_eps: 0.836358\n",
      "  182049/2000000: episode: 969, duration: 6.293s, episode steps: 131, steps per second: 21, episode reward: 47.200, mean reward: 0.360 [-1.000, 0.500], mean action: 2.565 [0.000, 6.000], mean observation: 172.127 [23.000, 255.000], loss: 0.025976, mean_absolute_error: 5.707175, mean_q: 6.663039, mean_eps: 0.836214\n",
      "  182180/2000000: episode: 970, duration: 6.214s, episode steps: 131, steps per second: 21, episode reward: 45.200, mean reward: 0.345 [-1.000, 0.500], mean action: 2.702 [0.000, 6.000], mean observation: 172.484 [23.000, 255.000], loss: 0.028168, mean_absolute_error: 5.805224, mean_q: 6.783381, mean_eps: 0.836097\n",
      "  182359/2000000: episode: 971, duration: 8.932s, episode steps: 179, steps per second: 20, episode reward: 122.700, mean reward: 0.685 [-1.000, 1.000], mean action: 2.715 [0.000, 6.000], mean observation: 171.043 [23.000, 255.000], loss: 0.031761, mean_absolute_error: 5.818126, mean_q: 6.815416, mean_eps: 0.835959\n",
      "  182525/2000000: episode: 972, duration: 8.154s, episode steps: 166, steps per second: 20, episode reward: 97.200, mean reward: 0.586 [-1.000, 1.000], mean action: 3.048 [0.000, 6.000], mean observation: 170.611 [23.000, 255.000], loss: 0.024296, mean_absolute_error: 5.950239, mean_q: 6.944361, mean_eps: 0.835802\n",
      "  182747/2000000: episode: 973, duration: 11.694s, episode steps: 222, steps per second: 19, episode reward: 97.200, mean reward: 0.438 [-1.000, 1.000], mean action: 3.167 [0.000, 6.000], mean observation: 172.731 [23.000, 255.000], loss: 0.022085, mean_absolute_error: 5.833915, mean_q: 6.815562, mean_eps: 0.835628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  182905/2000000: episode: 974, duration: 7.728s, episode steps: 158, steps per second: 20, episode reward: 65.500, mean reward: 0.415 [-1.000, 0.500], mean action: 2.987 [0.000, 6.000], mean observation: 171.066 [24.000, 255.000], loss: 0.025503, mean_absolute_error: 5.821872, mean_q: 6.790817, mean_eps: 0.835457\n",
      "  183086/2000000: episode: 975, duration: 9.057s, episode steps: 181, steps per second: 20, episode reward: 121.400, mean reward: 0.671 [-1.000, 1.000], mean action: 3.055 [0.000, 6.000], mean observation: 170.754 [24.000, 255.000], loss: 0.027451, mean_absolute_error: 5.793579, mean_q: 6.765702, mean_eps: 0.835304\n",
      "  183231/2000000: episode: 976, duration: 7.037s, episode steps: 145, steps per second: 21, episode reward: 54.600, mean reward: 0.377 [-1.000, 0.500], mean action: 2.779 [0.000, 6.000], mean observation: 171.888 [23.000, 255.000], loss: 0.035717, mean_absolute_error: 5.860497, mean_q: 6.847341, mean_eps: 0.835158\n",
      "  183379/2000000: episode: 977, duration: 7.081s, episode steps: 148, steps per second: 21, episode reward: 49.700, mean reward: 0.336 [-1.000, 0.500], mean action: 2.986 [0.000, 6.000], mean observation: 171.833 [23.000, 255.000], loss: 0.026664, mean_absolute_error: 5.785907, mean_q: 6.768906, mean_eps: 0.835026\n",
      "  183552/2000000: episode: 978, duration: 8.499s, episode steps: 173, steps per second: 20, episode reward: 82.500, mean reward: 0.477 [-1.000, 1.000], mean action: 3.006 [0.000, 6.000], mean observation: 171.305 [23.000, 255.000], loss: 0.022309, mean_absolute_error: 5.739290, mean_q: 6.703742, mean_eps: 0.834882\n",
      "  183702/2000000: episode: 979, duration: 8.044s, episode steps: 150, steps per second: 19, episode reward: 85.900, mean reward: 0.573 [-1.000, 1.000], mean action: 3.313 [0.000, 6.000], mean observation: 171.882 [23.000, 255.000], loss: 0.024567, mean_absolute_error: 5.786326, mean_q: 6.759795, mean_eps: 0.834737\n",
      "  183868/2000000: episode: 980, duration: 8.131s, episode steps: 166, steps per second: 20, episode reward: 68.300, mean reward: 0.411 [-1.000, 0.500], mean action: 2.837 [0.000, 6.000], mean observation: 171.490 [23.000, 255.000], loss: 0.026034, mean_absolute_error: 5.844471, mean_q: 6.818152, mean_eps: 0.834594\n",
      "  184086/2000000: episode: 981, duration: 11.365s, episode steps: 218, steps per second: 19, episode reward: 102.100, mean reward: 0.468 [-1.000, 1.000], mean action: 3.243 [0.000, 6.000], mean observation: 172.707 [20.000, 255.000], loss: 0.022422, mean_absolute_error: 5.689801, mean_q: 6.655796, mean_eps: 0.834422\n",
      "  184262/2000000: episode: 982, duration: 8.694s, episode steps: 176, steps per second: 20, episode reward: 73.900, mean reward: 0.420 [-1.000, 1.000], mean action: 2.841 [0.000, 6.000], mean observation: 172.079 [24.000, 255.000], loss: 0.025134, mean_absolute_error: 5.936643, mean_q: 6.935664, mean_eps: 0.834243\n",
      "  184457/2000000: episode: 983, duration: 9.975s, episode steps: 195, steps per second: 20, episode reward: 119.000, mean reward: 0.610 [-1.000, 1.000], mean action: 2.867 [0.000, 6.000], mean observation: 172.483 [24.000, 255.000], loss: 0.027692, mean_absolute_error: 5.928773, mean_q: 6.938603, mean_eps: 0.834076\n",
      "  184596/2000000: episode: 984, duration: 7.531s, episode steps: 139, steps per second: 18, episode reward: 77.300, mean reward: 0.556 [-1.000, 1.000], mean action: 3.468 [0.000, 6.000], mean observation: 171.732 [24.000, 255.000], loss: 0.027113, mean_absolute_error: 5.823537, mean_q: 6.805382, mean_eps: 0.833927\n",
      "  184777/2000000: episode: 985, duration: 9.024s, episode steps: 181, steps per second: 20, episode reward: 111.500, mean reward: 0.616 [-1.000, 1.000], mean action: 2.956 [0.000, 6.000], mean observation: 171.165 [23.000, 255.000], loss: 0.022709, mean_absolute_error: 5.815951, mean_q: 6.797949, mean_eps: 0.833783\n",
      "  184962/2000000: episode: 986, duration: 9.305s, episode steps: 185, steps per second: 20, episode reward: 99.000, mean reward: 0.535 [-1.000, 1.000], mean action: 3.086 [0.000, 6.000], mean observation: 171.631 [23.000, 255.000], loss: 0.020067, mean_absolute_error: 5.733268, mean_q: 6.689844, mean_eps: 0.833617\n",
      "  185096/2000000: episode: 987, duration: 6.394s, episode steps: 134, steps per second: 21, episode reward: 48.300, mean reward: 0.360 [-1.000, 0.500], mean action: 2.836 [0.000, 6.000], mean observation: 172.622 [24.000, 255.000], loss: 0.021926, mean_absolute_error: 5.864996, mean_q: 6.859823, mean_eps: 0.833475\n",
      "  185307/2000000: episode: 988, duration: 10.990s, episode steps: 211, steps per second: 19, episode reward: 135.700, mean reward: 0.643 [-1.000, 1.000], mean action: 3.218 [0.000, 6.000], mean observation: 172.675 [23.000, 255.000], loss: 0.027988, mean_absolute_error: 5.899475, mean_q: 6.885749, mean_eps: 0.833320\n",
      "  185489/2000000: episode: 989, duration: 9.164s, episode steps: 182, steps per second: 20, episode reward: 120.800, mean reward: 0.664 [-1.000, 1.000], mean action: 3.044 [0.000, 6.000], mean observation: 171.197 [24.000, 255.000], loss: 0.020970, mean_absolute_error: 5.787140, mean_q: 6.752204, mean_eps: 0.833142\n",
      "  185667/2000000: episode: 990, duration: 8.867s, episode steps: 178, steps per second: 20, episode reward: 103.900, mean reward: 0.584 [-1.000, 1.000], mean action: 3.152 [0.000, 6.000], mean observation: 171.348 [23.000, 255.000], loss: 0.026535, mean_absolute_error: 5.863249, mean_q: 6.853309, mean_eps: 0.832980\n",
      "  185860/2000000: episode: 991, duration: 9.803s, episode steps: 193, steps per second: 20, episode reward: 127.900, mean reward: 0.663 [-1.000, 1.000], mean action: 3.057 [0.000, 6.000], mean observation: 171.986 [23.000, 255.000], loss: 0.021980, mean_absolute_error: 5.786814, mean_q: 6.755661, mean_eps: 0.832814\n",
      "  186002/2000000: episode: 992, duration: 7.656s, episode steps: 142, steps per second: 19, episode reward: 78.700, mean reward: 0.554 [-1.000, 1.000], mean action: 3.401 [0.000, 6.000], mean observation: 172.471 [23.000, 255.000], loss: 0.023499, mean_absolute_error: 5.838356, mean_q: 6.820750, mean_eps: 0.832663\n",
      "  186180/2000000: episode: 993, duration: 8.795s, episode steps: 178, steps per second: 20, episode reward: 70.700, mean reward: 0.397 [-1.000, 0.500], mean action: 3.062 [0.000, 6.000], mean observation: 172.547 [24.000, 255.000], loss: 0.025650, mean_absolute_error: 5.804933, mean_q: 6.765286, mean_eps: 0.832519\n",
      "  186310/2000000: episode: 994, duration: 6.161s, episode steps: 130, steps per second: 21, episode reward: 45.500, mean reward: 0.350 [-1.000, 0.500], mean action: 2.762 [0.000, 6.000], mean observation: 173.152 [23.000, 255.000], loss: 0.025753, mean_absolute_error: 6.042476, mean_q: 7.066524, mean_eps: 0.832380\n",
      "  186512/2000000: episode: 995, duration: 10.482s, episode steps: 202, steps per second: 19, episode reward: 139.300, mean reward: 0.690 [-1.000, 1.000], mean action: 3.114 [0.000, 6.000], mean observation: 172.844 [23.000, 255.000], loss: 0.021787, mean_absolute_error: 5.799868, mean_q: 6.773919, mean_eps: 0.832231\n",
      "  186703/2000000: episode: 996, duration: 9.655s, episode steps: 191, steps per second: 20, episode reward: 88.000, mean reward: 0.461 [-1.000, 0.500], mean action: 3.052 [0.000, 6.000], mean observation: 172.959 [24.000, 255.000], loss: 0.023694, mean_absolute_error: 5.877593, mean_q: 6.872788, mean_eps: 0.832055\n",
      "  186882/2000000: episode: 997, duration: 8.885s, episode steps: 179, steps per second: 20, episode reward: 76.000, mean reward: 0.425 [-1.000, 0.500], mean action: 2.955 [0.000, 6.000], mean observation: 172.391 [24.000, 255.000], loss: 0.026906, mean_absolute_error: 5.908606, mean_q: 6.908047, mean_eps: 0.831887\n",
      "  187003/2000000: episode: 998, duration: 5.726s, episode steps: 121, steps per second: 21, episode reward: 43.800, mean reward: 0.362 [-1.000, 0.500], mean action: 2.488 [0.000, 6.000], mean observation: 173.077 [24.000, 255.000], loss: 0.019676, mean_absolute_error: 5.852783, mean_q: 6.816490, mean_eps: 0.831752\n",
      "  187187/2000000: episode: 999, duration: 9.279s, episode steps: 184, steps per second: 20, episode reward: 124.900, mean reward: 0.679 [-1.000, 1.000], mean action: 3.038 [0.000, 6.000], mean observation: 171.798 [24.000, 255.000], loss: 0.018909, mean_absolute_error: 5.792041, mean_q: 6.762473, mean_eps: 0.831615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  187364/2000000: episode: 1000, duration: 8.793s, episode steps: 177, steps per second: 20, episode reward: 96.500, mean reward: 0.545 [-1.000, 1.000], mean action: 2.893 [0.000, 6.000], mean observation: 171.831 [23.000, 255.000], loss: 0.024269, mean_absolute_error: 5.810051, mean_q: 6.791269, mean_eps: 0.831453\n",
      "  187540/2000000: episode: 1001, duration: 8.764s, episode steps: 176, steps per second: 20, episode reward: 112.800, mean reward: 0.641 [-1.000, 1.000], mean action: 2.864 [0.000, 6.000], mean observation: 171.585 [24.000, 255.000], loss: 0.022028, mean_absolute_error: 5.936697, mean_q: 6.935905, mean_eps: 0.831295\n",
      "  187751/2000000: episode: 1002, duration: 10.980s, episode steps: 211, steps per second: 19, episode reward: 148.100, mean reward: 0.702 [-1.000, 1.000], mean action: 3.204 [0.000, 6.000], mean observation: 173.415 [24.000, 255.000], loss: 0.022299, mean_absolute_error: 5.744198, mean_q: 6.715208, mean_eps: 0.831120\n",
      "  187968/2000000: episode: 1003, duration: 11.356s, episode steps: 217, steps per second: 19, episode reward: 170.800, mean reward: 0.787 [-1.000, 1.000], mean action: 3.106 [0.000, 6.000], mean observation: 172.595 [24.000, 255.000], loss: 0.025603, mean_absolute_error: 5.779098, mean_q: 6.741445, mean_eps: 0.830928\n",
      "  188111/2000000: episode: 1004, duration: 7.693s, episode steps: 143, steps per second: 19, episode reward: 80.800, mean reward: 0.565 [-1.000, 1.000], mean action: 3.252 [0.000, 6.000], mean observation: 172.877 [23.000, 255.000], loss: 0.022239, mean_absolute_error: 5.692518, mean_q: 6.633166, mean_eps: 0.830766\n",
      "  188317/2000000: episode: 1005, duration: 10.823s, episode steps: 206, steps per second: 19, episode reward: 152.000, mean reward: 0.738 [-1.000, 1.000], mean action: 3.078 [0.000, 6.000], mean observation: 173.285 [24.000, 255.000], loss: 0.023784, mean_absolute_error: 5.843012, mean_q: 6.822003, mean_eps: 0.830607\n",
      "  188454/2000000: episode: 1006, duration: 6.510s, episode steps: 137, steps per second: 21, episode reward: 49.400, mean reward: 0.361 [-1.000, 0.500], mean action: 2.591 [0.000, 6.000], mean observation: 173.314 [24.000, 255.000], loss: 0.028919, mean_absolute_error: 6.005969, mean_q: 7.008393, mean_eps: 0.830453\n",
      "  188647/2000000: episode: 1007, duration: 9.874s, episode steps: 193, steps per second: 20, episode reward: 119.200, mean reward: 0.618 [-1.000, 1.000], mean action: 3.098 [0.000, 6.000], mean observation: 172.323 [24.000, 255.000], loss: 0.025361, mean_absolute_error: 5.901443, mean_q: 6.882038, mean_eps: 0.830305\n",
      "  188826/2000000: episode: 1008, duration: 8.922s, episode steps: 179, steps per second: 20, episode reward: 123.200, mean reward: 0.688 [-1.000, 1.000], mean action: 2.883 [0.000, 6.000], mean observation: 171.770 [25.000, 255.000], loss: 0.027350, mean_absolute_error: 5.997137, mean_q: 7.011312, mean_eps: 0.830138\n",
      "  189022/2000000: episode: 1009, duration: 10.046s, episode steps: 196, steps per second: 20, episode reward: 121.100, mean reward: 0.618 [-1.000, 1.000], mean action: 2.939 [0.000, 6.000], mean observation: 172.945 [23.000, 255.000], loss: 0.024442, mean_absolute_error: 5.807880, mean_q: 6.791530, mean_eps: 0.829968\n",
      "  189190/2000000: episode: 1010, duration: 8.323s, episode steps: 168, steps per second: 20, episode reward: 69.700, mean reward: 0.415 [-1.000, 0.500], mean action: 3.060 [0.000, 6.000], mean observation: 172.371 [24.000, 255.000], loss: 0.020492, mean_absolute_error: 5.772738, mean_q: 6.744152, mean_eps: 0.829805\n",
      "  189378/2000000: episode: 1011, duration: 9.662s, episode steps: 188, steps per second: 19, episode reward: 140.900, mean reward: 0.749 [-1.000, 1.000], mean action: 2.926 [0.000, 6.000], mean observation: 172.048 [24.000, 255.000], loss: 0.023141, mean_absolute_error: 5.965852, mean_q: 6.976634, mean_eps: 0.829644\n",
      "  189517/2000000: episode: 1012, duration: 7.490s, episode steps: 139, steps per second: 19, episode reward: 75.900, mean reward: 0.546 [-1.000, 1.000], mean action: 3.165 [0.000, 6.000], mean observation: 173.011 [23.000, 255.000], loss: 0.022542, mean_absolute_error: 5.828371, mean_q: 6.809558, mean_eps: 0.829497\n",
      "  189650/2000000: episode: 1013, duration: 6.288s, episode steps: 133, steps per second: 21, episode reward: 45.000, mean reward: 0.338 [-1.000, 0.500], mean action: 2.684 [0.000, 6.000], mean observation: 173.650 [23.000, 255.000], loss: 0.023119, mean_absolute_error: 5.921730, mean_q: 6.922212, mean_eps: 0.829374\n",
      "  189798/2000000: episode: 1014, duration: 7.147s, episode steps: 148, steps per second: 21, episode reward: 58.900, mean reward: 0.398 [-1.000, 0.500], mean action: 2.959 [0.000, 6.000], mean observation: 172.626 [23.000, 255.000], loss: 0.023597, mean_absolute_error: 5.872942, mean_q: 6.865687, mean_eps: 0.829248\n",
      "  189987/2000000: episode: 1015, duration: 9.579s, episode steps: 189, steps per second: 20, episode reward: 128.900, mean reward: 0.682 [-1.000, 1.000], mean action: 3.101 [0.000, 6.000], mean observation: 171.898 [23.000, 255.000], loss: 0.022698, mean_absolute_error: 5.732022, mean_q: 6.685739, mean_eps: 0.829097\n",
      "  190182/2000000: episode: 1016, duration: 9.970s, episode steps: 195, steps per second: 20, episode reward: 137.000, mean reward: 0.703 [-1.000, 1.000], mean action: 2.974 [0.000, 6.000], mean observation: 172.514 [24.000, 255.000], loss: 0.044731, mean_absolute_error: 6.289856, mean_q: 7.352973, mean_eps: 0.828924\n",
      "  190324/2000000: episode: 1017, duration: 7.653s, episode steps: 142, steps per second: 19, episode reward: 79.700, mean reward: 0.561 [-1.000, 1.000], mean action: 3.451 [0.000, 6.000], mean observation: 172.566 [23.000, 255.000], loss: 0.033702, mean_absolute_error: 6.169093, mean_q: 7.201746, mean_eps: 0.828773\n",
      "  190526/2000000: episode: 1018, duration: 10.402s, episode steps: 202, steps per second: 19, episode reward: 145.900, mean reward: 0.722 [-1.000, 1.000], mean action: 2.990 [0.000, 6.000], mean observation: 173.454 [24.000, 255.000], loss: 0.033294, mean_absolute_error: 6.290970, mean_q: 7.367178, mean_eps: 0.828618\n",
      "  190753/2000000: episode: 1019, duration: 11.791s, episode steps: 227, steps per second: 19, episode reward: 118.500, mean reward: 0.522 [-1.000, 1.000], mean action: 3.216 [0.000, 6.000], mean observation: 173.438 [23.000, 255.000], loss: 0.029860, mean_absolute_error: 6.270090, mean_q: 7.322866, mean_eps: 0.828424\n",
      "  190958/2000000: episode: 1020, duration: 10.635s, episode steps: 205, steps per second: 19, episode reward: 126.200, mean reward: 0.616 [-1.000, 1.000], mean action: 3.146 [0.000, 6.000], mean observation: 172.557 [23.000, 255.000], loss: 0.027934, mean_absolute_error: 6.165447, mean_q: 7.206231, mean_eps: 0.828230\n",
      "  191117/2000000: episode: 1021, duration: 8.498s, episode steps: 159, steps per second: 19, episode reward: 82.000, mean reward: 0.516 [-1.000, 1.000], mean action: 3.151 [0.000, 6.000], mean observation: 173.257 [24.000, 255.000], loss: 0.032008, mean_absolute_error: 6.215232, mean_q: 7.284879, mean_eps: 0.828066\n",
      "  191321/2000000: episode: 1022, duration: 10.490s, episode steps: 204, steps per second: 19, episode reward: 106.800, mean reward: 0.524 [-1.000, 1.000], mean action: 3.010 [0.000, 6.000], mean observation: 172.611 [24.000, 255.000], loss: 0.028229, mean_absolute_error: 6.271649, mean_q: 7.346821, mean_eps: 0.827902\n",
      "  191508/2000000: episode: 1023, duration: 9.491s, episode steps: 187, steps per second: 20, episode reward: 129.400, mean reward: 0.692 [-1.000, 1.000], mean action: 3.107 [0.000, 6.000], mean observation: 171.366 [24.000, 255.000], loss: 0.025822, mean_absolute_error: 6.296019, mean_q: 7.361050, mean_eps: 0.827727\n",
      "  191685/2000000: episode: 1024, duration: 8.827s, episode steps: 177, steps per second: 20, episode reward: 107.300, mean reward: 0.606 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 171.382 [24.000, 255.000], loss: 0.032268, mean_absolute_error: 6.169252, mean_q: 7.198775, mean_eps: 0.827564\n",
      "  191890/2000000: episode: 1025, duration: 10.654s, episode steps: 205, steps per second: 19, episode reward: 149.900, mean reward: 0.731 [-1.000, 1.000], mean action: 3.268 [0.000, 6.000], mean observation: 172.566 [24.000, 255.000], loss: 0.028885, mean_absolute_error: 6.274174, mean_q: 7.337202, mean_eps: 0.827391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  192119/2000000: episode: 1026, duration: 11.920s, episode steps: 229, steps per second: 19, episode reward: 130.200, mean reward: 0.569 [-1.000, 1.000], mean action: 3.231 [0.000, 6.000], mean observation: 173.438 [24.000, 255.000], loss: 0.026656, mean_absolute_error: 6.355163, mean_q: 7.422024, mean_eps: 0.827196\n",
      "  192301/2000000: episode: 1027, duration: 9.149s, episode steps: 182, steps per second: 20, episode reward: 126.700, mean reward: 0.696 [-1.000, 1.000], mean action: 2.907 [0.000, 6.000], mean observation: 171.214 [24.000, 255.000], loss: 0.029747, mean_absolute_error: 6.488971, mean_q: 7.590756, mean_eps: 0.827011\n",
      "  192498/2000000: episode: 1028, duration: 10.042s, episode steps: 197, steps per second: 20, episode reward: 108.500, mean reward: 0.551 [-1.000, 1.000], mean action: 3.147 [0.000, 6.000], mean observation: 172.140 [24.000, 255.000], loss: 0.027907, mean_absolute_error: 6.249208, mean_q: 7.306063, mean_eps: 0.826840\n",
      "  192626/2000000: episode: 1029, duration: 6.907s, episode steps: 128, steps per second: 19, episode reward: 71.700, mean reward: 0.560 [-1.000, 1.000], mean action: 3.430 [0.000, 6.000], mean observation: 173.366 [24.000, 255.000], loss: 0.031310, mean_absolute_error: 6.362676, mean_q: 7.440624, mean_eps: 0.826694\n",
      "  192834/2000000: episode: 1030, duration: 10.610s, episode steps: 208, steps per second: 20, episode reward: 111.900, mean reward: 0.538 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 172.811 [24.000, 255.000], loss: 0.031074, mean_absolute_error: 6.389511, mean_q: 7.477043, mean_eps: 0.826543\n",
      "  193067/2000000: episode: 1031, duration: 12.297s, episode steps: 233, steps per second: 19, episode reward: 126.600, mean reward: 0.543 [-1.000, 1.000], mean action: 3.219 [0.000, 6.000], mean observation: 173.372 [23.000, 255.000], loss: 0.027719, mean_absolute_error: 6.260049, mean_q: 7.322096, mean_eps: 0.826345\n",
      "  193250/2000000: episode: 1032, duration: 9.239s, episode steps: 183, steps per second: 20, episode reward: 108.800, mean reward: 0.595 [-1.000, 1.000], mean action: 2.962 [0.000, 6.000], mean observation: 171.200 [24.000, 255.000], loss: 0.030325, mean_absolute_error: 6.312685, mean_q: 7.375306, mean_eps: 0.826158\n",
      "  193429/2000000: episode: 1033, duration: 8.853s, episode steps: 179, steps per second: 20, episode reward: 72.000, mean reward: 0.402 [-1.000, 0.500], mean action: 3.050 [0.000, 6.000], mean observation: 171.801 [24.000, 255.000], loss: 0.027833, mean_absolute_error: 6.429782, mean_q: 7.520354, mean_eps: 0.825994\n",
      "  193642/2000000: episode: 1034, duration: 11.083s, episode steps: 213, steps per second: 19, episode reward: 132.200, mean reward: 0.621 [-1.000, 1.000], mean action: 3.028 [0.000, 6.000], mean observation: 172.408 [25.000, 255.000], loss: 0.028023, mean_absolute_error: 6.399359, mean_q: 7.481226, mean_eps: 0.825818\n",
      "  193756/2000000: episode: 1035, duration: 6.284s, episode steps: 114, steps per second: 18, episode reward: 66.000, mean reward: 0.579 [-1.000, 1.000], mean action: 3.482 [0.000, 6.000], mean observation: 173.462 [24.000, 255.000], loss: 0.026532, mean_absolute_error: 6.363751, mean_q: 7.439620, mean_eps: 0.825672\n",
      "  193937/2000000: episode: 1036, duration: 9.092s, episode steps: 181, steps per second: 20, episode reward: 73.400, mean reward: 0.406 [-1.000, 0.500], mean action: 3.133 [0.000, 6.000], mean observation: 172.117 [22.000, 255.000], loss: 0.027708, mean_absolute_error: 6.355847, mean_q: 7.420608, mean_eps: 0.825539\n",
      "  194124/2000000: episode: 1037, duration: 9.619s, episode steps: 187, steps per second: 19, episode reward: 133.400, mean reward: 0.713 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 171.418 [24.000, 255.000], loss: 0.028092, mean_absolute_error: 6.252102, mean_q: 7.304701, mean_eps: 0.825373\n",
      "  194343/2000000: episode: 1038, duration: 11.422s, episode steps: 219, steps per second: 19, episode reward: 148.400, mean reward: 0.678 [-1.000, 1.000], mean action: 3.237 [0.000, 6.000], mean observation: 172.520 [23.000, 255.000], loss: 0.028785, mean_absolute_error: 6.217447, mean_q: 7.262422, mean_eps: 0.825191\n",
      "  194551/2000000: episode: 1039, duration: 10.913s, episode steps: 208, steps per second: 19, episode reward: 133.000, mean reward: 0.639 [-1.000, 1.000], mean action: 3.053 [0.000, 6.000], mean observation: 172.346 [24.000, 255.000], loss: 0.029316, mean_absolute_error: 6.215718, mean_q: 7.279824, mean_eps: 0.824999\n",
      "  194760/2000000: episode: 1040, duration: 10.821s, episode steps: 209, steps per second: 19, episode reward: 126.100, mean reward: 0.603 [-1.000, 1.000], mean action: 2.904 [0.000, 6.000], mean observation: 172.928 [24.000, 255.000], loss: 0.025803, mean_absolute_error: 6.353433, mean_q: 7.430637, mean_eps: 0.824811\n",
      "  194973/2000000: episode: 1041, duration: 11.236s, episode steps: 213, steps per second: 19, episode reward: 114.900, mean reward: 0.539 [-1.000, 1.000], mean action: 3.066 [0.000, 6.000], mean observation: 172.463 [24.000, 255.000], loss: 0.026707, mean_absolute_error: 6.393523, mean_q: 7.478844, mean_eps: 0.824621\n",
      "  195173/2000000: episode: 1042, duration: 10.457s, episode steps: 200, steps per second: 19, episode reward: 125.400, mean reward: 0.627 [-1.000, 1.000], mean action: 2.935 [0.000, 6.000], mean observation: 173.182 [23.000, 255.000], loss: 0.028683, mean_absolute_error: 6.432522, mean_q: 7.522699, mean_eps: 0.824433\n",
      "  195350/2000000: episode: 1043, duration: 8.861s, episode steps: 177, steps per second: 20, episode reward: 107.200, mean reward: 0.606 [-1.000, 1.000], mean action: 2.983 [0.000, 6.000], mean observation: 171.744 [24.000, 255.000], loss: 0.026430, mean_absolute_error: 6.160193, mean_q: 7.206465, mean_eps: 0.824264\n",
      "  195530/2000000: episode: 1044, duration: 9.077s, episode steps: 180, steps per second: 20, episode reward: 126.200, mean reward: 0.701 [-1.000, 1.000], mean action: 2.822 [0.000, 6.000], mean observation: 171.874 [23.000, 255.000], loss: 0.029326, mean_absolute_error: 6.395136, mean_q: 7.473610, mean_eps: 0.824104\n",
      "  195710/2000000: episode: 1045, duration: 9.078s, episode steps: 180, steps per second: 20, episode reward: 78.400, mean reward: 0.436 [-1.000, 1.000], mean action: 2.878 [0.000, 6.000], mean observation: 172.205 [24.000, 255.000], loss: 0.031451, mean_absolute_error: 6.140041, mean_q: 7.191587, mean_eps: 0.823942\n",
      "  195839/2000000: episode: 1046, duration: 7.001s, episode steps: 129, steps per second: 18, episode reward: 75.100, mean reward: 0.582 [-1.000, 1.000], mean action: 3.318 [0.000, 6.000], mean observation: 173.011 [23.000, 255.000], loss: 0.027924, mean_absolute_error: 6.493816, mean_q: 7.586655, mean_eps: 0.823803\n",
      "  196066/2000000: episode: 1047, duration: 11.975s, episode steps: 227, steps per second: 19, episode reward: 140.200, mean reward: 0.618 [-1.000, 1.000], mean action: 3.163 [0.000, 6.000], mean observation: 173.143 [24.000, 255.000], loss: 0.024797, mean_absolute_error: 6.272278, mean_q: 7.329130, mean_eps: 0.823643\n",
      "  196250/2000000: episode: 1048, duration: 9.402s, episode steps: 184, steps per second: 20, episode reward: 126.200, mean reward: 0.686 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 171.346 [23.000, 255.000], loss: 0.026054, mean_absolute_error: 6.360568, mean_q: 7.448939, mean_eps: 0.823458\n",
      "  196455/2000000: episode: 1049, duration: 10.668s, episode steps: 205, steps per second: 19, episode reward: 99.400, mean reward: 0.485 [-1.000, 1.000], mean action: 3.093 [0.000, 6.000], mean observation: 172.918 [24.000, 255.000], loss: 0.031387, mean_absolute_error: 6.359593, mean_q: 7.433141, mean_eps: 0.823283\n",
      "  196650/2000000: episode: 1050, duration: 10.098s, episode steps: 195, steps per second: 19, episode reward: 121.700, mean reward: 0.624 [-1.000, 1.000], mean action: 3.021 [0.000, 6.000], mean observation: 172.797 [24.000, 255.000], loss: 0.037231, mean_absolute_error: 6.390457, mean_q: 7.466388, mean_eps: 0.823103\n",
      "  196776/2000000: episode: 1051, duration: 6.875s, episode steps: 126, steps per second: 18, episode reward: 71.800, mean reward: 0.570 [-1.000, 1.000], mean action: 3.579 [0.000, 6.000], mean observation: 173.479 [24.000, 255.000], loss: 0.031054, mean_absolute_error: 6.435656, mean_q: 7.533223, mean_eps: 0.822959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  196970/2000000: episode: 1052, duration: 9.903s, episode steps: 194, steps per second: 20, episode reward: 109.200, mean reward: 0.563 [-1.000, 1.000], mean action: 2.928 [0.000, 6.000], mean observation: 172.249 [25.000, 255.000], loss: 0.029295, mean_absolute_error: 6.281114, mean_q: 7.343425, mean_eps: 0.822815\n",
      "  197126/2000000: episode: 1053, duration: 8.421s, episode steps: 156, steps per second: 19, episode reward: 77.000, mean reward: 0.494 [-1.000, 1.000], mean action: 3.090 [0.000, 6.000], mean observation: 174.441 [23.000, 255.000], loss: 0.027701, mean_absolute_error: 6.310555, mean_q: 7.369837, mean_eps: 0.822657\n",
      "  197257/2000000: episode: 1054, duration: 7.115s, episode steps: 131, steps per second: 18, episode reward: 73.100, mean reward: 0.558 [-1.000, 1.000], mean action: 3.389 [0.000, 6.000], mean observation: 172.979 [24.000, 255.000], loss: 0.026834, mean_absolute_error: 6.326372, mean_q: 7.389417, mean_eps: 0.822527\n",
      "  197405/2000000: episode: 1055, duration: 7.961s, episode steps: 148, steps per second: 19, episode reward: 77.600, mean reward: 0.524 [-1.000, 1.000], mean action: 3.480 [0.000, 6.000], mean observation: 173.105 [23.000, 255.000], loss: 0.027860, mean_absolute_error: 6.346757, mean_q: 7.406429, mean_eps: 0.822401\n",
      "  197542/2000000: episode: 1056, duration: 6.500s, episode steps: 137, steps per second: 21, episode reward: 44.200, mean reward: 0.323 [-1.000, 0.500], mean action: 2.672 [0.000, 6.000], mean observation: 173.501 [23.000, 255.000], loss: 0.030894, mean_absolute_error: 6.217273, mean_q: 7.259656, mean_eps: 0.822273\n",
      "  197710/2000000: episode: 1057, duration: 8.203s, episode steps: 168, steps per second: 20, episode reward: 71.600, mean reward: 0.426 [-1.000, 1.000], mean action: 3.083 [0.000, 6.000], mean observation: 172.397 [23.000, 255.000], loss: 0.029640, mean_absolute_error: 6.499012, mean_q: 7.580310, mean_eps: 0.822137\n",
      "  197890/2000000: episode: 1058, duration: 8.942s, episode steps: 180, steps per second: 20, episode reward: 94.200, mean reward: 0.523 [-1.000, 1.000], mean action: 2.922 [0.000, 6.000], mean observation: 173.015 [24.000, 255.000], loss: 0.029513, mean_absolute_error: 6.389709, mean_q: 7.473701, mean_eps: 0.821980\n",
      "  198074/2000000: episode: 1059, duration: 9.313s, episode steps: 184, steps per second: 20, episode reward: 121.700, mean reward: 0.661 [-1.000, 1.000], mean action: 2.880 [0.000, 6.000], mean observation: 172.459 [23.000, 255.000], loss: 0.027806, mean_absolute_error: 6.329496, mean_q: 7.379961, mean_eps: 0.821816\n",
      "  198240/2000000: episode: 1060, duration: 8.058s, episode steps: 166, steps per second: 21, episode reward: 67.500, mean reward: 0.407 [-1.000, 0.500], mean action: 2.892 [0.000, 6.000], mean observation: 172.644 [24.000, 255.000], loss: 0.028264, mean_absolute_error: 6.423413, mean_q: 7.511460, mean_eps: 0.821660\n",
      "  198418/2000000: episode: 1061, duration: 8.871s, episode steps: 178, steps per second: 20, episode reward: 76.300, mean reward: 0.429 [-1.000, 0.500], mean action: 3.174 [0.000, 6.000], mean observation: 172.456 [23.000, 255.000], loss: 0.029307, mean_absolute_error: 6.301694, mean_q: 7.355393, mean_eps: 0.821505\n",
      "  198620/2000000: episode: 1062, duration: 10.446s, episode steps: 202, steps per second: 19, episode reward: 110.600, mean reward: 0.548 [-1.000, 1.000], mean action: 3.114 [0.000, 6.000], mean observation: 173.498 [24.000, 255.000], loss: 0.028302, mean_absolute_error: 6.250802, mean_q: 7.298086, mean_eps: 0.821334\n",
      "  198798/2000000: episode: 1063, duration: 8.892s, episode steps: 178, steps per second: 20, episode reward: 123.100, mean reward: 0.692 [-1.000, 1.000], mean action: 2.848 [0.000, 6.000], mean observation: 171.755 [24.000, 255.000], loss: 0.024040, mean_absolute_error: 6.339676, mean_q: 7.411600, mean_eps: 0.821163\n",
      "  198946/2000000: episode: 1064, duration: 7.982s, episode steps: 148, steps per second: 19, episode reward: 83.400, mean reward: 0.564 [-1.000, 1.000], mean action: 3.385 [0.000, 6.000], mean observation: 173.663 [23.000, 255.000], loss: 0.027299, mean_absolute_error: 6.459715, mean_q: 7.557078, mean_eps: 0.821015\n",
      "  199136/2000000: episode: 1065, duration: 9.765s, episode steps: 190, steps per second: 19, episode reward: 120.100, mean reward: 0.632 [-1.000, 1.000], mean action: 3.068 [0.000, 6.000], mean observation: 172.679 [24.000, 255.000], loss: 0.025789, mean_absolute_error: 6.319776, mean_q: 7.399515, mean_eps: 0.820864\n",
      "  199317/2000000: episode: 1066, duration: 9.071s, episode steps: 181, steps per second: 20, episode reward: 69.400, mean reward: 0.383 [-1.000, 0.500], mean action: 3.133 [0.000, 6.000], mean observation: 172.648 [24.000, 255.000], loss: 0.032485, mean_absolute_error: 6.367706, mean_q: 7.446256, mean_eps: 0.820697\n",
      "  199422/2000000: episode: 1067, duration: 5.010s, episode steps: 105, steps per second: 21, episode reward: 39.000, mean reward: 0.371 [-1.000, 0.500], mean action: 2.362 [0.000, 6.000], mean observation: 172.401 [22.000, 255.000], loss: 0.029444, mean_absolute_error: 6.406587, mean_q: 7.495202, mean_eps: 0.820567\n",
      "  199628/2000000: episode: 1068, duration: 10.845s, episode steps: 206, steps per second: 19, episode reward: 150.300, mean reward: 0.730 [-1.000, 1.000], mean action: 3.218 [0.000, 6.000], mean observation: 172.751 [21.000, 255.000], loss: 0.029730, mean_absolute_error: 6.417383, mean_q: 7.500840, mean_eps: 0.820428\n",
      "  199843/2000000: episode: 1069, duration: 11.401s, episode steps: 215, steps per second: 19, episode reward: 115.100, mean reward: 0.535 [-1.000, 1.000], mean action: 2.981 [0.000, 6.000], mean observation: 173.294 [23.000, 255.000], loss: 0.027974, mean_absolute_error: 6.334787, mean_q: 7.403387, mean_eps: 0.820239\n",
      "  200047/2000000: episode: 1070, duration: 10.450s, episode steps: 204, steps per second: 20, episode reward: 118.000, mean reward: 0.578 [-1.000, 1.000], mean action: 3.098 [0.000, 6.000], mean observation: 173.245 [24.000, 255.000], loss: 0.043596, mean_absolute_error: 6.613796, mean_q: 7.730443, mean_eps: 0.820050\n",
      "  200242/2000000: episode: 1071, duration: 10.020s, episode steps: 195, steps per second: 19, episode reward: 117.800, mean reward: 0.604 [-1.000, 1.000], mean action: 2.800 [0.000, 6.000], mean observation: 172.446 [24.000, 255.000], loss: 0.033885, mean_absolute_error: 6.705649, mean_q: 7.843655, mean_eps: 0.819870\n",
      "  200423/2000000: episode: 1072, duration: 9.048s, episode steps: 181, steps per second: 20, episode reward: 123.000, mean reward: 0.680 [-1.000, 1.000], mean action: 2.928 [0.000, 6.000], mean observation: 171.373 [23.000, 255.000], loss: 0.030583, mean_absolute_error: 6.808581, mean_q: 7.958441, mean_eps: 0.819701\n",
      "  200627/2000000: episode: 1073, duration: 10.538s, episode steps: 204, steps per second: 19, episode reward: 106.300, mean reward: 0.521 [-1.000, 1.000], mean action: 3.108 [0.000, 6.000], mean observation: 173.033 [23.000, 255.000], loss: 0.032195, mean_absolute_error: 6.751438, mean_q: 7.904413, mean_eps: 0.819528\n",
      "  200816/2000000: episode: 1074, duration: 9.697s, episode steps: 189, steps per second: 19, episode reward: 127.400, mean reward: 0.674 [-1.000, 1.000], mean action: 2.947 [0.000, 6.000], mean observation: 171.755 [23.000, 255.000], loss: 0.031998, mean_absolute_error: 6.683963, mean_q: 7.804330, mean_eps: 0.819352\n",
      "  200990/2000000: episode: 1075, duration: 8.786s, episode steps: 174, steps per second: 20, episode reward: 90.300, mean reward: 0.519 [-1.000, 1.000], mean action: 2.960 [0.000, 6.000], mean observation: 171.953 [24.000, 255.000], loss: 0.037564, mean_absolute_error: 6.707414, mean_q: 7.841178, mean_eps: 0.819188\n",
      "  201146/2000000: episode: 1076, duration: 7.535s, episode steps: 156, steps per second: 21, episode reward: 48.100, mean reward: 0.308 [-1.000, 0.500], mean action: 3.103 [0.000, 6.000], mean observation: 173.023 [23.000, 255.000], loss: 0.033027, mean_absolute_error: 6.638500, mean_q: 7.757775, mean_eps: 0.819039\n",
      "  201335/2000000: episode: 1077, duration: 9.681s, episode steps: 189, steps per second: 20, episode reward: 119.500, mean reward: 0.632 [-1.000, 1.000], mean action: 2.958 [0.000, 6.000], mean observation: 172.304 [24.000, 255.000], loss: 0.035847, mean_absolute_error: 6.747209, mean_q: 7.884406, mean_eps: 0.818884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  201538/2000000: episode: 1078, duration: 10.505s, episode steps: 203, steps per second: 19, episode reward: 118.700, mean reward: 0.585 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 173.075 [24.000, 255.000], loss: 0.031379, mean_absolute_error: 6.722366, mean_q: 7.854523, mean_eps: 0.818708\n",
      "  201694/2000000: episode: 1079, duration: 7.588s, episode steps: 156, steps per second: 21, episode reward: 58.500, mean reward: 0.375 [-1.000, 0.500], mean action: 2.923 [0.000, 6.000], mean observation: 172.040 [23.000, 255.000], loss: 0.037458, mean_absolute_error: 6.840010, mean_q: 8.003948, mean_eps: 0.818546\n",
      "  201886/2000000: episode: 1080, duration: 9.865s, episode steps: 192, steps per second: 19, episode reward: 121.600, mean reward: 0.633 [-1.000, 1.000], mean action: 2.943 [0.000, 6.000], mean observation: 171.988 [23.000, 255.000], loss: 0.036938, mean_absolute_error: 6.868239, mean_q: 8.028159, mean_eps: 0.818389\n",
      "  202109/2000000: episode: 1081, duration: 11.371s, episode steps: 223, steps per second: 20, episode reward: 76.800, mean reward: 0.344 [-1.000, 0.500], mean action: 3.233 [0.000, 6.000], mean observation: 173.396 [23.000, 255.000], loss: 0.031749, mean_absolute_error: 6.736753, mean_q: 7.889360, mean_eps: 0.818202\n",
      "  202269/2000000: episode: 1082, duration: 7.709s, episode steps: 160, steps per second: 21, episode reward: 55.700, mean reward: 0.348 [-1.000, 0.500], mean action: 2.744 [0.000, 6.000], mean observation: 172.010 [24.000, 255.000], loss: 0.035753, mean_absolute_error: 6.730354, mean_q: 7.884162, mean_eps: 0.818029\n",
      "  202470/2000000: episode: 1083, duration: 10.390s, episode steps: 201, steps per second: 19, episode reward: 146.200, mean reward: 0.727 [-1.000, 1.000], mean action: 3.100 [0.000, 6.000], mean observation: 172.761 [23.000, 255.000], loss: 0.034152, mean_absolute_error: 6.475009, mean_q: 7.573869, mean_eps: 0.817867\n",
      "  202667/2000000: episode: 1084, duration: 10.128s, episode steps: 197, steps per second: 19, episode reward: 94.300, mean reward: 0.479 [-1.000, 1.000], mean action: 3.274 [0.000, 6.000], mean observation: 172.759 [24.000, 255.000], loss: 0.032242, mean_absolute_error: 6.671151, mean_q: 7.797830, mean_eps: 0.817689\n",
      "  202866/2000000: episode: 1085, duration: 10.205s, episode steps: 199, steps per second: 19, episode reward: 129.600, mean reward: 0.651 [-1.000, 1.000], mean action: 3.010 [0.000, 6.000], mean observation: 172.724 [23.000, 255.000], loss: 0.030949, mean_absolute_error: 6.759958, mean_q: 7.898034, mean_eps: 0.817511\n",
      "  203057/2000000: episode: 1086, duration: 9.771s, episode steps: 191, steps per second: 20, episode reward: 104.800, mean reward: 0.549 [-1.000, 1.000], mean action: 3.272 [0.000, 6.000], mean observation: 172.305 [23.000, 255.000], loss: 0.032254, mean_absolute_error: 6.682454, mean_q: 7.803246, mean_eps: 0.817334\n",
      "  203263/2000000: episode: 1087, duration: 10.775s, episode steps: 206, steps per second: 19, episode reward: 125.000, mean reward: 0.607 [-1.000, 1.000], mean action: 3.233 [0.000, 6.000], mean observation: 172.711 [24.000, 255.000], loss: 0.031647, mean_absolute_error: 6.741242, mean_q: 7.888844, mean_eps: 0.817156\n",
      "  203449/2000000: episode: 1088, duration: 9.317s, episode steps: 186, steps per second: 20, episode reward: 59.100, mean reward: 0.318 [-1.000, 0.500], mean action: 2.898 [0.000, 6.000], mean observation: 172.881 [24.000, 255.000], loss: 0.031773, mean_absolute_error: 6.762100, mean_q: 7.912731, mean_eps: 0.816980\n",
      "  203640/2000000: episode: 1089, duration: 9.828s, episode steps: 191, steps per second: 19, episode reward: 122.700, mean reward: 0.642 [-1.000, 1.000], mean action: 3.115 [0.000, 6.000], mean observation: 172.079 [24.000, 255.000], loss: 0.032050, mean_absolute_error: 6.686931, mean_q: 7.810513, mean_eps: 0.816810\n",
      "  203830/2000000: episode: 1090, duration: 9.680s, episode steps: 190, steps per second: 20, episode reward: 114.800, mean reward: 0.604 [-1.000, 1.000], mean action: 3.047 [0.000, 6.000], mean observation: 172.573 [24.000, 255.000], loss: 0.032410, mean_absolute_error: 6.623339, mean_q: 7.740058, mean_eps: 0.816639\n",
      "  204050/2000000: episode: 1091, duration: 11.593s, episode steps: 220, steps per second: 19, episode reward: 105.700, mean reward: 0.480 [-1.000, 1.000], mean action: 3.068 [0.000, 6.000], mean observation: 173.440 [24.000, 255.000], loss: 0.035298, mean_absolute_error: 6.667771, mean_q: 7.791457, mean_eps: 0.816454\n",
      "  204188/2000000: episode: 1092, duration: 6.659s, episode steps: 138, steps per second: 21, episode reward: 50.700, mean reward: 0.367 [-1.000, 0.500], mean action: 2.790 [0.000, 6.000], mean observation: 172.540 [23.000, 255.000], loss: 0.030843, mean_absolute_error: 6.721787, mean_q: 7.849881, mean_eps: 0.816294\n",
      "  204346/2000000: episode: 1093, duration: 7.803s, episode steps: 158, steps per second: 20, episode reward: 66.300, mean reward: 0.420 [-1.000, 0.500], mean action: 2.880 [0.000, 6.000], mean observation: 171.497 [24.000, 255.000], loss: 0.034082, mean_absolute_error: 6.831196, mean_q: 7.977817, mean_eps: 0.816161\n",
      "  204491/2000000: episode: 1094, duration: 6.926s, episode steps: 145, steps per second: 21, episode reward: 47.800, mean reward: 0.330 [-1.000, 0.500], mean action: 2.772 [0.000, 6.000], mean observation: 172.591 [23.000, 255.000], loss: 0.031721, mean_absolute_error: 6.752656, mean_q: 7.875526, mean_eps: 0.816024\n",
      "  204709/2000000: episode: 1095, duration: 11.434s, episode steps: 218, steps per second: 19, episode reward: 168.300, mean reward: 0.772 [-1.000, 1.000], mean action: 3.147 [0.000, 6.000], mean observation: 172.338 [24.000, 255.000], loss: 0.033480, mean_absolute_error: 6.730021, mean_q: 7.868426, mean_eps: 0.815860\n",
      "  204890/2000000: episode: 1096, duration: 9.098s, episode steps: 181, steps per second: 20, episode reward: 128.500, mean reward: 0.710 [-1.000, 1.000], mean action: 2.751 [0.000, 6.000], mean observation: 171.250 [24.000, 255.000], loss: 0.034729, mean_absolute_error: 6.810193, mean_q: 7.969590, mean_eps: 0.815680\n",
      "  205080/2000000: episode: 1097, duration: 9.789s, episode steps: 190, steps per second: 19, episode reward: 142.600, mean reward: 0.751 [-1.000, 1.000], mean action: 3.253 [0.000, 6.000], mean observation: 171.717 [23.000, 255.000], loss: 0.035003, mean_absolute_error: 6.786833, mean_q: 7.933036, mean_eps: 0.815514\n",
      "  205262/2000000: episode: 1098, duration: 9.147s, episode steps: 182, steps per second: 20, episode reward: 125.700, mean reward: 0.691 [-1.000, 1.000], mean action: 3.016 [0.000, 6.000], mean observation: 170.929 [23.000, 255.000], loss: 0.032769, mean_absolute_error: 6.631438, mean_q: 7.755167, mean_eps: 0.815347\n",
      "  205458/2000000: episode: 1099, duration: 10.141s, episode steps: 196, steps per second: 19, episode reward: 127.600, mean reward: 0.651 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 171.742 [23.000, 255.000], loss: 0.033532, mean_absolute_error: 6.854241, mean_q: 8.018972, mean_eps: 0.815176\n",
      "  205663/2000000: episode: 1100, duration: 10.718s, episode steps: 205, steps per second: 19, episode reward: 146.900, mean reward: 0.717 [-1.000, 1.000], mean action: 3.073 [0.000, 6.000], mean observation: 172.412 [23.000, 255.000], loss: 0.031433, mean_absolute_error: 6.727003, mean_q: 7.860151, mean_eps: 0.814996\n",
      "  205840/2000000: episode: 1101, duration: 8.851s, episode steps: 177, steps per second: 20, episode reward: 79.100, mean reward: 0.447 [-1.000, 1.000], mean action: 3.040 [0.000, 6.000], mean observation: 171.775 [23.000, 255.000], loss: 0.031479, mean_absolute_error: 6.850461, mean_q: 8.013386, mean_eps: 0.814825\n",
      "  206045/2000000: episode: 1102, duration: 10.782s, episode steps: 205, steps per second: 19, episode reward: 152.700, mean reward: 0.745 [-1.000, 1.000], mean action: 3.073 [0.000, 6.000], mean observation: 172.857 [24.000, 255.000], loss: 0.034659, mean_absolute_error: 6.951056, mean_q: 8.135999, mean_eps: 0.814652\n",
      "  206254/2000000: episode: 1103, duration: 10.993s, episode steps: 209, steps per second: 19, episode reward: 105.600, mean reward: 0.505 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 172.583 [23.000, 255.000], loss: 0.032563, mean_absolute_error: 6.934635, mean_q: 8.111897, mean_eps: 0.814465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  206439/2000000: episode: 1104, duration: 9.401s, episode steps: 185, steps per second: 20, episode reward: 127.400, mean reward: 0.689 [-1.000, 1.000], mean action: 3.141 [0.000, 6.000], mean observation: 171.458 [23.000, 255.000], loss: 0.033419, mean_absolute_error: 6.762541, mean_q: 7.900109, mean_eps: 0.814289\n",
      "  206635/2000000: episode: 1105, duration: 10.057s, episode steps: 196, steps per second: 19, episode reward: 119.600, mean reward: 0.610 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 172.523 [23.000, 255.000], loss: 0.036882, mean_absolute_error: 6.751001, mean_q: 7.883942, mean_eps: 0.814118\n",
      "  206830/2000000: episode: 1106, duration: 9.933s, episode steps: 195, steps per second: 20, episode reward: 119.800, mean reward: 0.614 [-1.000, 1.000], mean action: 3.041 [0.000, 6.000], mean observation: 171.147 [23.000, 255.000], loss: 0.033691, mean_absolute_error: 6.654193, mean_q: 7.775126, mean_eps: 0.813941\n",
      "  207013/2000000: episode: 1107, duration: 9.092s, episode steps: 183, steps per second: 20, episode reward: 70.400, mean reward: 0.385 [-1.000, 0.500], mean action: 3.044 [0.000, 6.000], mean observation: 171.762 [22.000, 255.000], loss: 0.030884, mean_absolute_error: 6.785993, mean_q: 7.928385, mean_eps: 0.813770\n",
      "  207155/2000000: episode: 1108, duration: 7.650s, episode steps: 142, steps per second: 19, episode reward: 77.800, mean reward: 0.548 [-1.000, 1.000], mean action: 3.535 [0.000, 6.000], mean observation: 172.731 [24.000, 255.000], loss: 0.031817, mean_absolute_error: 6.749449, mean_q: 7.872934, mean_eps: 0.813624\n",
      "  207357/2000000: episode: 1109, duration: 10.410s, episode steps: 202, steps per second: 19, episode reward: 125.800, mean reward: 0.623 [-1.000, 1.000], mean action: 2.980 [0.000, 6.000], mean observation: 172.317 [23.000, 255.000], loss: 0.032709, mean_absolute_error: 6.911320, mean_q: 8.070347, mean_eps: 0.813470\n",
      "  207562/2000000: episode: 1110, duration: 10.570s, episode steps: 205, steps per second: 19, episode reward: 141.600, mean reward: 0.691 [-1.000, 1.000], mean action: 3.166 [0.000, 6.000], mean observation: 171.879 [24.000, 255.000], loss: 0.034857, mean_absolute_error: 6.812766, mean_q: 7.956101, mean_eps: 0.813286\n",
      "  207769/2000000: episode: 1111, duration: 10.781s, episode steps: 207, steps per second: 19, episode reward: 127.900, mean reward: 0.618 [-1.000, 1.000], mean action: 3.261 [0.000, 6.000], mean observation: 171.805 [23.000, 255.000], loss: 0.031279, mean_absolute_error: 6.661345, mean_q: 7.783148, mean_eps: 0.813101\n",
      "  207950/2000000: episode: 1112, duration: 8.864s, episode steps: 181, steps per second: 20, episode reward: 69.800, mean reward: 0.386 [-1.000, 0.500], mean action: 3.028 [0.000, 6.000], mean observation: 171.868 [24.000, 255.000], loss: 0.034470, mean_absolute_error: 6.847269, mean_q: 8.001702, mean_eps: 0.812926\n",
      "  208091/2000000: episode: 1113, duration: 6.855s, episode steps: 141, steps per second: 21, episode reward: 56.200, mean reward: 0.399 [-1.000, 0.500], mean action: 2.773 [0.000, 6.000], mean observation: 171.884 [23.000, 255.000], loss: 0.030588, mean_absolute_error: 6.608649, mean_q: 7.721429, mean_eps: 0.812782\n",
      "  208283/2000000: episode: 1114, duration: 9.652s, episode steps: 192, steps per second: 20, episode reward: 116.800, mean reward: 0.608 [-1.000, 1.000], mean action: 2.797 [0.000, 6.000], mean observation: 172.083 [23.000, 255.000], loss: 0.032519, mean_absolute_error: 6.660306, mean_q: 7.787693, mean_eps: 0.812633\n",
      "  208460/2000000: episode: 1115, duration: 8.774s, episode steps: 177, steps per second: 20, episode reward: 115.500, mean reward: 0.653 [-1.000, 1.000], mean action: 2.938 [0.000, 6.000], mean observation: 170.611 [23.000, 255.000], loss: 0.029505, mean_absolute_error: 6.770248, mean_q: 7.903480, mean_eps: 0.812467\n",
      "  208662/2000000: episode: 1116, duration: 10.411s, episode steps: 202, steps per second: 19, episode reward: 107.700, mean reward: 0.533 [-1.000, 1.000], mean action: 3.045 [0.000, 6.000], mean observation: 172.488 [23.000, 255.000], loss: 0.033325, mean_absolute_error: 6.690580, mean_q: 7.816055, mean_eps: 0.812296\n",
      "  208854/2000000: episode: 1117, duration: 9.826s, episode steps: 192, steps per second: 20, episode reward: 117.300, mean reward: 0.611 [-1.000, 1.000], mean action: 3.047 [0.000, 6.000], mean observation: 171.478 [24.000, 255.000], loss: 0.031999, mean_absolute_error: 6.793047, mean_q: 7.937863, mean_eps: 0.812118\n",
      "  209040/2000000: episode: 1118, duration: 9.495s, episode steps: 186, steps per second: 20, episode reward: 85.200, mean reward: 0.458 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 171.514 [23.000, 255.000], loss: 0.033579, mean_absolute_error: 6.759070, mean_q: 7.897801, mean_eps: 0.811949\n",
      "  209210/2000000: episode: 1119, duration: 8.309s, episode steps: 170, steps per second: 20, episode reward: 66.700, mean reward: 0.392 [-1.000, 0.500], mean action: 2.876 [0.000, 6.000], mean observation: 171.802 [24.000, 255.000], loss: 0.034568, mean_absolute_error: 6.767461, mean_q: 7.909403, mean_eps: 0.811788\n",
      "  209390/2000000: episode: 1120, duration: 8.815s, episode steps: 180, steps per second: 20, episode reward: 69.300, mean reward: 0.385 [-1.000, 0.500], mean action: 3.000 [0.000, 6.000], mean observation: 171.853 [23.000, 255.000], loss: 0.032460, mean_absolute_error: 6.818080, mean_q: 7.971057, mean_eps: 0.811630\n",
      "  209594/2000000: episode: 1121, duration: 10.497s, episode steps: 204, steps per second: 19, episode reward: 117.400, mean reward: 0.575 [-1.000, 1.000], mean action: 2.985 [0.000, 6.000], mean observation: 172.440 [23.000, 255.000], loss: 0.030078, mean_absolute_error: 6.654818, mean_q: 7.784200, mean_eps: 0.811457\n",
      "  209777/2000000: episode: 1122, duration: 9.200s, episode steps: 183, steps per second: 20, episode reward: 82.000, mean reward: 0.448 [-1.000, 1.000], mean action: 3.055 [0.000, 6.000], mean observation: 171.377 [23.000, 255.000], loss: 0.029480, mean_absolute_error: 6.748970, mean_q: 7.899011, mean_eps: 0.811283\n",
      "  209967/2000000: episode: 1123, duration: 9.655s, episode steps: 190, steps per second: 20, episode reward: 122.500, mean reward: 0.645 [-1.000, 1.000], mean action: 2.921 [0.000, 6.000], mean observation: 171.458 [24.000, 255.000], loss: 0.032348, mean_absolute_error: 6.624590, mean_q: 7.748474, mean_eps: 0.811115\n",
      "  210175/2000000: episode: 1124, duration: 10.829s, episode steps: 208, steps per second: 19, episode reward: 151.500, mean reward: 0.728 [-1.000, 1.000], mean action: 3.005 [0.000, 6.000], mean observation: 171.851 [23.000, 255.000], loss: 0.055973, mean_absolute_error: 6.919174, mean_q: 8.088085, mean_eps: 0.810937\n",
      "  210321/2000000: episode: 1125, duration: 7.067s, episode steps: 146, steps per second: 21, episode reward: 49.500, mean reward: 0.339 [-1.000, 0.500], mean action: 2.884 [0.000, 6.000], mean observation: 171.982 [22.000, 255.000], loss: 0.039859, mean_absolute_error: 7.063137, mean_q: 8.254982, mean_eps: 0.810777\n",
      "  210532/2000000: episode: 1126, duration: 11.090s, episode steps: 211, steps per second: 19, episode reward: 136.200, mean reward: 0.645 [-1.000, 1.000], mean action: 3.047 [0.000, 6.000], mean observation: 172.070 [24.000, 255.000], loss: 0.045196, mean_absolute_error: 7.156920, mean_q: 8.368527, mean_eps: 0.810617\n",
      "  210752/2000000: episode: 1127, duration: 11.496s, episode steps: 220, steps per second: 19, episode reward: 142.000, mean reward: 0.645 [-1.000, 1.000], mean action: 3.164 [0.000, 6.000], mean observation: 172.288 [24.000, 255.000], loss: 0.041357, mean_absolute_error: 7.157093, mean_q: 8.360653, mean_eps: 0.810424\n",
      "  210962/2000000: episode: 1128, duration: 10.706s, episode steps: 210, steps per second: 20, episode reward: 93.500, mean reward: 0.445 [-1.000, 0.500], mean action: 3.052 [0.000, 6.000], mean observation: 172.670 [22.000, 255.000], loss: 0.038424, mean_absolute_error: 7.095728, mean_q: 8.302988, mean_eps: 0.810230\n",
      "  211105/2000000: episode: 1129, duration: 6.829s, episode steps: 143, steps per second: 21, episode reward: 49.600, mean reward: 0.347 [-1.000, 0.500], mean action: 2.790 [0.000, 6.000], mean observation: 172.468 [24.000, 255.000], loss: 0.041589, mean_absolute_error: 7.002188, mean_q: 8.193024, mean_eps: 0.810069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  211248/2000000: episode: 1130, duration: 6.854s, episode steps: 143, steps per second: 21, episode reward: 53.200, mean reward: 0.372 [-1.000, 0.500], mean action: 2.895 [0.000, 6.000], mean observation: 172.414 [24.000, 255.000], loss: 0.032632, mean_absolute_error: 6.844618, mean_q: 7.989669, mean_eps: 0.809942\n",
      "  211452/2000000: episode: 1131, duration: 10.534s, episode steps: 204, steps per second: 19, episode reward: 127.000, mean reward: 0.623 [-1.000, 1.000], mean action: 3.020 [0.000, 6.000], mean observation: 172.890 [23.000, 255.000], loss: 0.041472, mean_absolute_error: 7.052898, mean_q: 8.235692, mean_eps: 0.809787\n",
      "  211612/2000000: episode: 1132, duration: 7.885s, episode steps: 160, steps per second: 20, episode reward: 65.300, mean reward: 0.408 [-1.000, 0.500], mean action: 2.769 [0.000, 6.000], mean observation: 172.104 [25.000, 255.000], loss: 0.034801, mean_absolute_error: 7.272625, mean_q: 8.500445, mean_eps: 0.809623\n",
      "  211814/2000000: episode: 1133, duration: 10.552s, episode steps: 202, steps per second: 19, episode reward: 122.300, mean reward: 0.605 [-1.000, 1.000], mean action: 3.084 [0.000, 6.000], mean observation: 172.243 [23.000, 255.000], loss: 0.038426, mean_absolute_error: 6.988561, mean_q: 8.153041, mean_eps: 0.809459\n",
      "  211932/2000000: episode: 1134, duration: 5.552s, episode steps: 118, steps per second: 21, episode reward: 40.700, mean reward: 0.345 [-1.000, 0.500], mean action: 2.644 [0.000, 6.000], mean observation: 172.723 [23.000, 255.000], loss: 0.033824, mean_absolute_error: 6.881414, mean_q: 8.050735, mean_eps: 0.809315\n",
      "  212154/2000000: episode: 1135, duration: 11.674s, episode steps: 222, steps per second: 19, episode reward: 96.500, mean reward: 0.435 [-1.000, 1.000], mean action: 3.149 [0.000, 6.000], mean observation: 173.740 [23.000, 255.000], loss: 0.035258, mean_absolute_error: 7.017175, mean_q: 8.212133, mean_eps: 0.809162\n",
      "  212282/2000000: episode: 1136, duration: 6.946s, episode steps: 128, steps per second: 18, episode reward: 75.600, mean reward: 0.591 [-1.000, 1.000], mean action: 3.547 [0.000, 6.000], mean observation: 172.967 [24.000, 255.000], loss: 0.042066, mean_absolute_error: 7.037562, mean_q: 8.212385, mean_eps: 0.809004\n",
      "  212442/2000000: episode: 1137, duration: 7.818s, episode steps: 160, steps per second: 20, episode reward: 64.900, mean reward: 0.406 [-1.000, 0.500], mean action: 2.944 [0.000, 6.000], mean observation: 172.064 [24.000, 255.000], loss: 0.045088, mean_absolute_error: 7.065283, mean_q: 8.265619, mean_eps: 0.808874\n",
      "  212603/2000000: episode: 1138, duration: 7.911s, episode steps: 161, steps per second: 20, episode reward: 67.000, mean reward: 0.416 [-1.000, 0.500], mean action: 3.012 [0.000, 6.000], mean observation: 172.020 [24.000, 255.000], loss: 0.031123, mean_absolute_error: 7.204344, mean_q: 8.419914, mean_eps: 0.808730\n",
      "  212793/2000000: episode: 1139, duration: 9.822s, episode steps: 190, steps per second: 19, episode reward: 139.000, mean reward: 0.732 [-1.000, 1.000], mean action: 3.084 [0.000, 6.000], mean observation: 171.737 [24.000, 255.000], loss: 0.036815, mean_absolute_error: 7.056571, mean_q: 8.245597, mean_eps: 0.808572\n",
      "  212990/2000000: episode: 1140, duration: 10.162s, episode steps: 197, steps per second: 19, episode reward: 126.500, mean reward: 0.642 [-1.000, 1.000], mean action: 2.964 [0.000, 6.000], mean observation: 172.987 [23.000, 255.000], loss: 0.044264, mean_absolute_error: 7.110602, mean_q: 8.324774, mean_eps: 0.808397\n",
      "  213167/2000000: episode: 1141, duration: 8.695s, episode steps: 177, steps per second: 20, episode reward: 71.800, mean reward: 0.406 [-1.000, 0.500], mean action: 2.949 [0.000, 6.000], mean observation: 172.660 [23.000, 255.000], loss: 0.040451, mean_absolute_error: 7.005632, mean_q: 8.189569, mean_eps: 0.808230\n",
      "  213370/2000000: episode: 1142, duration: 10.525s, episode steps: 203, steps per second: 19, episode reward: 152.200, mean reward: 0.750 [-1.000, 1.000], mean action: 3.187 [0.000, 6.000], mean observation: 173.050 [23.000, 255.000], loss: 0.035008, mean_absolute_error: 6.895477, mean_q: 8.058360, mean_eps: 0.808059\n",
      "  213562/2000000: episode: 1143, duration: 9.682s, episode steps: 192, steps per second: 20, episode reward: 63.300, mean reward: 0.330 [-1.000, 0.500], mean action: 3.099 [0.000, 6.000], mean observation: 172.963 [23.000, 255.000], loss: 0.036777, mean_absolute_error: 6.979279, mean_q: 8.158069, mean_eps: 0.807881\n",
      "  213766/2000000: episode: 1144, duration: 10.427s, episode steps: 204, steps per second: 20, episode reward: 124.300, mean reward: 0.609 [-1.000, 1.000], mean action: 2.931 [0.000, 6.000], mean observation: 174.046 [23.000, 255.000], loss: 0.038534, mean_absolute_error: 7.040005, mean_q: 8.233930, mean_eps: 0.807702\n",
      "  213949/2000000: episode: 1145, duration: 9.228s, episode steps: 183, steps per second: 20, episode reward: 76.800, mean reward: 0.420 [-1.000, 0.500], mean action: 2.847 [0.000, 6.000], mean observation: 172.442 [23.000, 255.000], loss: 0.038478, mean_absolute_error: 7.000290, mean_q: 8.172535, mean_eps: 0.807528\n",
      "  214138/2000000: episode: 1146, duration: 9.573s, episode steps: 189, steps per second: 20, episode reward: 86.600, mean reward: 0.458 [-1.000, 0.500], mean action: 3.021 [0.000, 6.000], mean observation: 172.966 [23.000, 255.000], loss: 0.036625, mean_absolute_error: 7.062467, mean_q: 8.253293, mean_eps: 0.807360\n",
      "  214330/2000000: episode: 1147, duration: 9.925s, episode steps: 192, steps per second: 19, episode reward: 125.700, mean reward: 0.655 [-1.000, 1.000], mean action: 3.026 [0.000, 6.000], mean observation: 172.419 [24.000, 255.000], loss: 0.034614, mean_absolute_error: 7.093910, mean_q: 8.276939, mean_eps: 0.807189\n",
      "  214510/2000000: episode: 1148, duration: 8.711s, episode steps: 180, steps per second: 21, episode reward: 59.700, mean reward: 0.332 [-1.000, 0.500], mean action: 3.106 [0.000, 6.000], mean observation: 173.244 [23.000, 255.000], loss: 0.038166, mean_absolute_error: 7.194206, mean_q: 8.396761, mean_eps: 0.807022\n",
      "  214732/2000000: episode: 1149, duration: 11.702s, episode steps: 222, steps per second: 19, episode reward: 136.500, mean reward: 0.615 [-1.000, 1.000], mean action: 3.176 [0.000, 6.000], mean observation: 173.219 [24.000, 255.000], loss: 0.031146, mean_absolute_error: 6.976082, mean_q: 8.152550, mean_eps: 0.806842\n",
      "  214908/2000000: episode: 1150, duration: 8.708s, episode steps: 176, steps per second: 20, episode reward: 62.100, mean reward: 0.353 [-1.000, 0.500], mean action: 3.210 [0.000, 6.000], mean observation: 173.308 [25.000, 255.000], loss: 0.036599, mean_absolute_error: 7.076519, mean_q: 8.274632, mean_eps: 0.806664\n",
      "  215125/2000000: episode: 1151, duration: 11.489s, episode steps: 217, steps per second: 19, episode reward: 95.400, mean reward: 0.440 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 174.385 [23.000, 255.000], loss: 0.037732, mean_absolute_error: 6.866795, mean_q: 8.009908, mean_eps: 0.806486\n",
      "  215314/2000000: episode: 1152, duration: 9.705s, episode steps: 189, steps per second: 19, episode reward: 108.800, mean reward: 0.576 [-1.000, 1.000], mean action: 3.127 [0.000, 6.000], mean observation: 172.493 [24.000, 255.000], loss: 0.035037, mean_absolute_error: 7.056392, mean_q: 8.252271, mean_eps: 0.806302\n",
      "  215489/2000000: episode: 1153, duration: 8.583s, episode steps: 175, steps per second: 20, episode reward: 76.600, mean reward: 0.438 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 172.500 [23.000, 255.000], loss: 0.033289, mean_absolute_error: 6.835888, mean_q: 7.970475, mean_eps: 0.806138\n",
      "  215702/2000000: episode: 1154, duration: 11.096s, episode steps: 213, steps per second: 19, episode reward: 136.800, mean reward: 0.642 [-1.000, 1.000], mean action: 3.207 [0.000, 6.000], mean observation: 172.850 [23.000, 255.000], loss: 0.033238, mean_absolute_error: 6.952261, mean_q: 8.110851, mean_eps: 0.805964\n",
      "  215918/2000000: episode: 1155, duration: 11.251s, episode steps: 216, steps per second: 19, episode reward: 146.800, mean reward: 0.680 [-1.000, 1.000], mean action: 3.194 [0.000, 6.000], mean observation: 172.443 [24.000, 255.000], loss: 0.034989, mean_absolute_error: 6.922697, mean_q: 8.087260, mean_eps: 0.805771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  216121/2000000: episode: 1156, duration: 10.503s, episode steps: 203, steps per second: 19, episode reward: 138.700, mean reward: 0.683 [-1.000, 1.000], mean action: 3.015 [0.000, 6.000], mean observation: 173.144 [24.000, 255.000], loss: 0.035750, mean_absolute_error: 6.836564, mean_q: 7.989757, mean_eps: 0.805582\n",
      "  216342/2000000: episode: 1157, duration: 11.632s, episode steps: 221, steps per second: 19, episode reward: 109.500, mean reward: 0.495 [-1.000, 1.000], mean action: 3.217 [0.000, 6.000], mean observation: 173.221 [24.000, 255.000], loss: 0.032175, mean_absolute_error: 6.874139, mean_q: 8.030782, mean_eps: 0.805391\n",
      "  216535/2000000: episode: 1158, duration: 9.887s, episode steps: 193, steps per second: 20, episode reward: 122.700, mean reward: 0.636 [-1.000, 1.000], mean action: 3.052 [0.000, 6.000], mean observation: 172.513 [24.000, 255.000], loss: 0.035588, mean_absolute_error: 6.990523, mean_q: 8.167908, mean_eps: 0.805206\n",
      "  216726/2000000: episode: 1159, duration: 9.851s, episode steps: 191, steps per second: 19, episode reward: 130.300, mean reward: 0.682 [-1.000, 1.000], mean action: 3.188 [0.000, 6.000], mean observation: 172.152 [24.000, 255.000], loss: 0.037918, mean_absolute_error: 6.923626, mean_q: 8.084144, mean_eps: 0.805033\n",
      "  216920/2000000: episode: 1160, duration: 9.886s, episode steps: 194, steps per second: 20, episode reward: 124.500, mean reward: 0.642 [-1.000, 1.000], mean action: 2.881 [0.000, 6.000], mean observation: 172.613 [24.000, 255.000], loss: 0.041023, mean_absolute_error: 7.042726, mean_q: 8.248510, mean_eps: 0.804860\n",
      "  217128/2000000: episode: 1161, duration: 10.298s, episode steps: 208, steps per second: 20, episode reward: 54.100, mean reward: 0.260 [-1.000, 0.500], mean action: 2.976 [0.000, 6.000], mean observation: 173.985 [24.000, 255.000], loss: 0.034899, mean_absolute_error: 7.057971, mean_q: 8.228290, mean_eps: 0.804680\n",
      "  217315/2000000: episode: 1162, duration: 9.619s, episode steps: 187, steps per second: 19, episode reward: 84.800, mean reward: 0.453 [-1.000, 0.500], mean action: 3.139 [0.000, 6.000], mean observation: 172.912 [25.000, 255.000], loss: 0.036268, mean_absolute_error: 6.982572, mean_q: 8.181402, mean_eps: 0.804502\n",
      "  217499/2000000: episode: 1163, duration: 9.343s, episode steps: 184, steps per second: 20, episode reward: 131.800, mean reward: 0.716 [-1.000, 1.000], mean action: 2.880 [0.000, 6.000], mean observation: 171.530 [23.000, 255.000], loss: 0.040532, mean_absolute_error: 7.140485, mean_q: 8.367038, mean_eps: 0.804335\n",
      "  217691/2000000: episode: 1164, duration: 9.880s, episode steps: 192, steps per second: 19, episode reward: 138.200, mean reward: 0.720 [-1.000, 1.000], mean action: 3.104 [0.000, 6.000], mean observation: 171.998 [24.000, 255.000], loss: 0.041859, mean_absolute_error: 7.053542, mean_q: 8.240092, mean_eps: 0.804165\n",
      "  217896/2000000: episode: 1165, duration: 10.599s, episode steps: 205, steps per second: 19, episode reward: 116.100, mean reward: 0.566 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 172.845 [24.000, 255.000], loss: 0.041918, mean_absolute_error: 6.965136, mean_q: 8.143954, mean_eps: 0.803987\n",
      "  218105/2000000: episode: 1166, duration: 10.961s, episode steps: 209, steps per second: 19, episode reward: 139.000, mean reward: 0.665 [-1.000, 1.000], mean action: 2.938 [0.000, 6.000], mean observation: 173.079 [23.000, 255.000], loss: 0.033605, mean_absolute_error: 7.211290, mean_q: 8.420818, mean_eps: 0.803800\n",
      "  218263/2000000: episode: 1167, duration: 7.709s, episode steps: 158, steps per second: 20, episode reward: 83.600, mean reward: 0.529 [-1.000, 1.000], mean action: 2.975 [0.000, 6.000], mean observation: 171.484 [23.000, 255.000], loss: 0.036851, mean_absolute_error: 6.911227, mean_q: 8.083030, mean_eps: 0.803634\n",
      "  218460/2000000: episode: 1168, duration: 10.188s, episode steps: 197, steps per second: 19, episode reward: 135.700, mean reward: 0.689 [-1.000, 1.000], mean action: 2.975 [0.000, 6.000], mean observation: 172.599 [22.000, 255.000], loss: 0.037388, mean_absolute_error: 7.195991, mean_q: 8.410111, mean_eps: 0.803476\n",
      "  218640/2000000: episode: 1169, duration: 8.975s, episode steps: 180, steps per second: 20, episode reward: 122.500, mean reward: 0.681 [-1.000, 1.000], mean action: 3.039 [0.000, 6.000], mean observation: 171.428 [24.000, 255.000], loss: 0.033910, mean_absolute_error: 7.178513, mean_q: 8.390498, mean_eps: 0.803307\n",
      "  218826/2000000: episode: 1170, duration: 9.504s, episode steps: 186, steps per second: 20, episode reward: 105.200, mean reward: 0.566 [-1.000, 1.000], mean action: 3.247 [0.000, 6.000], mean observation: 171.944 [23.000, 255.000], loss: 0.033447, mean_absolute_error: 7.173764, mean_q: 8.390875, mean_eps: 0.803141\n",
      "  219047/2000000: episode: 1171, duration: 11.617s, episode steps: 221, steps per second: 19, episode reward: 99.900, mean reward: 0.452 [-1.000, 1.000], mean action: 2.986 [0.000, 6.000], mean observation: 173.508 [22.000, 255.000], loss: 0.030235, mean_absolute_error: 6.966537, mean_q: 8.136762, mean_eps: 0.802958\n",
      "  219200/2000000: episode: 1172, duration: 7.294s, episode steps: 153, steps per second: 21, episode reward: 48.200, mean reward: 0.315 [-1.000, 0.500], mean action: 2.752 [0.000, 6.000], mean observation: 172.533 [23.000, 255.000], loss: 0.040262, mean_absolute_error: 7.092187, mean_q: 8.292019, mean_eps: 0.802790\n",
      "  219354/2000000: episode: 1173, duration: 7.445s, episode steps: 154, steps per second: 21, episode reward: 63.500, mean reward: 0.412 [-1.000, 0.500], mean action: 2.740 [0.000, 6.000], mean observation: 171.862 [24.000, 255.000], loss: 0.036461, mean_absolute_error: 7.079184, mean_q: 8.286712, mean_eps: 0.802652\n",
      "  219558/2000000: episode: 1174, duration: 10.515s, episode steps: 204, steps per second: 19, episode reward: 110.700, mean reward: 0.543 [-1.000, 1.000], mean action: 3.093 [0.000, 6.000], mean observation: 172.889 [23.000, 255.000], loss: 0.039767, mean_absolute_error: 7.035113, mean_q: 8.234884, mean_eps: 0.802490\n",
      "  219718/2000000: episode: 1175, duration: 7.828s, episode steps: 160, steps per second: 20, episode reward: 61.300, mean reward: 0.383 [-1.000, 0.500], mean action: 2.919 [0.000, 6.000], mean observation: 172.187 [24.000, 255.000], loss: 0.037325, mean_absolute_error: 7.134343, mean_q: 8.348277, mean_eps: 0.802326\n",
      "  219902/2000000: episode: 1176, duration: 9.272s, episode steps: 184, steps per second: 20, episode reward: 132.900, mean reward: 0.722 [-1.000, 1.000], mean action: 2.978 [0.000, 6.000], mean observation: 171.078 [24.000, 255.000], loss: 0.036872, mean_absolute_error: 7.058829, mean_q: 8.231692, mean_eps: 0.802171\n",
      "  220038/2000000: episode: 1177, duration: 6.437s, episode steps: 136, steps per second: 21, episode reward: 48.100, mean reward: 0.354 [-1.000, 0.500], mean action: 2.559 [0.000, 6.000], mean observation: 172.611 [23.000, 255.000], loss: 0.048454, mean_absolute_error: 7.129938, mean_q: 8.322572, mean_eps: 0.802027\n",
      "  220226/2000000: episode: 1178, duration: 9.320s, episode steps: 188, steps per second: 20, episode reward: 115.900, mean reward: 0.616 [-1.000, 1.000], mean action: 2.957 [0.000, 6.000], mean observation: 171.402 [23.000, 255.000], loss: 0.043120, mean_absolute_error: 7.295624, mean_q: 8.521238, mean_eps: 0.801881\n",
      "  220362/2000000: episode: 1179, duration: 6.372s, episode steps: 136, steps per second: 21, episode reward: 50.500, mean reward: 0.371 [-1.000, 0.500], mean action: 2.838 [0.000, 6.000], mean observation: 172.341 [24.000, 255.000], loss: 0.045351, mean_absolute_error: 7.569755, mean_q: 8.822976, mean_eps: 0.801735\n",
      "  220555/2000000: episode: 1180, duration: 9.625s, episode steps: 193, steps per second: 20, episode reward: 148.800, mean reward: 0.771 [-1.000, 1.000], mean action: 3.176 [0.000, 6.000], mean observation: 171.564 [24.000, 255.000], loss: 0.038441, mean_absolute_error: 7.389389, mean_q: 8.621116, mean_eps: 0.801588\n",
      "  220798/2000000: episode: 1181, duration: 12.202s, episode steps: 243, steps per second: 20, episode reward: 66.000, mean reward: 0.272 [-1.000, 0.500], mean action: 3.362 [0.000, 6.000], mean observation: 173.249 [23.000, 255.000], loss: 0.037715, mean_absolute_error: 7.402719, mean_q: 8.642788, mean_eps: 0.801392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  220946/2000000: episode: 1182, duration: 7.009s, episode steps: 148, steps per second: 21, episode reward: 59.300, mean reward: 0.401 [-1.000, 0.500], mean action: 2.791 [0.000, 6.000], mean observation: 171.600 [24.000, 255.000], loss: 0.042012, mean_absolute_error: 7.550496, mean_q: 8.814235, mean_eps: 0.801215\n",
      "  221116/2000000: episode: 1183, duration: 8.191s, episode steps: 170, steps per second: 21, episode reward: 84.100, mean reward: 0.495 [-1.000, 1.000], mean action: 2.918 [0.000, 6.000], mean observation: 171.009 [23.000, 255.000], loss: 0.040642, mean_absolute_error: 7.437483, mean_q: 8.668906, mean_eps: 0.801073\n",
      "  221331/2000000: episode: 1184, duration: 10.915s, episode steps: 215, steps per second: 20, episode reward: 114.400, mean reward: 0.532 [-1.000, 1.000], mean action: 2.967 [0.000, 6.000], mean observation: 172.704 [24.000, 255.000], loss: 0.045458, mean_absolute_error: 7.420510, mean_q: 8.662327, mean_eps: 0.800900\n",
      "  221540/2000000: episode: 1185, duration: 10.731s, episode steps: 209, steps per second: 19, episode reward: 154.200, mean reward: 0.738 [-1.000, 1.000], mean action: 3.206 [0.000, 6.000], mean observation: 172.277 [24.000, 255.000], loss: 0.041179, mean_absolute_error: 7.297622, mean_q: 8.514279, mean_eps: 0.800709\n",
      "  221757/2000000: episode: 1186, duration: 11.105s, episode steps: 217, steps per second: 20, episode reward: 131.900, mean reward: 0.608 [-1.000, 1.000], mean action: 3.120 [0.000, 6.000], mean observation: 172.738 [23.000, 255.000], loss: 0.040751, mean_absolute_error: 7.514650, mean_q: 8.773462, mean_eps: 0.800517\n",
      "  221954/2000000: episode: 1187, duration: 10.020s, episode steps: 197, steps per second: 20, episode reward: 124.400, mean reward: 0.631 [-1.000, 1.000], mean action: 3.086 [0.000, 6.000], mean observation: 172.139 [24.000, 255.000], loss: 0.041845, mean_absolute_error: 7.319422, mean_q: 8.552038, mean_eps: 0.800330\n",
      "  222077/2000000: episode: 1188, duration: 5.663s, episode steps: 123, steps per second: 22, episode reward: 42.400, mean reward: 0.345 [-1.000, 0.500], mean action: 2.488 [0.000, 6.000], mean observation: 173.044 [23.000, 255.000], loss: 0.043675, mean_absolute_error: 7.178607, mean_q: 8.382734, mean_eps: 0.800186\n",
      "  222263/2000000: episode: 1189, duration: 9.133s, episode steps: 186, steps per second: 20, episode reward: 102.100, mean reward: 0.549 [-1.000, 1.000], mean action: 2.968 [0.000, 6.000], mean observation: 172.069 [23.000, 255.000], loss: 0.041640, mean_absolute_error: 7.434716, mean_q: 8.675508, mean_eps: 0.800047\n",
      "  222370/2000000: episode: 1190, duration: 4.935s, episode steps: 107, steps per second: 22, episode reward: 39.600, mean reward: 0.370 [-1.000, 0.500], mean action: 2.224 [0.000, 6.000], mean observation: 172.546 [24.000, 255.000], loss: 0.035802, mean_absolute_error: 7.245304, mean_q: 8.467459, mean_eps: 0.799916\n",
      "  222552/2000000: episode: 1191, duration: 8.992s, episode steps: 182, steps per second: 20, episode reward: 121.000, mean reward: 0.665 [-1.000, 1.000], mean action: 3.060 [0.000, 6.000], mean observation: 171.975 [23.000, 255.000], loss: 0.039492, mean_absolute_error: 7.427771, mean_q: 8.680662, mean_eps: 0.799786\n",
      "  222739/2000000: episode: 1192, duration: 9.298s, episode steps: 187, steps per second: 20, episode reward: 137.300, mean reward: 0.734 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 171.345 [24.000, 255.000], loss: 0.040119, mean_absolute_error: 7.460672, mean_q: 8.701937, mean_eps: 0.799620\n",
      "  222922/2000000: episode: 1193, duration: 8.954s, episode steps: 183, steps per second: 20, episode reward: 117.600, mean reward: 0.643 [-1.000, 1.000], mean action: 3.175 [0.000, 6.000], mean observation: 171.073 [24.000, 255.000], loss: 0.038285, mean_absolute_error: 7.325336, mean_q: 8.546232, mean_eps: 0.799453\n",
      "  223100/2000000: episode: 1194, duration: 8.685s, episode steps: 178, steps per second: 20, episode reward: 109.800, mean reward: 0.617 [-1.000, 1.000], mean action: 3.034 [0.000, 6.000], mean observation: 171.798 [24.000, 255.000], loss: 0.040778, mean_absolute_error: 7.430124, mean_q: 8.671357, mean_eps: 0.799291\n",
      "  223272/2000000: episode: 1195, duration: 8.263s, episode steps: 172, steps per second: 21, episode reward: 70.900, mean reward: 0.412 [-1.000, 0.500], mean action: 2.738 [0.000, 6.000], mean observation: 172.105 [22.000, 255.000], loss: 0.046065, mean_absolute_error: 7.560813, mean_q: 8.861907, mean_eps: 0.799134\n",
      "  223417/2000000: episode: 1196, duration: 6.722s, episode steps: 145, steps per second: 22, episode reward: 49.000, mean reward: 0.338 [-1.000, 0.500], mean action: 2.841 [0.000, 6.000], mean observation: 173.269 [23.000, 255.000], loss: 0.039541, mean_absolute_error: 7.437033, mean_q: 8.673755, mean_eps: 0.798990\n",
      "  223594/2000000: episode: 1197, duration: 8.589s, episode steps: 177, steps per second: 21, episode reward: 109.800, mean reward: 0.620 [-1.000, 1.000], mean action: 2.791 [0.000, 6.000], mean observation: 171.512 [24.000, 255.000], loss: 0.037049, mean_absolute_error: 7.220003, mean_q: 8.409707, mean_eps: 0.798845\n",
      "  223787/2000000: episode: 1198, duration: 9.444s, episode steps: 193, steps per second: 20, episode reward: 69.800, mean reward: 0.362 [-1.000, 0.500], mean action: 3.212 [0.000, 6.000], mean observation: 172.957 [24.000, 255.000], loss: 0.040512, mean_absolute_error: 7.328613, mean_q: 8.551020, mean_eps: 0.798679\n",
      "  223930/2000000: episode: 1199, duration: 6.762s, episode steps: 143, steps per second: 21, episode reward: 54.400, mean reward: 0.380 [-1.000, 0.500], mean action: 2.448 [0.000, 6.000], mean observation: 173.081 [24.000, 255.000], loss: 0.038512, mean_absolute_error: 7.410645, mean_q: 8.660106, mean_eps: 0.798528\n",
      "  224066/2000000: episode: 1200, duration: 6.349s, episode steps: 136, steps per second: 21, episode reward: 50.100, mean reward: 0.368 [-1.000, 0.500], mean action: 2.772 [0.000, 6.000], mean observation: 173.169 [23.000, 255.000], loss: 0.044167, mean_absolute_error: 7.347505, mean_q: 8.568283, mean_eps: 0.798402\n",
      "  224248/2000000: episode: 1201, duration: 8.949s, episode steps: 182, steps per second: 20, episode reward: 129.900, mean reward: 0.714 [-1.000, 1.000], mean action: 2.885 [0.000, 6.000], mean observation: 171.681 [23.000, 255.000], loss: 0.037967, mean_absolute_error: 7.444561, mean_q: 8.697936, mean_eps: 0.798260\n",
      "  224389/2000000: episode: 1202, duration: 7.488s, episode steps: 141, steps per second: 19, episode reward: 76.400, mean reward: 0.542 [-1.000, 1.000], mean action: 3.461 [0.000, 6.000], mean observation: 173.259 [21.000, 255.000], loss: 0.040040, mean_absolute_error: 7.313787, mean_q: 8.550306, mean_eps: 0.798114\n",
      "  224538/2000000: episode: 1203, duration: 7.014s, episode steps: 149, steps per second: 21, episode reward: 53.000, mean reward: 0.356 [-1.000, 0.500], mean action: 2.644 [0.000, 6.000], mean observation: 172.893 [24.000, 255.000], loss: 0.038707, mean_absolute_error: 7.576724, mean_q: 8.842498, mean_eps: 0.797982\n",
      "  224736/2000000: episode: 1204, duration: 10.026s, episode steps: 198, steps per second: 20, episode reward: 93.100, mean reward: 0.470 [-1.000, 1.000], mean action: 3.076 [0.000, 6.000], mean observation: 172.936 [23.000, 255.000], loss: 0.042476, mean_absolute_error: 7.406858, mean_q: 8.647630, mean_eps: 0.797828\n",
      "  224860/2000000: episode: 1205, duration: 5.738s, episode steps: 124, steps per second: 22, episode reward: 42.500, mean reward: 0.343 [-1.000, 0.500], mean action: 2.782 [0.000, 6.000], mean observation: 173.223 [24.000, 255.000], loss: 0.043304, mean_absolute_error: 7.354169, mean_q: 8.590345, mean_eps: 0.797684\n",
      "  225022/2000000: episode: 1206, duration: 7.828s, episode steps: 162, steps per second: 21, episode reward: 67.100, mean reward: 0.414 [-1.000, 0.500], mean action: 2.698 [0.000, 6.000], mean observation: 172.272 [25.000, 255.000], loss: 0.042421, mean_absolute_error: 7.392388, mean_q: 8.624311, mean_eps: 0.797554\n",
      "  225228/2000000: episode: 1207, duration: 10.377s, episode steps: 206, steps per second: 20, episode reward: 138.000, mean reward: 0.670 [-1.000, 1.000], mean action: 3.029 [0.000, 6.000], mean observation: 173.249 [23.000, 255.000], loss: 0.041372, mean_absolute_error: 7.435268, mean_q: 8.670993, mean_eps: 0.797388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  225380/2000000: episode: 1208, duration: 8.049s, episode steps: 152, steps per second: 19, episode reward: 80.100, mean reward: 0.527 [-1.000, 1.000], mean action: 3.362 [0.000, 6.000], mean observation: 174.177 [23.000, 255.000], loss: 0.040430, mean_absolute_error: 7.504049, mean_q: 8.755747, mean_eps: 0.797228\n",
      "  225539/2000000: episode: 1209, duration: 7.437s, episode steps: 159, steps per second: 21, episode reward: 48.000, mean reward: 0.302 [-1.000, 0.500], mean action: 3.019 [0.000, 6.000], mean observation: 173.102 [24.000, 255.000], loss: 0.039494, mean_absolute_error: 7.223494, mean_q: 8.419102, mean_eps: 0.797088\n",
      "  225752/2000000: episode: 1210, duration: 10.850s, episode steps: 213, steps per second: 20, episode reward: 141.000, mean reward: 0.662 [-1.000, 1.000], mean action: 3.113 [0.000, 6.000], mean observation: 172.897 [24.000, 255.000], loss: 0.040204, mean_absolute_error: 7.314839, mean_q: 8.521498, mean_eps: 0.796920\n",
      "  225940/2000000: episode: 1211, duration: 9.199s, episode steps: 188, steps per second: 20, episode reward: 70.900, mean reward: 0.377 [-1.000, 0.500], mean action: 3.074 [0.000, 6.000], mean observation: 173.169 [24.000, 255.000], loss: 0.039882, mean_absolute_error: 7.327473, mean_q: 8.537858, mean_eps: 0.796740\n",
      "  226102/2000000: episode: 1212, duration: 7.739s, episode steps: 162, steps per second: 21, episode reward: 66.300, mean reward: 0.409 [-1.000, 0.500], mean action: 2.772 [0.000, 6.000], mean observation: 172.061 [22.000, 255.000], loss: 0.038635, mean_absolute_error: 7.356460, mean_q: 8.571756, mean_eps: 0.796582\n",
      "  226325/2000000: episode: 1213, duration: 11.352s, episode steps: 223, steps per second: 20, episode reward: 156.900, mean reward: 0.704 [-1.000, 1.000], mean action: 3.269 [0.000, 6.000], mean observation: 172.661 [24.000, 255.000], loss: 0.035700, mean_absolute_error: 7.335957, mean_q: 8.554858, mean_eps: 0.796407\n",
      "  226514/2000000: episode: 1214, duration: 9.408s, episode steps: 189, steps per second: 20, episode reward: 121.100, mean reward: 0.641 [-1.000, 1.000], mean action: 3.095 [0.000, 6.000], mean observation: 172.036 [24.000, 255.000], loss: 0.036832, mean_absolute_error: 7.240334, mean_q: 8.449104, mean_eps: 0.796222\n",
      "  226727/2000000: episode: 1215, duration: 10.768s, episode steps: 213, steps per second: 20, episode reward: 128.300, mean reward: 0.602 [-1.000, 1.000], mean action: 3.211 [0.000, 6.000], mean observation: 172.821 [23.000, 255.000], loss: 0.035507, mean_absolute_error: 7.346941, mean_q: 8.569949, mean_eps: 0.796042\n",
      "  226922/2000000: episode: 1216, duration: 9.879s, episode steps: 195, steps per second: 20, episode reward: 128.600, mean reward: 0.659 [-1.000, 1.000], mean action: 3.062 [0.000, 6.000], mean observation: 172.421 [23.000, 255.000], loss: 0.038096, mean_absolute_error: 7.366463, mean_q: 8.600417, mean_eps: 0.795858\n",
      "  227129/2000000: episode: 1217, duration: 10.623s, episode steps: 207, steps per second: 19, episode reward: 151.900, mean reward: 0.734 [-1.000, 1.000], mean action: 3.106 [0.000, 6.000], mean observation: 172.976 [23.000, 255.000], loss: 0.035788, mean_absolute_error: 7.326843, mean_q: 8.539458, mean_eps: 0.795677\n",
      "  227292/2000000: episode: 1218, duration: 7.826s, episode steps: 163, steps per second: 21, episode reward: 74.100, mean reward: 0.455 [-1.000, 1.000], mean action: 2.877 [0.000, 6.000], mean observation: 172.021 [24.000, 255.000], loss: 0.036693, mean_absolute_error: 7.437696, mean_q: 8.682095, mean_eps: 0.795511\n",
      "  227507/2000000: episode: 1219, duration: 11.051s, episode steps: 215, steps per second: 19, episode reward: 140.000, mean reward: 0.651 [-1.000, 1.000], mean action: 3.126 [0.000, 6.000], mean observation: 172.967 [25.000, 255.000], loss: 0.036257, mean_absolute_error: 7.436475, mean_q: 8.684716, mean_eps: 0.795342\n",
      "  227666/2000000: episode: 1220, duration: 7.663s, episode steps: 159, steps per second: 21, episode reward: 83.900, mean reward: 0.528 [-1.000, 1.000], mean action: 2.799 [0.000, 6.000], mean observation: 172.044 [23.000, 255.000], loss: 0.034307, mean_absolute_error: 7.297206, mean_q: 8.507883, mean_eps: 0.795173\n",
      "  227815/2000000: episode: 1221, duration: 7.851s, episode steps: 149, steps per second: 19, episode reward: 76.700, mean reward: 0.515 [-1.000, 1.000], mean action: 3.255 [0.000, 6.000], mean observation: 173.645 [22.000, 255.000], loss: 0.043456, mean_absolute_error: 7.477401, mean_q: 8.715245, mean_eps: 0.795034\n",
      "  228026/2000000: episode: 1222, duration: 10.773s, episode steps: 211, steps per second: 20, episode reward: 146.000, mean reward: 0.692 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 172.894 [24.000, 255.000], loss: 0.038351, mean_absolute_error: 7.338669, mean_q: 8.568134, mean_eps: 0.794872\n",
      "  228224/2000000: episode: 1223, duration: 9.905s, episode steps: 198, steps per second: 20, episode reward: 120.100, mean reward: 0.607 [-1.000, 1.000], mean action: 3.040 [0.000, 6.000], mean observation: 172.400 [24.000, 255.000], loss: 0.038321, mean_absolute_error: 7.454100, mean_q: 8.710892, mean_eps: 0.794688\n",
      "  228440/2000000: episode: 1224, duration: 11.093s, episode steps: 216, steps per second: 19, episode reward: 166.600, mean reward: 0.771 [-1.000, 1.000], mean action: 3.282 [0.000, 6.000], mean observation: 172.624 [25.000, 255.000], loss: 0.036839, mean_absolute_error: 7.362474, mean_q: 8.602158, mean_eps: 0.794503\n",
      "  228613/2000000: episode: 1225, duration: 8.255s, episode steps: 173, steps per second: 21, episode reward: 50.600, mean reward: 0.292 [-1.000, 0.500], mean action: 2.884 [0.000, 6.000], mean observation: 172.562 [24.000, 255.000], loss: 0.036710, mean_absolute_error: 7.485199, mean_q: 8.727940, mean_eps: 0.794327\n",
      "  228762/2000000: episode: 1226, duration: 7.034s, episode steps: 149, steps per second: 21, episode reward: 57.800, mean reward: 0.388 [-1.000, 0.500], mean action: 2.698 [0.000, 6.000], mean observation: 172.168 [24.000, 255.000], loss: 0.037440, mean_absolute_error: 7.383119, mean_q: 8.620050, mean_eps: 0.794181\n",
      "  228955/2000000: episode: 1227, duration: 9.618s, episode steps: 193, steps per second: 20, episode reward: 150.400, mean reward: 0.779 [-1.000, 1.000], mean action: 3.073 [0.000, 6.000], mean observation: 171.732 [25.000, 255.000], loss: 0.032405, mean_absolute_error: 7.478433, mean_q: 8.730329, mean_eps: 0.794028\n",
      "  229162/2000000: episode: 1228, duration: 10.508s, episode steps: 207, steps per second: 20, episode reward: 144.300, mean reward: 0.697 [-1.000, 1.000], mean action: 3.092 [0.000, 6.000], mean observation: 172.331 [23.000, 255.000], loss: 0.037309, mean_absolute_error: 7.536737, mean_q: 8.802745, mean_eps: 0.793848\n",
      "  229328/2000000: episode: 1229, duration: 7.944s, episode steps: 166, steps per second: 21, episode reward: 71.200, mean reward: 0.429 [-1.000, 1.000], mean action: 2.970 [0.000, 6.000], mean observation: 171.800 [23.000, 255.000], loss: 0.040416, mean_absolute_error: 7.610174, mean_q: 8.882692, mean_eps: 0.793680\n",
      "  229534/2000000: episode: 1230, duration: 10.372s, episode steps: 206, steps per second: 20, episode reward: 144.300, mean reward: 0.700 [-1.000, 1.000], mean action: 2.927 [0.000, 6.000], mean observation: 173.483 [24.000, 255.000], loss: 0.042179, mean_absolute_error: 7.437226, mean_q: 8.685058, mean_eps: 0.793513\n",
      "  229749/2000000: episode: 1231, duration: 11.064s, episode steps: 215, steps per second: 19, episode reward: 128.600, mean reward: 0.598 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 173.249 [24.000, 255.000], loss: 0.042316, mean_absolute_error: 7.371123, mean_q: 8.605680, mean_eps: 0.793322\n",
      "  229957/2000000: episode: 1232, duration: 10.626s, episode steps: 208, steps per second: 20, episode reward: 141.600, mean reward: 0.681 [-1.000, 1.000], mean action: 2.947 [0.000, 6.000], mean observation: 173.027 [24.000, 255.000], loss: 0.035355, mean_absolute_error: 7.425858, mean_q: 8.667987, mean_eps: 0.793131\n",
      "  230167/2000000: episode: 1233, duration: 10.687s, episode steps: 210, steps per second: 20, episode reward: 139.300, mean reward: 0.663 [-1.000, 1.000], mean action: 3.281 [0.000, 6.000], mean observation: 172.626 [23.000, 255.000], loss: 0.061202, mean_absolute_error: 7.772445, mean_q: 9.086095, mean_eps: 0.792944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  230376/2000000: episode: 1234, duration: 10.494s, episode steps: 209, steps per second: 20, episode reward: 116.700, mean reward: 0.558 [-1.000, 1.000], mean action: 3.225 [0.000, 6.000], mean observation: 172.911 [24.000, 255.000], loss: 0.042853, mean_absolute_error: 7.903509, mean_q: 9.238590, mean_eps: 0.792757\n",
      "  230529/2000000: episode: 1235, duration: 7.197s, episode steps: 153, steps per second: 21, episode reward: 49.400, mean reward: 0.323 [-1.000, 0.500], mean action: 2.758 [0.000, 6.000], mean observation: 172.725 [23.000, 255.000], loss: 0.038743, mean_absolute_error: 7.720376, mean_q: 9.008449, mean_eps: 0.792593\n",
      "  230724/2000000: episode: 1236, duration: 9.822s, episode steps: 195, steps per second: 20, episode reward: 101.300, mean reward: 0.519 [-1.000, 1.000], mean action: 2.954 [0.000, 6.000], mean observation: 172.554 [24.000, 255.000], loss: 0.049801, mean_absolute_error: 7.826747, mean_q: 9.148859, mean_eps: 0.792437\n",
      "  230925/2000000: episode: 1237, duration: 10.213s, episode steps: 201, steps per second: 20, episode reward: 147.200, mean reward: 0.732 [-1.000, 1.000], mean action: 3.139 [0.000, 6.000], mean observation: 172.325 [24.000, 255.000], loss: 0.046334, mean_absolute_error: 7.947896, mean_q: 9.291925, mean_eps: 0.792258\n",
      "  231070/2000000: episode: 1238, duration: 6.891s, episode steps: 145, steps per second: 21, episode reward: 53.400, mean reward: 0.368 [-1.000, 0.500], mean action: 2.945 [0.000, 6.000], mean observation: 172.405 [23.000, 255.000], loss: 0.045339, mean_absolute_error: 8.026132, mean_q: 9.381524, mean_eps: 0.792102\n",
      "  231243/2000000: episode: 1239, duration: 8.284s, episode steps: 173, steps per second: 21, episode reward: 67.400, mean reward: 0.390 [-1.000, 0.500], mean action: 2.960 [0.000, 6.000], mean observation: 172.090 [24.000, 255.000], loss: 0.042406, mean_absolute_error: 7.872380, mean_q: 9.234825, mean_eps: 0.791960\n",
      "  231455/2000000: episode: 1240, duration: 10.734s, episode steps: 212, steps per second: 20, episode reward: 124.600, mean reward: 0.588 [-1.000, 1.000], mean action: 3.075 [0.000, 6.000], mean observation: 172.524 [23.000, 255.000], loss: 0.042860, mean_absolute_error: 7.868445, mean_q: 9.187480, mean_eps: 0.791787\n",
      "  231582/2000000: episode: 1241, duration: 6.761s, episode steps: 127, steps per second: 19, episode reward: 71.700, mean reward: 0.565 [-1.000, 1.000], mean action: 3.394 [0.000, 6.000], mean observation: 173.409 [23.000, 255.000], loss: 0.047451, mean_absolute_error: 7.813720, mean_q: 9.124419, mean_eps: 0.791634\n",
      "  231813/2000000: episode: 1242, duration: 12.017s, episode steps: 231, steps per second: 19, episode reward: 109.300, mean reward: 0.473 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 173.186 [23.000, 255.000], loss: 0.040492, mean_absolute_error: 7.731141, mean_q: 9.027036, mean_eps: 0.791472\n",
      "  232010/2000000: episode: 1243, duration: 9.937s, episode steps: 197, steps per second: 20, episode reward: 138.000, mean reward: 0.701 [-1.000, 1.000], mean action: 3.061 [0.000, 6.000], mean observation: 171.701 [24.000, 255.000], loss: 0.036335, mean_absolute_error: 7.747495, mean_q: 9.043362, mean_eps: 0.791279\n",
      "  232192/2000000: episode: 1244, duration: 8.873s, episode steps: 182, steps per second: 21, episode reward: 120.800, mean reward: 0.664 [-1.000, 1.000], mean action: 2.813 [0.000, 6.000], mean observation: 170.999 [23.000, 255.000], loss: 0.042906, mean_absolute_error: 7.785656, mean_q: 9.097395, mean_eps: 0.791110\n",
      "  232386/2000000: episode: 1245, duration: 9.720s, episode steps: 194, steps per second: 20, episode reward: 136.700, mean reward: 0.705 [-1.000, 1.000], mean action: 2.969 [0.000, 6.000], mean observation: 171.531 [24.000, 255.000], loss: 0.042814, mean_absolute_error: 7.973200, mean_q: 9.304083, mean_eps: 0.790941\n",
      "  232529/2000000: episode: 1246, duration: 6.629s, episode steps: 143, steps per second: 22, episode reward: 46.400, mean reward: 0.324 [-1.000, 0.500], mean action: 2.636 [0.000, 6.000], mean observation: 172.928 [23.000, 255.000], loss: 0.050518, mean_absolute_error: 8.033522, mean_q: 9.403743, mean_eps: 0.790788\n",
      "  232755/2000000: episode: 1247, duration: 11.630s, episode steps: 226, steps per second: 19, episode reward: 118.100, mean reward: 0.523 [-1.000, 1.000], mean action: 3.168 [0.000, 6.000], mean observation: 172.663 [24.000, 255.000], loss: 0.045829, mean_absolute_error: 7.897526, mean_q: 9.225297, mean_eps: 0.790622\n",
      "  232940/2000000: episode: 1248, duration: 9.332s, episode steps: 185, steps per second: 20, episode reward: 104.500, mean reward: 0.565 [-1.000, 1.000], mean action: 2.924 [0.000, 6.000], mean observation: 171.279 [23.000, 255.000], loss: 0.051772, mean_absolute_error: 7.888091, mean_q: 9.215569, mean_eps: 0.790439\n",
      "  233093/2000000: episode: 1249, duration: 7.290s, episode steps: 153, steps per second: 21, episode reward: 57.400, mean reward: 0.375 [-1.000, 0.500], mean action: 2.660 [0.000, 6.000], mean observation: 172.116 [23.000, 255.000], loss: 0.043449, mean_absolute_error: 7.819489, mean_q: 9.111724, mean_eps: 0.790286\n",
      "  233301/2000000: episode: 1250, duration: 10.556s, episode steps: 208, steps per second: 20, episode reward: 133.200, mean reward: 0.640 [-1.000, 1.000], mean action: 3.168 [0.000, 6.000], mean observation: 172.694 [24.000, 255.000], loss: 0.038582, mean_absolute_error: 7.820110, mean_q: 9.133127, mean_eps: 0.790122\n",
      "  233487/2000000: episode: 1251, duration: 9.206s, episode steps: 186, steps per second: 20, episode reward: 120.300, mean reward: 0.647 [-1.000, 1.000], mean action: 2.892 [0.000, 6.000], mean observation: 171.425 [23.000, 255.000], loss: 0.041152, mean_absolute_error: 7.819091, mean_q: 9.117278, mean_eps: 0.789945\n",
      "  233714/2000000: episode: 1252, duration: 11.635s, episode steps: 227, steps per second: 20, episode reward: 149.600, mean reward: 0.659 [-1.000, 1.000], mean action: 3.211 [0.000, 6.000], mean observation: 172.393 [23.000, 255.000], loss: 0.044218, mean_absolute_error: 7.825675, mean_q: 9.159739, mean_eps: 0.789760\n",
      "  233906/2000000: episode: 1253, duration: 9.589s, episode steps: 192, steps per second: 20, episode reward: 117.300, mean reward: 0.611 [-1.000, 1.000], mean action: 3.214 [0.000, 6.000], mean observation: 171.286 [25.000, 255.000], loss: 0.047304, mean_absolute_error: 8.078986, mean_q: 9.443947, mean_eps: 0.789571\n",
      "  234095/2000000: episode: 1254, duration: 9.510s, episode steps: 189, steps per second: 20, episode reward: 94.500, mean reward: 0.500 [-1.000, 1.000], mean action: 2.921 [0.000, 6.000], mean observation: 171.668 [23.000, 255.000], loss: 0.044557, mean_absolute_error: 7.877867, mean_q: 9.199021, mean_eps: 0.789400\n",
      "  234261/2000000: episode: 1255, duration: 8.105s, episode steps: 166, steps per second: 20, episode reward: 107.300, mean reward: 0.646 [-1.000, 1.000], mean action: 2.777 [0.000, 6.000], mean observation: 170.466 [24.000, 255.000], loss: 0.045514, mean_absolute_error: 7.869637, mean_q: 9.195639, mean_eps: 0.789240\n",
      "  234489/2000000: episode: 1256, duration: 11.798s, episode steps: 228, steps per second: 19, episode reward: 111.600, mean reward: 0.489 [-1.000, 1.000], mean action: 3.289 [0.000, 6.000], mean observation: 172.402 [23.000, 255.000], loss: 0.048624, mean_absolute_error: 8.034762, mean_q: 9.386806, mean_eps: 0.789062\n",
      "  234685/2000000: episode: 1257, duration: 10.041s, episode steps: 196, steps per second: 20, episode reward: 135.800, mean reward: 0.693 [-1.000, 1.000], mean action: 3.240 [0.000, 6.000], mean observation: 171.400 [23.000, 255.000], loss: 0.043103, mean_absolute_error: 7.890994, mean_q: 9.230832, mean_eps: 0.788871\n",
      "  234822/2000000: episode: 1258, duration: 7.237s, episode steps: 137, steps per second: 19, episode reward: 73.400, mean reward: 0.536 [-1.000, 1.000], mean action: 3.409 [0.000, 6.000], mean observation: 172.232 [22.000, 255.000], loss: 0.048937, mean_absolute_error: 8.083436, mean_q: 9.440670, mean_eps: 0.788721\n",
      "  234969/2000000: episode: 1259, duration: 6.865s, episode steps: 147, steps per second: 21, episode reward: 50.400, mean reward: 0.343 [-1.000, 0.500], mean action: 2.660 [0.000, 6.000], mean observation: 172.001 [24.000, 255.000], loss: 0.052819, mean_absolute_error: 8.009850, mean_q: 9.371949, mean_eps: 0.788594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  235160/2000000: episode: 1260, duration: 9.460s, episode steps: 191, steps per second: 20, episode reward: 120.000, mean reward: 0.628 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 171.080 [23.000, 255.000], loss: 0.039344, mean_absolute_error: 7.863026, mean_q: 9.187043, mean_eps: 0.788442\n",
      "  235293/2000000: episode: 1261, duration: 7.161s, episode steps: 133, steps per second: 19, episode reward: 71.300, mean reward: 0.536 [-1.000, 1.000], mean action: 3.451 [0.000, 6.000], mean observation: 172.599 [24.000, 255.000], loss: 0.039944, mean_absolute_error: 7.870234, mean_q: 9.202430, mean_eps: 0.788297\n",
      "  235487/2000000: episode: 1262, duration: 9.690s, episode steps: 194, steps per second: 20, episode reward: 118.100, mean reward: 0.609 [-1.000, 1.000], mean action: 3.067 [0.000, 6.000], mean observation: 170.871 [24.000, 255.000], loss: 0.043788, mean_absolute_error: 7.972942, mean_q: 9.326177, mean_eps: 0.788149\n",
      "  235700/2000000: episode: 1263, duration: 10.906s, episode steps: 213, steps per second: 20, episode reward: 151.900, mean reward: 0.713 [-1.000, 1.000], mean action: 3.211 [0.000, 6.000], mean observation: 172.133 [24.000, 255.000], loss: 0.041399, mean_absolute_error: 7.972512, mean_q: 9.332860, mean_eps: 0.787967\n",
      "  235896/2000000: episode: 1264, duration: 16.459s, episode steps: 196, steps per second: 12, episode reward: 133.800, mean reward: 0.683 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 171.315 [23.000, 255.000], loss: 0.039059, mean_absolute_error: 7.841258, mean_q: 9.163332, mean_eps: 0.787784\n",
      "  236194/2000000: episode: 1265, duration: 99.635s, episode steps: 298, steps per second: 3, episode reward: 184.800, mean reward: 0.620 [-1.000, 1.000], mean action: 3.060 [0.000, 6.000], mean observation: 175.545 [30.000, 255.000], loss: 0.042400, mean_absolute_error: 7.918069, mean_q: 9.236663, mean_eps: 0.787560\n",
      "  236480/2000000: episode: 1266, duration: 95.682s, episode steps: 286, steps per second: 3, episode reward: 118.700, mean reward: 0.415 [-1.000, 0.500], mean action: 3.035 [0.000, 6.000], mean observation: 176.613 [30.000, 255.000], loss: 0.042693, mean_absolute_error: 7.952299, mean_q: 9.288398, mean_eps: 0.787298\n",
      "  236698/2000000: episode: 1267, duration: 72.965s, episode steps: 218, steps per second: 3, episode reward: 79.100, mean reward: 0.363 [-1.000, 0.500], mean action: 2.720 [0.000, 6.000], mean observation: 177.571 [31.000, 255.000], loss: 0.040056, mean_absolute_error: 7.775476, mean_q: 9.087504, mean_eps: 0.787071\n",
      "  237085/2000000: episode: 1268, duration: 129.363s, episode steps: 387, steps per second: 3, episode reward: 285.600, mean reward: 0.738 [-1.000, 1.000], mean action: 3.145 [0.000, 6.000], mean observation: 177.597 [29.000, 255.000], loss: 0.042532, mean_absolute_error: 7.863568, mean_q: 9.185350, mean_eps: 0.786797\n",
      "  237377/2000000: episode: 1269, duration: 89.659s, episode steps: 292, steps per second: 3, episode reward: 116.900, mean reward: 0.400 [-1.000, 0.500], mean action: 2.856 [0.000, 6.000], mean observation: 176.800 [20.000, 255.000], loss: 0.034610, mean_absolute_error: 7.927760, mean_q: 9.258630, mean_eps: 0.786491\n",
      "  237576/2000000: episode: 1270, duration: 10.075s, episode steps: 199, steps per second: 20, episode reward: 117.300, mean reward: 0.589 [-1.000, 1.000], mean action: 2.940 [0.000, 6.000], mean observation: 172.947 [23.000, 255.000], loss: 0.041002, mean_absolute_error: 7.822827, mean_q: 9.122098, mean_eps: 0.786272\n",
      "  237765/2000000: episode: 1271, duration: 9.250s, episode steps: 189, steps per second: 20, episode reward: 87.000, mean reward: 0.460 [-1.000, 0.500], mean action: 3.402 [0.000, 6.000], mean observation: 171.969 [23.000, 255.000], loss: 0.046542, mean_absolute_error: 7.911565, mean_q: 9.234841, mean_eps: 0.786097\n",
      "  238027/2000000: episode: 1272, duration: 10.875s, episode steps: 262, steps per second: 24, episode reward: 173.900, mean reward: 0.664 [-1.000, 1.000], mean action: 3.164 [0.000, 6.000], mean observation: 172.799 [23.000, 255.000], loss: 0.048135, mean_absolute_error: 7.866827, mean_q: 9.200330, mean_eps: 0.785894\n",
      "  238211/2000000: episode: 1273, duration: 7.213s, episode steps: 184, steps per second: 26, episode reward: 70.100, mean reward: 0.381 [-1.000, 0.500], mean action: 2.810 [0.000, 6.000], mean observation: 172.068 [25.000, 255.000], loss: 0.039480, mean_absolute_error: 7.848096, mean_q: 9.166454, mean_eps: 0.785694\n",
      "  238491/2000000: episode: 1274, duration: 11.081s, episode steps: 280, steps per second: 25, episode reward: 178.000, mean reward: 0.636 [-1.000, 1.000], mean action: 3.204 [0.000, 6.000], mean observation: 172.718 [24.000, 255.000], loss: 0.041064, mean_absolute_error: 7.923416, mean_q: 9.262463, mean_eps: 0.785485\n",
      "  238704/2000000: episode: 1275, duration: 8.276s, episode steps: 213, steps per second: 26, episode reward: 85.000, mean reward: 0.399 [-1.000, 0.500], mean action: 2.883 [0.000, 6.000], mean observation: 172.227 [23.000, 255.000], loss: 0.038763, mean_absolute_error: 7.885383, mean_q: 9.212904, mean_eps: 0.785264\n",
      "  238936/2000000: episode: 1276, duration: 9.707s, episode steps: 232, steps per second: 24, episode reward: 158.000, mean reward: 0.681 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 171.919 [23.000, 255.000], loss: 0.038536, mean_absolute_error: 7.939865, mean_q: 9.273513, mean_eps: 0.785064\n",
      "  239183/2000000: episode: 1277, duration: 10.404s, episode steps: 247, steps per second: 24, episode reward: 162.400, mean reward: 0.657 [-1.000, 1.000], mean action: 3.231 [0.000, 6.000], mean observation: 172.699 [25.000, 255.000], loss: 0.041181, mean_absolute_error: 7.828926, mean_q: 9.147399, mean_eps: 0.784848\n",
      "  239410/2000000: episode: 1278, duration: 9.269s, episode steps: 227, steps per second: 24, episode reward: 159.700, mean reward: 0.704 [-1.000, 1.000], mean action: 2.969 [0.000, 6.000], mean observation: 170.969 [24.000, 255.000], loss: 0.039242, mean_absolute_error: 7.836606, mean_q: 9.154591, mean_eps: 0.784634\n",
      "  239664/2000000: episode: 1279, duration: 10.701s, episode steps: 254, steps per second: 24, episode reward: 151.600, mean reward: 0.597 [-1.000, 1.000], mean action: 3.110 [0.000, 6.000], mean observation: 173.189 [24.000, 255.000], loss: 0.042809, mean_absolute_error: 7.871923, mean_q: 9.177776, mean_eps: 0.784418\n",
      "  239919/2000000: episode: 1280, duration: 10.435s, episode steps: 255, steps per second: 24, episode reward: 98.000, mean reward: 0.384 [-1.000, 0.500], mean action: 3.275 [0.000, 6.000], mean observation: 173.180 [24.000, 255.000], loss: 0.044295, mean_absolute_error: 7.949750, mean_q: 9.279232, mean_eps: 0.784189\n",
      "  240187/2000000: episode: 1281, duration: 11.457s, episode steps: 268, steps per second: 23, episode reward: 166.200, mean reward: 0.620 [-1.000, 1.000], mean action: 3.067 [0.000, 6.000], mean observation: 172.390 [24.000, 255.000], loss: 0.058433, mean_absolute_error: 8.179675, mean_q: 9.565349, mean_eps: 0.783953\n",
      "  240468/2000000: episode: 1282, duration: 11.020s, episode steps: 281, steps per second: 25, episode reward: 214.500, mean reward: 0.763 [-1.000, 1.000], mean action: 3.217 [0.000, 6.000], mean observation: 172.176 [24.000, 255.000], loss: 0.042313, mean_absolute_error: 8.289131, mean_q: 9.692738, mean_eps: 0.783707\n",
      "  240648/2000000: episode: 1283, duration: 6.345s, episode steps: 180, steps per second: 28, episode reward: 62.500, mean reward: 0.347 [-1.000, 0.500], mean action: 2.717 [0.000, 6.000], mean observation: 172.798 [23.000, 255.000], loss: 0.040397, mean_absolute_error: 8.273577, mean_q: 9.669353, mean_eps: 0.783500\n",
      "  240931/2000000: episode: 1284, duration: 11.327s, episode steps: 283, steps per second: 25, episode reward: 172.000, mean reward: 0.608 [-1.000, 1.000], mean action: 3.145 [0.000, 6.000], mean observation: 172.063 [23.000, 255.000], loss: 0.043654, mean_absolute_error: 8.133377, mean_q: 9.502754, mean_eps: 0.783291\n",
      "  241085/2000000: episode: 1285, duration: 6.820s, episode steps: 154, steps per second: 23, episode reward: 92.700, mean reward: 0.602 [-1.000, 1.000], mean action: 3.338 [0.000, 6.000], mean observation: 172.852 [23.000, 255.000], loss: 0.047486, mean_absolute_error: 8.172504, mean_q: 9.559782, mean_eps: 0.783093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  241314/2000000: episode: 1286, duration: 9.404s, episode steps: 229, steps per second: 24, episode reward: 108.000, mean reward: 0.472 [-1.000, 1.000], mean action: 2.987 [0.000, 6.000], mean observation: 172.361 [23.000, 255.000], loss: 0.049006, mean_absolute_error: 8.191283, mean_q: 9.562855, mean_eps: 0.782920\n",
      "  241482/2000000: episode: 1287, duration: 6.809s, episode steps: 168, steps per second: 25, episode reward: 66.500, mean reward: 0.396 [-1.000, 0.500], mean action: 2.827 [0.000, 6.000], mean observation: 172.520 [24.000, 255.000], loss: 0.048976, mean_absolute_error: 8.119226, mean_q: 9.477423, mean_eps: 0.782742\n",
      "  241727/2000000: episode: 1288, duration: 10.517s, episode steps: 245, steps per second: 23, episode reward: 171.200, mean reward: 0.699 [-1.000, 1.000], mean action: 2.980 [0.000, 6.000], mean observation: 172.701 [24.000, 255.000], loss: 0.047237, mean_absolute_error: 8.025545, mean_q: 9.372753, mean_eps: 0.782556\n",
      "  242005/2000000: episode: 1289, duration: 10.998s, episode steps: 278, steps per second: 25, episode reward: 131.500, mean reward: 0.473 [-1.000, 1.000], mean action: 3.187 [0.000, 6.000], mean observation: 173.280 [22.000, 255.000], loss: 0.048333, mean_absolute_error: 8.222470, mean_q: 9.601610, mean_eps: 0.782321\n",
      "  242262/2000000: episode: 1290, duration: 10.096s, episode steps: 257, steps per second: 25, episode reward: 167.000, mean reward: 0.650 [-1.000, 1.000], mean action: 3.086 [0.000, 6.000], mean observation: 172.397 [23.000, 255.000], loss: 0.042738, mean_absolute_error: 8.067332, mean_q: 9.416715, mean_eps: 0.782079\n",
      "  242500/2000000: episode: 1291, duration: 8.671s, episode steps: 238, steps per second: 27, episode reward: 94.300, mean reward: 0.396 [-1.000, 0.500], mean action: 2.908 [0.000, 6.000], mean observation: 172.712 [24.000, 255.000], loss: 0.051644, mean_absolute_error: 8.282176, mean_q: 9.659230, mean_eps: 0.781858\n",
      "  242755/2000000: episode: 1292, duration: 9.959s, episode steps: 255, steps per second: 26, episode reward: 122.000, mean reward: 0.478 [-1.000, 1.000], mean action: 3.110 [0.000, 6.000], mean observation: 172.516 [24.000, 255.000], loss: 0.051643, mean_absolute_error: 8.127330, mean_q: 9.478188, mean_eps: 0.781637\n",
      "  243026/2000000: episode: 1293, duration: 10.651s, episode steps: 271, steps per second: 25, episode reward: 180.300, mean reward: 0.665 [-1.000, 1.000], mean action: 2.845 [0.000, 6.000], mean observation: 173.302 [23.000, 255.000], loss: 0.044584, mean_absolute_error: 8.062554, mean_q: 9.404184, mean_eps: 0.781399\n",
      "  243318/2000000: episode: 1294, duration: 12.169s, episode steps: 292, steps per second: 24, episode reward: 143.800, mean reward: 0.492 [-1.000, 1.000], mean action: 3.134 [0.000, 6.000], mean observation: 173.232 [22.000, 255.000], loss: 0.048351, mean_absolute_error: 8.182168, mean_q: 9.549240, mean_eps: 0.781145\n",
      "  243547/2000000: episode: 1295, duration: 9.061s, episode steps: 229, steps per second: 25, episode reward: 141.200, mean reward: 0.617 [-1.000, 1.000], mean action: 2.917 [0.000, 6.000], mean observation: 171.570 [24.000, 255.000], loss: 0.044316, mean_absolute_error: 8.198339, mean_q: 9.563957, mean_eps: 0.780911\n",
      "  243806/2000000: episode: 1296, duration: 10.749s, episode steps: 259, steps per second: 24, episode reward: 173.700, mean reward: 0.671 [-1.000, 1.000], mean action: 3.239 [0.000, 6.000], mean observation: 172.719 [24.000, 255.000], loss: 0.044428, mean_absolute_error: 8.196884, mean_q: 9.567756, mean_eps: 0.780692\n",
      "  244060/2000000: episode: 1297, duration: 9.813s, episode steps: 254, steps per second: 26, episode reward: 149.900, mean reward: 0.590 [-1.000, 1.000], mean action: 3.213 [0.000, 6.000], mean observation: 172.617 [23.000, 255.000], loss: 0.044903, mean_absolute_error: 8.116994, mean_q: 9.488392, mean_eps: 0.780461\n",
      "  244294/2000000: episode: 1298, duration: 8.587s, episode steps: 234, steps per second: 27, episode reward: 84.700, mean reward: 0.362 [-1.000, 0.500], mean action: 3.051 [0.000, 6.000], mean observation: 173.194 [21.000, 255.000], loss: 0.042649, mean_absolute_error: 8.160321, mean_q: 9.516303, mean_eps: 0.780242\n",
      "  244453/2000000: episode: 1299, duration: 6.709s, episode steps: 159, steps per second: 24, episode reward: 90.300, mean reward: 0.568 [-1.000, 1.000], mean action: 3.346 [0.000, 6.000], mean observation: 173.728 [24.000, 255.000], loss: 0.048305, mean_absolute_error: 8.088905, mean_q: 9.447804, mean_eps: 0.780063\n",
      "  244650/2000000: episode: 1300, duration: 7.142s, episode steps: 197, steps per second: 28, episode reward: 71.400, mean reward: 0.362 [-1.000, 0.500], mean action: 2.909 [0.000, 6.000], mean observation: 173.039 [24.000, 255.000], loss: 0.043365, mean_absolute_error: 8.186743, mean_q: 9.556770, mean_eps: 0.779903\n",
      "  244924/2000000: episode: 1301, duration: 10.873s, episode steps: 274, steps per second: 25, episode reward: 135.800, mean reward: 0.496 [-1.000, 1.000], mean action: 3.318 [0.000, 6.000], mean observation: 173.499 [24.000, 255.000], loss: 0.044182, mean_absolute_error: 7.962484, mean_q: 9.295193, mean_eps: 0.779693\n",
      "  245212/2000000: episode: 1302, duration: 11.639s, episode steps: 288, steps per second: 25, episode reward: 135.200, mean reward: 0.469 [-1.000, 1.000], mean action: 3.233 [0.000, 6.000], mean observation: 173.980 [23.000, 255.000], loss: 0.041149, mean_absolute_error: 8.113153, mean_q: 9.484631, mean_eps: 0.779441\n",
      "  245474/2000000: episode: 1303, duration: 10.372s, episode steps: 262, steps per second: 25, episode reward: 188.300, mean reward: 0.719 [-1.000, 1.000], mean action: 3.088 [0.000, 6.000], mean observation: 173.077 [24.000, 255.000], loss: 0.041481, mean_absolute_error: 8.120248, mean_q: 9.491377, mean_eps: 0.779192\n",
      "  245707/2000000: episode: 1304, duration: 9.270s, episode steps: 233, steps per second: 25, episode reward: 136.900, mean reward: 0.588 [-1.000, 1.000], mean action: 3.163 [0.000, 6.000], mean observation: 172.214 [24.000, 255.000], loss: 0.045608, mean_absolute_error: 8.082527, mean_q: 9.427063, mean_eps: 0.778969\n",
      "  245941/2000000: episode: 1305, duration: 9.701s, episode steps: 234, steps per second: 24, episode reward: 153.000, mean reward: 0.654 [-1.000, 1.000], mean action: 3.107 [0.000, 6.000], mean observation: 172.588 [24.000, 255.000], loss: 0.038556, mean_absolute_error: 8.066258, mean_q: 9.409139, mean_eps: 0.778758\n",
      "  246154/2000000: episode: 1306, duration: 8.690s, episode steps: 213, steps per second: 25, episode reward: 132.900, mean reward: 0.624 [-1.000, 1.000], mean action: 3.066 [0.000, 6.000], mean observation: 172.189 [24.000, 255.000], loss: 0.047983, mean_absolute_error: 8.249868, mean_q: 9.627038, mean_eps: 0.778557\n",
      "  246394/2000000: episode: 1307, duration: 9.318s, episode steps: 240, steps per second: 26, episode reward: 151.000, mean reward: 0.629 [-1.000, 1.000], mean action: 3.104 [0.000, 6.000], mean observation: 171.994 [23.000, 255.000], loss: 0.045381, mean_absolute_error: 8.058015, mean_q: 9.406322, mean_eps: 0.778353\n",
      "  246633/2000000: episode: 1308, duration: 8.991s, episode steps: 239, steps per second: 27, episode reward: 94.800, mean reward: 0.397 [-1.000, 0.500], mean action: 3.029 [0.000, 6.000], mean observation: 172.893 [24.000, 255.000], loss: 0.042803, mean_absolute_error: 8.117791, mean_q: 9.477887, mean_eps: 0.778137\n",
      "  246906/2000000: episode: 1309, duration: 10.735s, episode steps: 273, steps per second: 25, episode reward: 166.300, mean reward: 0.609 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 172.838 [23.000, 255.000], loss: 0.045180, mean_absolute_error: 8.176182, mean_q: 9.548121, mean_eps: 0.777907\n",
      "  247143/2000000: episode: 1310, duration: 9.098s, episode steps: 237, steps per second: 26, episode reward: 77.800, mean reward: 0.328 [-1.000, 0.500], mean action: 2.806 [0.000, 6.000], mean observation: 172.873 [24.000, 255.000], loss: 0.048453, mean_absolute_error: 8.306151, mean_q: 9.711664, mean_eps: 0.777678\n",
      "  247387/2000000: episode: 1311, duration: 9.170s, episode steps: 244, steps per second: 27, episode reward: 95.700, mean reward: 0.392 [-1.000, 0.500], mean action: 3.012 [0.000, 6.000], mean observation: 172.473 [23.000, 255.000], loss: 0.052193, mean_absolute_error: 8.095733, mean_q: 9.450315, mean_eps: 0.777462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  247569/2000000: episode: 1312, duration: 6.781s, episode steps: 182, steps per second: 27, episode reward: 75.500, mean reward: 0.415 [-1.000, 0.500], mean action: 2.962 [0.000, 6.000], mean observation: 172.185 [24.000, 255.000], loss: 0.048771, mean_absolute_error: 8.011139, mean_q: 9.364354, mean_eps: 0.777270\n",
      "  247838/2000000: episode: 1313, duration: 10.884s, episode steps: 269, steps per second: 25, episode reward: 172.900, mean reward: 0.643 [-1.000, 1.000], mean action: 3.156 [0.000, 6.000], mean observation: 173.058 [23.000, 255.000], loss: 0.043529, mean_absolute_error: 8.105471, mean_q: 9.459284, mean_eps: 0.777066\n",
      "  248130/2000000: episode: 1314, duration: 11.831s, episode steps: 292, steps per second: 25, episode reward: 200.900, mean reward: 0.688 [-1.000, 1.000], mean action: 3.257 [0.000, 6.000], mean observation: 173.197 [23.000, 255.000], loss: 0.043968, mean_absolute_error: 8.165351, mean_q: 9.535113, mean_eps: 0.776814\n",
      "  248390/2000000: episode: 1315, duration: 10.058s, episode steps: 260, steps per second: 26, episode reward: 158.100, mean reward: 0.608 [-1.000, 1.000], mean action: 3.050 [0.000, 6.000], mean observation: 173.219 [24.000, 255.000], loss: 0.041973, mean_absolute_error: 8.079692, mean_q: 9.430607, mean_eps: 0.776566\n",
      "  248629/2000000: episode: 1316, duration: 9.232s, episode steps: 239, steps per second: 26, episode reward: 129.400, mean reward: 0.541 [-1.000, 1.000], mean action: 3.059 [0.000, 6.000], mean observation: 172.273 [24.000, 255.000], loss: 0.044341, mean_absolute_error: 8.203253, mean_q: 9.577104, mean_eps: 0.776341\n",
      "  248869/2000000: episode: 1317, duration: 9.176s, episode steps: 240, steps per second: 26, episode reward: 160.400, mean reward: 0.668 [-1.000, 1.000], mean action: 2.958 [0.000, 6.000], mean observation: 171.532 [24.000, 255.000], loss: 0.044337, mean_absolute_error: 8.227136, mean_q: 9.612049, mean_eps: 0.776125\n",
      "  249125/2000000: episode: 1318, duration: 10.012s, episode steps: 256, steps per second: 26, episode reward: 177.800, mean reward: 0.695 [-1.000, 1.000], mean action: 3.164 [0.000, 6.000], mean observation: 172.545 [24.000, 255.000], loss: 0.043573, mean_absolute_error: 8.295402, mean_q: 9.685038, mean_eps: 0.775902\n",
      "  249393/2000000: episode: 1319, duration: 10.922s, episode steps: 268, steps per second: 25, episode reward: 141.900, mean reward: 0.529 [-1.000, 1.000], mean action: 3.075 [0.000, 6.000], mean observation: 172.994 [25.000, 255.000], loss: 0.040639, mean_absolute_error: 8.052248, mean_q: 9.414504, mean_eps: 0.775666\n",
      "  249628/2000000: episode: 1320, duration: 8.553s, episode steps: 235, steps per second: 27, episode reward: 119.000, mean reward: 0.506 [-1.000, 1.000], mean action: 3.026 [0.000, 6.000], mean observation: 171.687 [24.000, 255.000], loss: 0.042055, mean_absolute_error: 7.997148, mean_q: 9.321622, mean_eps: 0.775441\n",
      "  249911/2000000: episode: 1321, duration: 10.850s, episode steps: 283, steps per second: 26, episode reward: 143.000, mean reward: 0.505 [-1.000, 1.000], mean action: 3.247 [0.000, 6.000], mean observation: 172.685 [23.000, 255.000], loss: 0.043545, mean_absolute_error: 8.220264, mean_q: 9.608333, mean_eps: 0.775209\n",
      "  250161/2000000: episode: 1322, duration: 9.617s, episode steps: 250, steps per second: 26, episode reward: 174.600, mean reward: 0.698 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 171.606 [23.000, 255.000], loss: 0.063591, mean_absolute_error: 8.323409, mean_q: 9.718035, mean_eps: 0.774968\n",
      "  250427/2000000: episode: 1323, duration: 11.033s, episode steps: 266, steps per second: 24, episode reward: 197.900, mean reward: 0.744 [-1.000, 1.000], mean action: 3.165 [0.000, 6.000], mean observation: 172.358 [25.000, 255.000], loss: 0.049085, mean_absolute_error: 8.266624, mean_q: 9.647248, mean_eps: 0.774735\n",
      "  250646/2000000: episode: 1324, duration: 8.908s, episode steps: 219, steps per second: 25, episode reward: 127.500, mean reward: 0.582 [-1.000, 1.000], mean action: 3.100 [0.000, 6.000], mean observation: 171.666 [23.000, 255.000], loss: 0.043016, mean_absolute_error: 8.485311, mean_q: 9.906321, mean_eps: 0.774518\n",
      "  250882/2000000: episode: 1325, duration: 9.928s, episode steps: 236, steps per second: 24, episode reward: 170.900, mean reward: 0.724 [-1.000, 1.000], mean action: 3.301 [0.000, 6.000], mean observation: 171.826 [24.000, 255.000], loss: 0.046534, mean_absolute_error: 8.519732, mean_q: 9.955503, mean_eps: 0.774312\n",
      "  251093/2000000: episode: 1326, duration: 9.239s, episode steps: 211, steps per second: 23, episode reward: 149.100, mean reward: 0.707 [-1.000, 1.000], mean action: 3.014 [0.000, 6.000], mean observation: 171.500 [24.000, 255.000], loss: 0.046803, mean_absolute_error: 8.579878, mean_q: 10.012956, mean_eps: 0.774111\n",
      "  251342/2000000: episode: 1327, duration: 11.550s, episode steps: 249, steps per second: 22, episode reward: 160.100, mean reward: 0.643 [-1.000, 1.000], mean action: 3.177 [0.000, 6.000], mean observation: 172.677 [23.000, 255.000], loss: 0.044723, mean_absolute_error: 8.385202, mean_q: 9.783412, mean_eps: 0.773904\n",
      "  251497/2000000: episode: 1328, duration: 6.458s, episode steps: 155, steps per second: 24, episode reward: 57.200, mean reward: 0.369 [-1.000, 0.500], mean action: 2.910 [0.000, 6.000], mean observation: 172.804 [23.000, 255.000], loss: 0.046228, mean_absolute_error: 8.248657, mean_q: 9.621837, mean_eps: 0.773722\n",
      "  251739/2000000: episode: 1329, duration: 11.186s, episode steps: 242, steps per second: 22, episode reward: 129.800, mean reward: 0.536 [-1.000, 1.000], mean action: 3.112 [0.000, 6.000], mean observation: 173.421 [24.000, 255.000], loss: 0.047203, mean_absolute_error: 8.521362, mean_q: 9.942754, mean_eps: 0.773544\n",
      "  251963/2000000: episode: 1330, duration: 10.155s, episode steps: 224, steps per second: 22, episode reward: 137.200, mean reward: 0.612 [-1.000, 1.000], mean action: 3.009 [0.000, 6.000], mean observation: 172.137 [23.000, 255.000], loss: 0.047142, mean_absolute_error: 8.549856, mean_q: 9.994952, mean_eps: 0.773335\n",
      "  252195/2000000: episode: 1331, duration: 9.550s, episode steps: 232, steps per second: 24, episode reward: 145.900, mean reward: 0.629 [-1.000, 1.000], mean action: 2.948 [0.000, 6.000], mean observation: 171.165 [24.000, 255.000], loss: 0.049561, mean_absolute_error: 8.478887, mean_q: 9.906717, mean_eps: 0.773130\n",
      "  252453/2000000: episode: 1332, duration: 11.298s, episode steps: 258, steps per second: 23, episode reward: 143.200, mean reward: 0.555 [-1.000, 1.000], mean action: 2.965 [0.000, 6.000], mean observation: 173.324 [23.000, 255.000], loss: 0.049370, mean_absolute_error: 8.608087, mean_q: 10.065415, mean_eps: 0.772908\n",
      "  252703/2000000: episode: 1333, duration: 10.860s, episode steps: 250, steps per second: 23, episode reward: 187.800, mean reward: 0.751 [-1.000, 1.000], mean action: 3.008 [0.000, 6.000], mean observation: 172.736 [23.000, 255.000], loss: 0.045342, mean_absolute_error: 8.430162, mean_q: 9.839763, mean_eps: 0.772680\n",
      "  252922/2000000: episode: 1334, duration: 9.156s, episode steps: 219, steps per second: 24, episode reward: 163.600, mean reward: 0.747 [-1.000, 1.000], mean action: 3.064 [0.000, 6.000], mean observation: 171.351 [24.000, 255.000], loss: 0.041834, mean_absolute_error: 8.541031, mean_q: 9.979590, mean_eps: 0.772469\n",
      "  253138/2000000: episode: 1335, duration: 9.158s, episode steps: 216, steps per second: 24, episode reward: 117.200, mean reward: 0.543 [-1.000, 1.000], mean action: 2.972 [0.000, 6.000], mean observation: 171.670 [24.000, 255.000], loss: 0.043547, mean_absolute_error: 8.473517, mean_q: 9.885243, mean_eps: 0.772273\n",
      "  253430/2000000: episode: 1336, duration: 12.013s, episode steps: 292, steps per second: 24, episode reward: 182.800, mean reward: 0.626 [-1.000, 1.000], mean action: 3.394 [0.000, 6.000], mean observation: 172.696 [24.000, 255.000], loss: 0.041429, mean_absolute_error: 8.637216, mean_q: 10.095896, mean_eps: 0.772044\n",
      "  253680/2000000: episode: 1337, duration: 10.864s, episode steps: 250, steps per second: 23, episode reward: 156.300, mean reward: 0.625 [-1.000, 1.000], mean action: 2.956 [0.000, 6.000], mean observation: 172.376 [23.000, 255.000], loss: 0.045483, mean_absolute_error: 8.510503, mean_q: 9.937485, mean_eps: 0.771801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  253957/2000000: episode: 1338, duration: 11.376s, episode steps: 277, steps per second: 24, episode reward: 134.000, mean reward: 0.484 [-1.000, 1.000], mean action: 3.202 [0.000, 6.000], mean observation: 172.687 [23.000, 255.000], loss: 0.047065, mean_absolute_error: 8.483204, mean_q: 9.895828, mean_eps: 0.771564\n",
      "  254200/2000000: episode: 1339, duration: 10.755s, episode steps: 243, steps per second: 23, episode reward: 119.200, mean reward: 0.491 [-1.000, 1.000], mean action: 3.313 [0.000, 6.000], mean observation: 172.515 [24.000, 255.000], loss: 0.048999, mean_absolute_error: 8.587531, mean_q: 10.025798, mean_eps: 0.771330\n",
      "  254422/2000000: episode: 1340, duration: 9.316s, episode steps: 222, steps per second: 24, episode reward: 130.000, mean reward: 0.586 [-1.000, 1.000], mean action: 3.216 [0.000, 6.000], mean observation: 171.133 [24.000, 255.000], loss: 0.040181, mean_absolute_error: 8.473922, mean_q: 9.883853, mean_eps: 0.771121\n",
      "  254649/2000000: episode: 1341, duration: 9.544s, episode steps: 227, steps per second: 24, episode reward: 155.000, mean reward: 0.683 [-1.000, 1.000], mean action: 3.229 [0.000, 6.000], mean observation: 171.363 [23.000, 255.000], loss: 0.042916, mean_absolute_error: 8.406213, mean_q: 9.799359, mean_eps: 0.770918\n",
      "  254906/2000000: episode: 1342, duration: 10.889s, episode steps: 257, steps per second: 24, episode reward: 153.900, mean reward: 0.599 [-1.000, 1.000], mean action: 3.089 [0.000, 6.000], mean observation: 173.084 [23.000, 255.000], loss: 0.040834, mean_absolute_error: 8.370337, mean_q: 9.764921, mean_eps: 0.770700\n",
      "  255090/2000000: episode: 1343, duration: 7.195s, episode steps: 184, steps per second: 26, episode reward: 68.900, mean reward: 0.374 [-1.000, 0.500], mean action: 2.842 [0.000, 6.000], mean observation: 171.970 [24.000, 255.000], loss: 0.039437, mean_absolute_error: 8.504067, mean_q: 9.924507, mean_eps: 0.770502\n",
      "  255270/2000000: episode: 1344, duration: 6.887s, episode steps: 180, steps per second: 26, episode reward: 62.500, mean reward: 0.347 [-1.000, 0.500], mean action: 2.817 [0.000, 6.000], mean observation: 172.173 [23.000, 255.000], loss: 0.042418, mean_absolute_error: 8.495927, mean_q: 9.908669, mean_eps: 0.770338\n",
      "  255526/2000000: episode: 1345, duration: 10.375s, episode steps: 256, steps per second: 25, episode reward: 157.800, mean reward: 0.616 [-1.000, 1.000], mean action: 3.133 [0.000, 6.000], mean observation: 172.681 [23.000, 255.000], loss: 0.045307, mean_absolute_error: 8.490545, mean_q: 9.922718, mean_eps: 0.770142\n",
      "  255822/2000000: episode: 1346, duration: 11.909s, episode steps: 296, steps per second: 25, episode reward: 205.300, mean reward: 0.694 [-1.000, 1.000], mean action: 3.226 [0.000, 6.000], mean observation: 172.517 [24.000, 255.000], loss: 0.043368, mean_absolute_error: 8.412605, mean_q: 9.818247, mean_eps: 0.769893\n",
      "  256050/2000000: episode: 1347, duration: 8.347s, episode steps: 228, steps per second: 27, episode reward: 86.100, mean reward: 0.378 [-1.000, 0.500], mean action: 3.079 [0.000, 6.000], mean observation: 172.041 [24.000, 255.000], loss: 0.044460, mean_absolute_error: 8.281733, mean_q: 9.680290, mean_eps: 0.769658\n",
      "  256298/2000000: episode: 1348, duration: 9.910s, episode steps: 248, steps per second: 25, episode reward: 157.400, mean reward: 0.635 [-1.000, 1.000], mean action: 2.988 [0.000, 6.000], mean observation: 171.104 [23.000, 255.000], loss: 0.044068, mean_absolute_error: 8.532261, mean_q: 9.955292, mean_eps: 0.769443\n",
      "  256536/2000000: episode: 1349, duration: 9.992s, episode steps: 238, steps per second: 24, episode reward: 148.000, mean reward: 0.622 [-1.000, 1.000], mean action: 2.954 [0.000, 6.000], mean observation: 172.075 [24.000, 255.000], loss: 0.035899, mean_absolute_error: 8.417392, mean_q: 9.832716, mean_eps: 0.769226\n",
      "  256743/2000000: episode: 1350, duration: 9.546s, episode steps: 207, steps per second: 22, episode reward: 146.200, mean reward: 0.706 [-1.000, 1.000], mean action: 2.947 [0.000, 6.000], mean observation: 170.749 [24.000, 255.000], loss: 0.042811, mean_absolute_error: 8.488761, mean_q: 9.914108, mean_eps: 0.769026\n",
      "  256962/2000000: episode: 1351, duration: 9.624s, episode steps: 219, steps per second: 23, episode reward: 156.400, mean reward: 0.714 [-1.000, 1.000], mean action: 3.027 [0.000, 6.000], mean observation: 170.778 [24.000, 255.000], loss: 0.040382, mean_absolute_error: 8.523271, mean_q: 9.948455, mean_eps: 0.768833\n",
      "  257146/2000000: episode: 1352, duration: 7.615s, episode steps: 184, steps per second: 24, episode reward: 68.100, mean reward: 0.370 [-1.000, 0.500], mean action: 2.837 [0.000, 6.000], mean observation: 171.382 [24.000, 255.000], loss: 0.041034, mean_absolute_error: 8.432486, mean_q: 9.850592, mean_eps: 0.768651\n",
      "  257350/2000000: episode: 1353, duration: 8.038s, episode steps: 204, steps per second: 25, episode reward: 70.900, mean reward: 0.348 [-1.000, 0.500], mean action: 2.833 [0.000, 6.000], mean observation: 171.498 [24.000, 255.000], loss: 0.045642, mean_absolute_error: 8.442092, mean_q: 9.856320, mean_eps: 0.768477\n",
      "  257598/2000000: episode: 1354, duration: 10.079s, episode steps: 248, steps per second: 25, episode reward: 164.800, mean reward: 0.665 [-1.000, 1.000], mean action: 3.032 [0.000, 6.000], mean observation: 171.463 [23.000, 255.000], loss: 0.044495, mean_absolute_error: 8.497852, mean_q: 9.917986, mean_eps: 0.768273\n",
      "  257826/2000000: episode: 1355, duration: 8.793s, episode steps: 228, steps per second: 26, episode reward: 146.100, mean reward: 0.641 [-1.000, 1.000], mean action: 2.794 [0.000, 6.000], mean observation: 170.482 [23.000, 255.000], loss: 0.042384, mean_absolute_error: 8.419372, mean_q: 9.825091, mean_eps: 0.768059\n",
      "  258066/2000000: episode: 1356, duration: 9.931s, episode steps: 240, steps per second: 24, episode reward: 176.100, mean reward: 0.734 [-1.000, 1.000], mean action: 3.129 [0.000, 6.000], mean observation: 171.135 [21.000, 255.000], loss: 0.042301, mean_absolute_error: 8.658902, mean_q: 10.109681, mean_eps: 0.767849\n",
      "  258342/2000000: episode: 1357, duration: 11.151s, episode steps: 276, steps per second: 25, episode reward: 129.600, mean reward: 0.470 [-1.000, 1.000], mean action: 3.145 [0.000, 6.000], mean observation: 172.336 [24.000, 255.000], loss: 0.042002, mean_absolute_error: 8.389061, mean_q: 9.789766, mean_eps: 0.767616\n",
      "  258526/2000000: episode: 1358, duration: 8.275s, episode steps: 184, steps per second: 22, episode reward: 92.800, mean reward: 0.504 [-1.000, 1.000], mean action: 3.408 [0.000, 6.000], mean observation: 172.774 [23.000, 255.000], loss: 0.047381, mean_absolute_error: 8.747217, mean_q: 10.217738, mean_eps: 0.767409\n",
      "  258767/2000000: episode: 1359, duration: 10.465s, episode steps: 241, steps per second: 23, episode reward: 175.600, mean reward: 0.729 [-1.000, 1.000], mean action: 3.195 [0.000, 6.000], mean observation: 171.933 [24.000, 255.000], loss: 0.043522, mean_absolute_error: 8.519166, mean_q: 9.962898, mean_eps: 0.767219\n",
      "  259002/2000000: episode: 1360, duration: 9.729s, episode steps: 235, steps per second: 24, episode reward: 175.500, mean reward: 0.747 [-1.000, 1.000], mean action: 2.932 [0.000, 6.000], mean observation: 171.093 [24.000, 255.000], loss: 0.045223, mean_absolute_error: 8.352351, mean_q: 9.747991, mean_eps: 0.767004\n",
      "  259242/2000000: episode: 1361, duration: 9.977s, episode steps: 240, steps per second: 24, episode reward: 162.000, mean reward: 0.675 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 171.106 [23.000, 255.000], loss: 0.053785, mean_absolute_error: 8.599971, mean_q: 10.056646, mean_eps: 0.766790\n",
      "  259426/2000000: episode: 1362, duration: 6.994s, episode steps: 184, steps per second: 26, episode reward: 65.700, mean reward: 0.357 [-1.000, 0.500], mean action: 2.870 [0.000, 6.000], mean observation: 172.089 [24.000, 255.000], loss: 0.044171, mean_absolute_error: 8.647914, mean_q: 10.098820, mean_eps: 0.766599\n",
      "  259651/2000000: episode: 1363, duration: 8.925s, episode steps: 225, steps per second: 25, episode reward: 149.200, mean reward: 0.663 [-1.000, 1.000], mean action: 3.027 [0.000, 6.000], mean observation: 171.308 [24.000, 255.000], loss: 0.048143, mean_absolute_error: 8.480603, mean_q: 9.882726, mean_eps: 0.766416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  259852/2000000: episode: 1364, duration: 7.803s, episode steps: 201, steps per second: 26, episode reward: 73.000, mean reward: 0.363 [-1.000, 0.500], mean action: 2.980 [0.000, 6.000], mean observation: 171.655 [23.000, 255.000], loss: 0.053805, mean_absolute_error: 8.717806, mean_q: 10.177930, mean_eps: 0.766225\n",
      "  260134/2000000: episode: 1365, duration: 12.097s, episode steps: 282, steps per second: 23, episode reward: 186.000, mean reward: 0.660 [-1.000, 1.000], mean action: 3.358 [0.000, 6.000], mean observation: 172.624 [24.000, 255.000], loss: 0.061823, mean_absolute_error: 8.753381, mean_q: 10.215203, mean_eps: 0.766007\n",
      "  260403/2000000: episode: 1366, duration: 11.174s, episode steps: 269, steps per second: 24, episode reward: 175.700, mean reward: 0.653 [-1.000, 1.000], mean action: 3.156 [0.000, 6.000], mean observation: 172.333 [23.000, 255.000], loss: 0.048843, mean_absolute_error: 8.925636, mean_q: 10.424483, mean_eps: 0.765759\n",
      "  260663/2000000: episode: 1367, duration: 10.584s, episode steps: 260, steps per second: 25, episode reward: 154.000, mean reward: 0.592 [-1.000, 1.000], mean action: 2.912 [0.000, 6.000], mean observation: 172.717 [24.000, 255.000], loss: 0.048399, mean_absolute_error: 8.894951, mean_q: 10.384109, mean_eps: 0.765521\n",
      "  260914/2000000: episode: 1368, duration: 10.439s, episode steps: 251, steps per second: 24, episode reward: 143.500, mean reward: 0.572 [-1.000, 1.000], mean action: 2.948 [0.000, 6.000], mean observation: 172.009 [24.000, 255.000], loss: 0.042045, mean_absolute_error: 8.894284, mean_q: 10.397746, mean_eps: 0.765291\n",
      "  261154/2000000: episode: 1369, duration: 9.947s, episode steps: 240, steps per second: 24, episode reward: 171.500, mean reward: 0.715 [-1.000, 1.000], mean action: 3.017 [0.000, 6.000], mean observation: 171.458 [24.000, 255.000], loss: 0.047723, mean_absolute_error: 8.988318, mean_q: 10.484685, mean_eps: 0.765069\n",
      "  261374/2000000: episode: 1370, duration: 8.703s, episode steps: 220, steps per second: 25, episode reward: 152.600, mean reward: 0.694 [-1.000, 1.000], mean action: 2.695 [0.000, 6.000], mean observation: 170.634 [23.000, 255.000], loss: 0.055651, mean_absolute_error: 9.060222, mean_q: 10.574599, mean_eps: 0.764862\n",
      "  261643/2000000: episode: 1371, duration: 11.504s, episode steps: 269, steps per second: 23, episode reward: 148.800, mean reward: 0.553 [-1.000, 1.000], mean action: 3.294 [0.000, 6.000], mean observation: 171.800 [24.000, 255.000], loss: 0.049400, mean_absolute_error: 8.900344, mean_q: 10.415460, mean_eps: 0.764643\n",
      "  261870/2000000: episode: 1372, duration: 9.135s, episode steps: 227, steps per second: 25, episode reward: 159.100, mean reward: 0.701 [-1.000, 1.000], mean action: 2.824 [0.000, 6.000], mean observation: 171.037 [24.000, 255.000], loss: 0.051679, mean_absolute_error: 8.802919, mean_q: 10.272055, mean_eps: 0.764420\n",
      "  262093/2000000: episode: 1373, duration: 8.915s, episode steps: 223, steps per second: 25, episode reward: 143.500, mean reward: 0.643 [-1.000, 1.000], mean action: 3.152 [0.000, 6.000], mean observation: 170.928 [24.000, 255.000], loss: 0.045848, mean_absolute_error: 8.845295, mean_q: 10.327598, mean_eps: 0.764216\n",
      "  262327/2000000: episode: 1374, duration: 11.016s, episode steps: 234, steps per second: 21, episode reward: 160.600, mean reward: 0.686 [-1.000, 1.000], mean action: 3.274 [0.000, 6.000], mean observation: 172.072 [24.000, 255.000], loss: 0.047209, mean_absolute_error: 9.070908, mean_q: 10.587587, mean_eps: 0.764011\n",
      "  262547/2000000: episode: 1375, duration: 8.823s, episode steps: 220, steps per second: 25, episode reward: 150.500, mean reward: 0.684 [-1.000, 1.000], mean action: 3.018 [0.000, 6.000], mean observation: 171.156 [24.000, 255.000], loss: 0.053321, mean_absolute_error: 9.264944, mean_q: 10.821341, mean_eps: 0.763808\n",
      "  262755/2000000: episode: 1376, duration: 8.869s, episode steps: 208, steps per second: 23, episode reward: 125.700, mean reward: 0.604 [-1.000, 1.000], mean action: 3.125 [0.000, 6.000], mean observation: 171.421 [25.000, 255.000], loss: 0.053369, mean_absolute_error: 8.966618, mean_q: 10.472380, mean_eps: 0.763615\n",
      "  262998/2000000: episode: 1377, duration: 10.663s, episode steps: 243, steps per second: 23, episode reward: 131.400, mean reward: 0.541 [-1.000, 1.000], mean action: 2.959 [0.000, 6.000], mean observation: 172.323 [24.000, 255.000], loss: 0.048431, mean_absolute_error: 8.996565, mean_q: 10.508303, mean_eps: 0.763412\n",
      "  263158/2000000: episode: 1378, duration: 6.524s, episode steps: 160, steps per second: 25, episode reward: 57.700, mean reward: 0.361 [-1.000, 0.500], mean action: 2.875 [0.000, 6.000], mean observation: 172.730 [24.000, 255.000], loss: 0.047608, mean_absolute_error: 8.951998, mean_q: 10.472934, mean_eps: 0.763230\n",
      "  263374/2000000: episode: 1379, duration: 9.278s, episode steps: 216, steps per second: 23, episode reward: 151.500, mean reward: 0.701 [-1.000, 1.000], mean action: 2.880 [0.000, 6.000], mean observation: 171.267 [24.000, 255.000], loss: 0.049543, mean_absolute_error: 9.010462, mean_q: 10.534033, mean_eps: 0.763061\n",
      "  263642/2000000: episode: 1380, duration: 11.972s, episode steps: 268, steps per second: 22, episode reward: 127.200, mean reward: 0.475 [-1.000, 1.000], mean action: 3.325 [0.000, 6.000], mean observation: 173.559 [23.000, 255.000], loss: 0.045942, mean_absolute_error: 9.146701, mean_q: 10.684688, mean_eps: 0.762843\n",
      "  263886/2000000: episode: 1381, duration: 10.499s, episode steps: 244, steps per second: 23, episode reward: 149.900, mean reward: 0.614 [-1.000, 1.000], mean action: 3.246 [0.000, 6.000], mean observation: 172.358 [23.000, 255.000], loss: 0.044705, mean_absolute_error: 8.835739, mean_q: 10.315756, mean_eps: 0.762612\n",
      "  264090/2000000: episode: 1382, duration: 8.560s, episode steps: 204, steps per second: 24, episode reward: 77.300, mean reward: 0.379 [-1.000, 0.500], mean action: 2.961 [0.000, 6.000], mean observation: 172.871 [23.000, 255.000], loss: 0.045325, mean_absolute_error: 9.119985, mean_q: 10.670879, mean_eps: 0.762411\n",
      "  264347/2000000: episode: 1383, duration: 10.631s, episode steps: 257, steps per second: 24, episode reward: 140.200, mean reward: 0.546 [-1.000, 1.000], mean action: 3.117 [0.000, 6.000], mean observation: 173.853 [22.000, 255.000], loss: 0.051948, mean_absolute_error: 8.827404, mean_q: 10.302045, mean_eps: 0.762204\n",
      "  264614/2000000: episode: 1384, duration: 10.654s, episode steps: 267, steps per second: 25, episode reward: 153.800, mean reward: 0.576 [-1.000, 1.000], mean action: 2.951 [0.000, 6.000], mean observation: 173.049 [23.000, 255.000], loss: 0.048651, mean_absolute_error: 8.954370, mean_q: 10.460684, mean_eps: 0.761968\n",
      "  264874/2000000: episode: 1385, duration: 10.531s, episode steps: 260, steps per second: 25, episode reward: 171.600, mean reward: 0.660 [-1.000, 1.000], mean action: 3.181 [0.000, 6.000], mean observation: 173.094 [23.000, 255.000], loss: 0.043176, mean_absolute_error: 8.975329, mean_q: 10.488637, mean_eps: 0.761730\n",
      "  265120/2000000: episode: 1386, duration: 9.997s, episode steps: 246, steps per second: 25, episode reward: 189.000, mean reward: 0.768 [-1.000, 1.000], mean action: 3.297 [0.000, 6.000], mean observation: 172.165 [24.000, 255.000], loss: 0.050649, mean_absolute_error: 9.040715, mean_q: 10.559263, mean_eps: 0.761504\n",
      "  265339/2000000: episode: 1387, duration: 9.192s, episode steps: 219, steps per second: 24, episode reward: 70.400, mean reward: 0.321 [-1.000, 0.500], mean action: 3.329 [0.000, 6.000], mean observation: 173.140 [24.000, 255.000], loss: 0.049056, mean_absolute_error: 9.097850, mean_q: 10.622254, mean_eps: 0.761295\n",
      "  265590/2000000: episode: 1388, duration: 10.541s, episode steps: 251, steps per second: 24, episode reward: 156.300, mean reward: 0.623 [-1.000, 1.000], mean action: 2.932 [0.000, 6.000], mean observation: 172.805 [24.000, 255.000], loss: 0.047117, mean_absolute_error: 8.904391, mean_q: 10.377396, mean_eps: 0.761082\n",
      "  265833/2000000: episode: 1389, duration: 9.546s, episode steps: 243, steps per second: 25, episode reward: 62.000, mean reward: 0.255 [-1.000, 0.500], mean action: 3.193 [0.000, 6.000], mean observation: 173.725 [23.000, 255.000], loss: 0.046227, mean_absolute_error: 8.996754, mean_q: 10.498188, mean_eps: 0.760859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  266040/2000000: episode: 1390, duration: 8.192s, episode steps: 207, steps per second: 25, episode reward: 111.500, mean reward: 0.539 [-1.000, 1.000], mean action: 2.792 [0.000, 6.000], mean observation: 171.852 [24.000, 255.000], loss: 0.050897, mean_absolute_error: 9.150181, mean_q: 10.680137, mean_eps: 0.760658\n",
      "  266270/2000000: episode: 1391, duration: 9.747s, episode steps: 230, steps per second: 24, episode reward: 169.900, mean reward: 0.739 [-1.000, 1.000], mean action: 3.026 [0.000, 6.000], mean observation: 172.082 [24.000, 255.000], loss: 0.046930, mean_absolute_error: 9.122043, mean_q: 10.643674, mean_eps: 0.760461\n",
      "  266526/2000000: episode: 1392, duration: 10.528s, episode steps: 256, steps per second: 24, episode reward: 174.300, mean reward: 0.681 [-1.000, 1.000], mean action: 3.012 [0.000, 6.000], mean observation: 172.938 [24.000, 255.000], loss: 0.045353, mean_absolute_error: 9.016875, mean_q: 10.524752, mean_eps: 0.760242\n",
      "  266756/2000000: episode: 1393, duration: 8.923s, episode steps: 230, steps per second: 26, episode reward: 63.100, mean reward: 0.274 [-1.000, 0.500], mean action: 3.274 [0.000, 6.000], mean observation: 173.381 [24.000, 255.000], loss: 0.047403, mean_absolute_error: 8.864470, mean_q: 10.352639, mean_eps: 0.760024\n",
      "  267030/2000000: episode: 1394, duration: 11.264s, episode steps: 274, steps per second: 24, episode reward: 185.500, mean reward: 0.677 [-1.000, 1.000], mean action: 3.318 [0.000, 6.000], mean observation: 172.706 [23.000, 255.000], loss: 0.049734, mean_absolute_error: 8.993045, mean_q: 10.511614, mean_eps: 0.759797\n",
      "  267278/2000000: episode: 1395, duration: 10.155s, episode steps: 248, steps per second: 24, episode reward: 170.900, mean reward: 0.689 [-1.000, 1.000], mean action: 3.121 [0.000, 6.000], mean observation: 172.370 [24.000, 255.000], loss: 0.045553, mean_absolute_error: 9.132445, mean_q: 10.676956, mean_eps: 0.759561\n",
      "  267519/2000000: episode: 1396, duration: 9.451s, episode steps: 241, steps per second: 25, episode reward: 165.700, mean reward: 0.688 [-1.000, 1.000], mean action: 3.154 [0.000, 6.000], mean observation: 171.508 [24.000, 255.000], loss: 0.040361, mean_absolute_error: 8.843481, mean_q: 10.319189, mean_eps: 0.759342\n",
      "  267748/2000000: episode: 1397, duration: 8.961s, episode steps: 229, steps per second: 26, episode reward: 165.100, mean reward: 0.721 [-1.000, 1.000], mean action: 2.904 [0.000, 6.000], mean observation: 171.926 [23.000, 255.000], loss: 0.038031, mean_absolute_error: 8.888734, mean_q: 10.372884, mean_eps: 0.759131\n",
      "  267957/2000000: episode: 1398, duration: 8.399s, episode steps: 209, steps per second: 25, episode reward: 136.700, mean reward: 0.654 [-1.000, 1.000], mean action: 2.751 [0.000, 6.000], mean observation: 171.608 [24.000, 255.000], loss: 0.045070, mean_absolute_error: 8.874005, mean_q: 10.343863, mean_eps: 0.758933\n",
      "  268200/2000000: episode: 1399, duration: 10.465s, episode steps: 243, steps per second: 23, episode reward: 180.000, mean reward: 0.741 [-1.000, 1.000], mean action: 3.255 [0.000, 6.000], mean observation: 173.164 [23.000, 255.000], loss: 0.044526, mean_absolute_error: 9.013985, mean_q: 10.516842, mean_eps: 0.758730\n",
      "  268450/2000000: episode: 1400, duration: 10.097s, episode steps: 250, steps per second: 25, episode reward: 154.200, mean reward: 0.617 [-1.000, 1.000], mean action: 2.928 [0.000, 6.000], mean observation: 173.423 [23.000, 255.000], loss: 0.044159, mean_absolute_error: 8.862664, mean_q: 10.352866, mean_eps: 0.758508\n",
      "  268688/2000000: episode: 1401, duration: 9.844s, episode steps: 238, steps per second: 24, episode reward: 73.500, mean reward: 0.309 [-1.000, 0.500], mean action: 3.239 [0.000, 6.000], mean observation: 173.105 [23.000, 255.000], loss: 0.048430, mean_absolute_error: 9.058324, mean_q: 10.570711, mean_eps: 0.758289\n",
      "  268911/2000000: episode: 1402, duration: 9.180s, episode steps: 223, steps per second: 24, episode reward: 145.800, mean reward: 0.654 [-1.000, 1.000], mean action: 2.848 [0.000, 6.000], mean observation: 171.286 [24.000, 255.000], loss: 0.045763, mean_absolute_error: 8.963312, mean_q: 10.477887, mean_eps: 0.758082\n",
      "  269164/2000000: episode: 1403, duration: 10.744s, episode steps: 253, steps per second: 24, episode reward: 146.600, mean reward: 0.579 [-1.000, 1.000], mean action: 3.150 [0.000, 6.000], mean observation: 172.933 [23.000, 255.000], loss: 0.044516, mean_absolute_error: 9.051744, mean_q: 10.578503, mean_eps: 0.757868\n",
      "  269396/2000000: episode: 1404, duration: 9.542s, episode steps: 232, steps per second: 24, episode reward: 167.800, mean reward: 0.723 [-1.000, 1.000], mean action: 3.043 [0.000, 6.000], mean observation: 171.525 [24.000, 255.000], loss: 0.047065, mean_absolute_error: 8.993438, mean_q: 10.504777, mean_eps: 0.757650\n",
      "  269639/2000000: episode: 1405, duration: 10.350s, episode steps: 243, steps per second: 23, episode reward: 148.100, mean reward: 0.609 [-1.000, 1.000], mean action: 2.905 [0.000, 6.000], mean observation: 173.472 [24.000, 255.000], loss: 0.043163, mean_absolute_error: 8.887321, mean_q: 10.371272, mean_eps: 0.757436\n",
      "  269888/2000000: episode: 1406, duration: 9.553s, episode steps: 249, steps per second: 26, episode reward: 112.600, mean reward: 0.452 [-1.000, 0.500], mean action: 2.912 [0.000, 6.000], mean observation: 172.715 [24.000, 255.000], loss: 0.042431, mean_absolute_error: 8.907439, mean_q: 10.400066, mean_eps: 0.757214\n",
      "  270132/2000000: episode: 1407, duration: 9.910s, episode steps: 244, steps per second: 25, episode reward: 181.300, mean reward: 0.743 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 171.959 [24.000, 255.000], loss: 0.057967, mean_absolute_error: 9.335483, mean_q: 10.894987, mean_eps: 0.756993\n",
      "  270379/2000000: episode: 1408, duration: 9.906s, episode steps: 247, steps per second: 25, episode reward: 151.600, mean reward: 0.614 [-1.000, 1.000], mean action: 2.862 [0.000, 6.000], mean observation: 171.625 [22.000, 255.000], loss: 0.048194, mean_absolute_error: 9.393045, mean_q: 10.962098, mean_eps: 0.756771\n",
      "  270642/2000000: episode: 1409, duration: 10.646s, episode steps: 263, steps per second: 25, episode reward: 147.500, mean reward: 0.561 [-1.000, 1.000], mean action: 3.118 [0.000, 6.000], mean observation: 172.641 [24.000, 255.000], loss: 0.047766, mean_absolute_error: 9.652539, mean_q: 11.272869, mean_eps: 0.756541\n",
      "  270909/2000000: episode: 1410, duration: 11.317s, episode steps: 267, steps per second: 24, episode reward: 149.500, mean reward: 0.560 [-1.000, 1.000], mean action: 3.075 [0.000, 6.000], mean observation: 173.055 [23.000, 255.000], loss: 0.050251, mean_absolute_error: 9.504177, mean_q: 11.090183, mean_eps: 0.756302\n",
      "  271150/2000000: episode: 1411, duration: 9.469s, episode steps: 241, steps per second: 25, episode reward: 142.700, mean reward: 0.592 [-1.000, 1.000], mean action: 3.012 [0.000, 6.000], mean observation: 171.707 [23.000, 255.000], loss: 0.046076, mean_absolute_error: 9.511393, mean_q: 11.113085, mean_eps: 0.756073\n",
      "  271398/2000000: episode: 1412, duration: 10.760s, episode steps: 248, steps per second: 23, episode reward: 142.800, mean reward: 0.576 [-1.000, 1.000], mean action: 3.125 [0.000, 6.000], mean observation: 172.680 [24.000, 255.000], loss: 0.049965, mean_absolute_error: 9.349121, mean_q: 10.898378, mean_eps: 0.755853\n",
      "  271650/2000000: episode: 1413, duration: 10.884s, episode steps: 252, steps per second: 23, episode reward: 162.700, mean reward: 0.646 [-1.000, 1.000], mean action: 3.083 [0.000, 6.000], mean observation: 172.317 [23.000, 255.000], loss: 0.048314, mean_absolute_error: 9.467434, mean_q: 11.054708, mean_eps: 0.755628\n",
      "  271880/2000000: episode: 1414, duration: 9.417s, episode steps: 230, steps per second: 24, episode reward: 150.800, mean reward: 0.656 [-1.000, 1.000], mean action: 3.009 [0.000, 6.000], mean observation: 171.376 [24.000, 255.000], loss: 0.048363, mean_absolute_error: 9.669495, mean_q: 11.304454, mean_eps: 0.755412\n",
      "  272099/2000000: episode: 1415, duration: 8.798s, episode steps: 219, steps per second: 25, episode reward: 102.600, mean reward: 0.468 [-1.000, 1.000], mean action: 2.890 [0.000, 6.000], mean observation: 171.267 [24.000, 255.000], loss: 0.060492, mean_absolute_error: 9.465709, mean_q: 11.036004, mean_eps: 0.755211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  272330/2000000: episode: 1416, duration: 9.292s, episode steps: 231, steps per second: 25, episode reward: 150.200, mean reward: 0.650 [-1.000, 1.000], mean action: 3.035 [0.000, 6.000], mean observation: 170.771 [24.000, 255.000], loss: 0.045493, mean_absolute_error: 9.567300, mean_q: 11.180370, mean_eps: 0.755007\n",
      "  272587/2000000: episode: 1417, duration: 11.100s, episode steps: 257, steps per second: 23, episode reward: 163.500, mean reward: 0.636 [-1.000, 1.000], mean action: 3.195 [0.000, 6.000], mean observation: 172.732 [24.000, 255.000], loss: 0.048720, mean_absolute_error: 9.443271, mean_q: 11.027702, mean_eps: 0.754788\n",
      "  272810/2000000: episode: 1418, duration: 9.416s, episode steps: 223, steps per second: 24, episode reward: 152.400, mean reward: 0.683 [-1.000, 1.000], mean action: 3.036 [0.000, 6.000], mean observation: 171.298 [23.000, 255.000], loss: 0.050465, mean_absolute_error: 9.609512, mean_q: 11.210923, mean_eps: 0.754572\n",
      "  273089/2000000: episode: 1419, duration: 12.110s, episode steps: 279, steps per second: 23, episode reward: 188.700, mean reward: 0.676 [-1.000, 1.000], mean action: 3.362 [0.000, 6.000], mean observation: 172.462 [21.000, 255.000], loss: 0.054209, mean_absolute_error: 9.506088, mean_q: 11.107700, mean_eps: 0.754345\n",
      "  273323/2000000: episode: 1420, duration: 9.869s, episode steps: 234, steps per second: 24, episode reward: 158.200, mean reward: 0.676 [-1.000, 1.000], mean action: 3.209 [0.000, 6.000], mean observation: 171.371 [24.000, 255.000], loss: 0.053819, mean_absolute_error: 9.863924, mean_q: 11.520648, mean_eps: 0.754115\n",
      "  273542/2000000: episode: 1421, duration: 9.459s, episode steps: 219, steps per second: 23, episode reward: 156.000, mean reward: 0.712 [-1.000, 1.000], mean action: 3.087 [0.000, 6.000], mean observation: 171.590 [24.000, 255.000], loss: 0.059467, mean_absolute_error: 9.519070, mean_q: 11.119672, mean_eps: 0.753911\n",
      "  273766/2000000: episode: 1422, duration: 9.440s, episode steps: 224, steps per second: 24, episode reward: 162.800, mean reward: 0.727 [-1.000, 1.000], mean action: 3.040 [0.000, 6.000], mean observation: 171.250 [23.000, 255.000], loss: 0.056288, mean_absolute_error: 9.745750, mean_q: 11.388898, mean_eps: 0.753711\n",
      "  273990/2000000: episode: 1423, duration: 8.804s, episode steps: 224, steps per second: 25, episode reward: 147.900, mean reward: 0.660 [-1.000, 1.000], mean action: 3.009 [0.000, 6.000], mean observation: 171.320 [23.000, 255.000], loss: 0.052717, mean_absolute_error: 9.517683, mean_q: 11.118537, mean_eps: 0.753510\n",
      "  274214/2000000: episode: 1424, duration: 9.094s, episode steps: 224, steps per second: 25, episode reward: 131.700, mean reward: 0.588 [-1.000, 1.000], mean action: 3.196 [0.000, 6.000], mean observation: 171.625 [24.000, 255.000], loss: 0.050694, mean_absolute_error: 9.654697, mean_q: 11.268077, mean_eps: 0.753308\n",
      "  274452/2000000: episode: 1425, duration: 10.011s, episode steps: 238, steps per second: 24, episode reward: 172.100, mean reward: 0.723 [-1.000, 1.000], mean action: 3.118 [0.000, 6.000], mean observation: 171.771 [23.000, 255.000], loss: 0.053270, mean_absolute_error: 9.557936, mean_q: 11.158567, mean_eps: 0.753101\n",
      "  274726/2000000: episode: 1426, duration: 11.133s, episode steps: 274, steps per second: 25, episode reward: 215.800, mean reward: 0.788 [-1.000, 1.000], mean action: 3.303 [0.000, 6.000], mean observation: 172.203 [24.000, 255.000], loss: 0.050219, mean_absolute_error: 9.651255, mean_q: 11.275349, mean_eps: 0.752871\n",
      "  275026/2000000: episode: 1427, duration: 12.095s, episode steps: 300, steps per second: 25, episode reward: 213.800, mean reward: 0.713 [-1.000, 1.000], mean action: 3.347 [0.000, 6.000], mean observation: 172.505 [24.000, 255.000], loss: 0.048403, mean_absolute_error: 9.610061, mean_q: 11.234578, mean_eps: 0.752612\n",
      "  275242/2000000: episode: 1428, duration: 8.203s, episode steps: 216, steps per second: 26, episode reward: 78.900, mean reward: 0.365 [-1.000, 0.500], mean action: 2.843 [0.000, 6.000], mean observation: 172.436 [24.000, 255.000], loss: 0.049594, mean_absolute_error: 9.702618, mean_q: 11.335844, mean_eps: 0.752379\n",
      "  275328/2000000: episode: 1429, duration: 6.215s, episode steps: 86, steps per second: 14, episode reward: 31.100, mean reward: 0.362 [-1.000, 0.500], mean action: 2.140 [0.000, 6.000], mean observation: 173.270 [24.000, 255.000], loss: 0.054542, mean_absolute_error: 9.724962, mean_q: 11.338834, mean_eps: 0.752244\n",
      "  275438/2000000: episode: 1430, duration: 8.802s, episode steps: 110, steps per second: 12, episode reward: 60.900, mean reward: 0.554 [-1.000, 1.000], mean action: 3.691 [0.000, 6.000], mean observation: 173.769 [24.000, 255.000], loss: 0.049388, mean_absolute_error: 9.740759, mean_q: 11.395358, mean_eps: 0.752156\n",
      "  275646/2000000: episode: 1431, duration: 11.506s, episode steps: 208, steps per second: 18, episode reward: 148.600, mean reward: 0.714 [-1.000, 1.000], mean action: 3.139 [0.000, 6.000], mean observation: 171.938 [24.000, 255.000], loss: 0.055973, mean_absolute_error: 9.694157, mean_q: 11.315227, mean_eps: 0.752012\n",
      "  275888/2000000: episode: 1432, duration: 9.567s, episode steps: 242, steps per second: 25, episode reward: 158.200, mean reward: 0.654 [-1.000, 1.000], mean action: 3.099 [0.000, 6.000], mean observation: 171.949 [25.000, 255.000], loss: 0.047984, mean_absolute_error: 9.764996, mean_q: 11.418735, mean_eps: 0.751811\n",
      "  276115/2000000: episode: 1433, duration: 8.263s, episode steps: 227, steps per second: 27, episode reward: 88.800, mean reward: 0.391 [-1.000, 0.500], mean action: 2.921 [0.000, 6.000], mean observation: 172.901 [23.000, 255.000], loss: 0.048826, mean_absolute_error: 9.890679, mean_q: 11.553615, mean_eps: 0.751600\n",
      "  276367/2000000: episode: 1434, duration: 9.712s, episode steps: 252, steps per second: 26, episode reward: 175.100, mean reward: 0.695 [-1.000, 1.000], mean action: 3.127 [0.000, 6.000], mean observation: 171.748 [24.000, 255.000], loss: 0.057361, mean_absolute_error: 9.604725, mean_q: 11.208407, mean_eps: 0.751384\n",
      "  276600/2000000: episode: 1435, duration: 8.807s, episode steps: 233, steps per second: 26, episode reward: 154.400, mean reward: 0.663 [-1.000, 1.000], mean action: 2.996 [0.000, 6.000], mean observation: 171.608 [23.000, 255.000], loss: 0.047588, mean_absolute_error: 9.561151, mean_q: 11.150384, mean_eps: 0.751166\n",
      "  276892/2000000: episode: 1436, duration: 11.940s, episode steps: 292, steps per second: 24, episode reward: 198.300, mean reward: 0.679 [-1.000, 1.000], mean action: 3.288 [0.000, 6.000], mean observation: 173.440 [23.000, 255.000], loss: 0.049506, mean_absolute_error: 9.818869, mean_q: 11.472716, mean_eps: 0.750930\n",
      "  277166/2000000: episode: 1437, duration: 10.641s, episode steps: 274, steps per second: 26, episode reward: 183.900, mean reward: 0.671 [-1.000, 1.000], mean action: 3.146 [0.000, 6.000], mean observation: 173.094 [24.000, 255.000], loss: 0.052884, mean_absolute_error: 9.668414, mean_q: 11.284565, mean_eps: 0.750675\n",
      "  277426/2000000: episode: 1438, duration: 10.016s, episode steps: 260, steps per second: 26, episode reward: 180.000, mean reward: 0.692 [-1.000, 1.000], mean action: 3.127 [0.000, 6.000], mean observation: 172.601 [23.000, 255.000], loss: 0.047639, mean_absolute_error: 9.808631, mean_q: 11.459028, mean_eps: 0.750434\n",
      "  277609/2000000: episode: 1439, duration: 6.697s, episode steps: 183, steps per second: 27, episode reward: 58.800, mean reward: 0.321 [-1.000, 0.500], mean action: 2.798 [0.000, 6.000], mean observation: 173.474 [24.000, 255.000], loss: 0.050560, mean_absolute_error: 9.605665, mean_q: 11.227067, mean_eps: 0.750234\n",
      "  277879/2000000: episode: 1440, duration: 11.181s, episode steps: 270, steps per second: 24, episode reward: 167.100, mean reward: 0.619 [-1.000, 1.000], mean action: 3.296 [0.000, 6.000], mean observation: 173.076 [24.000, 255.000], loss: 0.048790, mean_absolute_error: 9.587720, mean_q: 11.199531, mean_eps: 0.750030\n",
      "  278115/2000000: episode: 1441, duration: 9.449s, episode steps: 236, steps per second: 25, episode reward: 179.800, mean reward: 0.762 [-1.000, 1.000], mean action: 3.237 [0.000, 6.000], mean observation: 171.926 [24.000, 255.000], loss: 0.044085, mean_absolute_error: 9.701450, mean_q: 11.321636, mean_eps: 0.749804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  278405/2000000: episode: 1442, duration: 11.649s, episode steps: 290, steps per second: 25, episode reward: 149.600, mean reward: 0.516 [-1.000, 1.000], mean action: 3.317 [0.000, 6.000], mean observation: 173.444 [24.000, 255.000], loss: 0.047524, mean_absolute_error: 9.632207, mean_q: 11.248237, mean_eps: 0.749566\n",
      "  278640/2000000: episode: 1443, duration: 9.384s, episode steps: 235, steps per second: 25, episode reward: 163.500, mean reward: 0.696 [-1.000, 1.000], mean action: 3.077 [0.000, 6.000], mean observation: 171.690 [24.000, 255.000], loss: 0.051694, mean_absolute_error: 9.864854, mean_q: 11.515477, mean_eps: 0.749330\n",
      "  278891/2000000: episode: 1444, duration: 10.028s, episode steps: 251, steps per second: 25, episode reward: 154.300, mean reward: 0.615 [-1.000, 1.000], mean action: 2.980 [0.000, 6.000], mean observation: 172.253 [24.000, 255.000], loss: 0.055295, mean_absolute_error: 9.790292, mean_q: 11.430258, mean_eps: 0.749112\n",
      "  279153/2000000: episode: 1445, duration: 10.563s, episode steps: 262, steps per second: 25, episode reward: 183.000, mean reward: 0.698 [-1.000, 1.000], mean action: 3.206 [0.000, 6.000], mean observation: 173.021 [23.000, 255.000], loss: 0.056594, mean_absolute_error: 9.802440, mean_q: 11.442867, mean_eps: 0.748880\n",
      "  279402/2000000: episode: 1446, duration: 9.751s, episode steps: 249, steps per second: 26, episode reward: 174.000, mean reward: 0.699 [-1.000, 1.000], mean action: 2.932 [0.000, 6.000], mean observation: 172.132 [23.000, 255.000], loss: 0.043625, mean_absolute_error: 9.678708, mean_q: 11.301074, mean_eps: 0.748650\n",
      "  279653/2000000: episode: 1447, duration: 9.872s, episode steps: 251, steps per second: 25, episode reward: 163.400, mean reward: 0.651 [-1.000, 1.000], mean action: 3.076 [0.000, 6.000], mean observation: 172.216 [22.000, 255.000], loss: 0.051367, mean_absolute_error: 9.774533, mean_q: 11.421591, mean_eps: 0.748425\n",
      "  279892/2000000: episode: 1448, duration: 9.214s, episode steps: 239, steps per second: 26, episode reward: 91.600, mean reward: 0.383 [-1.000, 0.500], mean action: 3.071 [0.000, 6.000], mean observation: 172.615 [24.000, 255.000], loss: 0.048375, mean_absolute_error: 9.936497, mean_q: 11.598799, mean_eps: 0.748205\n",
      "  280146/2000000: episode: 1449, duration: 10.107s, episode steps: 254, steps per second: 25, episode reward: 153.600, mean reward: 0.605 [-1.000, 1.000], mean action: 3.201 [0.000, 6.000], mean observation: 173.075 [23.000, 255.000], loss: 0.063320, mean_absolute_error: 9.953663, mean_q: 11.632205, mean_eps: 0.747984\n",
      "  280422/2000000: episode: 1450, duration: 11.088s, episode steps: 276, steps per second: 25, episode reward: 211.600, mean reward: 0.767 [-1.000, 1.000], mean action: 3.174 [0.000, 6.000], mean observation: 173.030 [23.000, 255.000], loss: 0.051761, mean_absolute_error: 10.017982, mean_q: 11.693873, mean_eps: 0.747744\n",
      "  280683/2000000: episode: 1451, duration: 10.481s, episode steps: 261, steps per second: 25, episode reward: 185.000, mean reward: 0.709 [-1.000, 1.000], mean action: 3.169 [0.000, 6.000], mean observation: 172.908 [23.000, 255.000], loss: 0.050155, mean_absolute_error: 10.192413, mean_q: 11.895467, mean_eps: 0.747503\n",
      "  280934/2000000: episode: 1452, duration: 9.798s, episode steps: 251, steps per second: 26, episode reward: 157.600, mean reward: 0.628 [-1.000, 1.000], mean action: 3.040 [0.000, 6.000], mean observation: 172.680 [24.000, 255.000], loss: 0.050330, mean_absolute_error: 10.287089, mean_q: 12.021328, mean_eps: 0.747273\n",
      "  281202/2000000: episode: 1453, duration: 10.508s, episode steps: 268, steps per second: 26, episode reward: 181.600, mean reward: 0.678 [-1.000, 1.000], mean action: 3.067 [0.000, 6.000], mean observation: 173.007 [24.000, 255.000], loss: 0.052906, mean_absolute_error: 10.201873, mean_q: 11.917710, mean_eps: 0.747039\n",
      "  281491/2000000: episode: 1454, duration: 11.527s, episode steps: 289, steps per second: 25, episode reward: 151.900, mean reward: 0.526 [-1.000, 1.000], mean action: 3.187 [0.000, 6.000], mean observation: 173.050 [23.000, 255.000], loss: 0.049793, mean_absolute_error: 10.212552, mean_q: 11.936595, mean_eps: 0.746789\n",
      "  281765/2000000: episode: 1455, duration: 10.477s, episode steps: 274, steps per second: 26, episode reward: 101.900, mean reward: 0.372 [-1.000, 0.500], mean action: 3.241 [0.000, 6.000], mean observation: 173.801 [23.000, 255.000], loss: 0.048210, mean_absolute_error: 10.198540, mean_q: 11.913700, mean_eps: 0.746535\n",
      "  281993/2000000: episode: 1456, duration: 8.933s, episode steps: 228, steps per second: 26, episode reward: 141.300, mean reward: 0.620 [-1.000, 1.000], mean action: 2.987 [0.000, 6.000], mean observation: 171.849 [24.000, 255.000], loss: 0.050766, mean_absolute_error: 10.346231, mean_q: 12.082236, mean_eps: 0.746308\n",
      "  282213/2000000: episode: 1457, duration: 8.740s, episode steps: 220, steps per second: 25, episode reward: 96.300, mean reward: 0.438 [-1.000, 1.000], mean action: 3.009 [0.000, 6.000], mean observation: 171.826 [24.000, 255.000], loss: 0.055349, mean_absolute_error: 10.130776, mean_q: 11.822067, mean_eps: 0.746106\n",
      "  282370/2000000: episode: 1458, duration: 6.772s, episode steps: 157, steps per second: 23, episode reward: 87.700, mean reward: 0.559 [-1.000, 1.000], mean action: 3.510 [0.000, 6.000], mean observation: 173.473 [23.000, 255.000], loss: 0.050472, mean_absolute_error: 10.370881, mean_q: 12.128821, mean_eps: 0.745937\n",
      "  282619/2000000: episode: 1459, duration: 9.639s, episode steps: 249, steps per second: 26, episode reward: 187.100, mean reward: 0.751 [-1.000, 1.000], mean action: 3.096 [0.000, 6.000], mean observation: 171.669 [25.000, 255.000], loss: 0.044001, mean_absolute_error: 10.113638, mean_q: 11.796519, mean_eps: 0.745755\n",
      "  282802/2000000: episode: 1460, duration: 7.579s, episode steps: 183, steps per second: 24, episode reward: 104.500, mean reward: 0.571 [-1.000, 1.000], mean action: 3.208 [0.000, 6.000], mean observation: 172.908 [24.000, 255.000], loss: 0.047299, mean_absolute_error: 10.207582, mean_q: 11.908025, mean_eps: 0.745561\n",
      "  283074/2000000: episode: 1461, duration: 10.799s, episode steps: 272, steps per second: 25, episode reward: 174.400, mean reward: 0.641 [-1.000, 1.000], mean action: 3.121 [0.000, 6.000], mean observation: 173.208 [24.000, 255.000], loss: 0.052765, mean_absolute_error: 10.272087, mean_q: 12.008297, mean_eps: 0.745356\n",
      "  283327/2000000: episode: 1462, duration: 9.742s, episode steps: 253, steps per second: 26, episode reward: 164.300, mean reward: 0.649 [-1.000, 1.000], mean action: 3.004 [0.000, 6.000], mean observation: 171.557 [24.000, 255.000], loss: 0.051223, mean_absolute_error: 10.272752, mean_q: 11.985763, mean_eps: 0.745120\n",
      "  283578/2000000: episode: 1463, duration: 9.837s, episode steps: 251, steps per second: 26, episode reward: 157.200, mean reward: 0.626 [-1.000, 1.000], mean action: 3.219 [0.000, 6.000], mean observation: 171.917 [24.000, 255.000], loss: 0.048826, mean_absolute_error: 10.004906, mean_q: 11.687071, mean_eps: 0.744893\n",
      "  283822/2000000: episode: 1464, duration: 9.327s, episode steps: 244, steps per second: 26, episode reward: 123.600, mean reward: 0.507 [-1.000, 1.000], mean action: 3.152 [0.000, 6.000], mean observation: 172.319 [24.000, 255.000], loss: 0.049956, mean_absolute_error: 10.401026, mean_q: 12.145905, mean_eps: 0.744670\n",
      "  284086/2000000: episode: 1465, duration: 10.411s, episode steps: 264, steps per second: 25, episode reward: 190.300, mean reward: 0.721 [-1.000, 1.000], mean action: 3.125 [0.000, 6.000], mean observation: 173.191 [24.000, 255.000], loss: 0.054281, mean_absolute_error: 10.368055, mean_q: 12.113998, mean_eps: 0.744441\n",
      "  284377/2000000: episode: 1466, duration: 11.591s, episode steps: 291, steps per second: 25, episode reward: 163.400, mean reward: 0.562 [-1.000, 1.000], mean action: 3.175 [0.000, 6.000], mean observation: 172.950 [24.000, 255.000], loss: 0.051922, mean_absolute_error: 10.151756, mean_q: 11.853561, mean_eps: 0.744191\n",
      "  284626/2000000: episode: 1467, duration: 9.777s, episode steps: 249, steps per second: 25, episode reward: 186.000, mean reward: 0.747 [-1.000, 1.000], mean action: 3.020 [0.000, 6.000], mean observation: 171.816 [24.000, 255.000], loss: 0.049266, mean_absolute_error: 10.219787, mean_q: 11.932862, mean_eps: 0.743948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  284874/2000000: episode: 1468, duration: 9.403s, episode steps: 248, steps per second: 26, episode reward: 103.700, mean reward: 0.418 [-1.000, 0.500], mean action: 2.956 [0.000, 6.000], mean observation: 172.802 [24.000, 255.000], loss: 0.049505, mean_absolute_error: 10.613106, mean_q: 12.386958, mean_eps: 0.743725\n",
      "  285137/2000000: episode: 1469, duration: 10.386s, episode steps: 263, steps per second: 25, episode reward: 177.200, mean reward: 0.674 [-1.000, 1.000], mean action: 3.251 [0.000, 6.000], mean observation: 172.605 [24.000, 255.000], loss: 0.044881, mean_absolute_error: 10.236789, mean_q: 11.945847, mean_eps: 0.743495\n",
      "  285396/2000000: episode: 1470, duration: 9.992s, episode steps: 259, steps per second: 26, episode reward: 148.500, mean reward: 0.573 [-1.000, 1.000], mean action: 3.286 [0.000, 6.000], mean observation: 172.368 [24.000, 255.000], loss: 0.050077, mean_absolute_error: 10.151369, mean_q: 11.856105, mean_eps: 0.743261\n",
      "  285666/2000000: episode: 1471, duration: 10.671s, episode steps: 270, steps per second: 25, episode reward: 189.100, mean reward: 0.700 [-1.000, 1.000], mean action: 3.156 [0.000, 6.000], mean observation: 172.432 [23.000, 255.000], loss: 0.046983, mean_absolute_error: 10.237939, mean_q: 11.953672, mean_eps: 0.743023\n",
      "  285920/2000000: episode: 1472, duration: 10.099s, episode steps: 254, steps per second: 25, episode reward: 180.000, mean reward: 0.709 [-1.000, 1.000], mean action: 3.016 [0.000, 6.000], mean observation: 172.598 [23.000, 255.000], loss: 0.059195, mean_absolute_error: 10.205653, mean_q: 11.933917, mean_eps: 0.742787\n",
      "  286158/2000000: episode: 1473, duration: 9.045s, episode steps: 238, steps per second: 26, episode reward: 92.300, mean reward: 0.388 [-1.000, 0.500], mean action: 3.223 [0.000, 6.000], mean observation: 172.256 [24.000, 255.000], loss: 0.054395, mean_absolute_error: 10.256965, mean_q: 11.990350, mean_eps: 0.742566\n",
      "  286429/2000000: episode: 1474, duration: 10.672s, episode steps: 271, steps per second: 25, episode reward: 148.300, mean reward: 0.547 [-1.000, 1.000], mean action: 3.089 [0.000, 6.000], mean observation: 172.498 [22.000, 255.000], loss: 0.045769, mean_absolute_error: 10.183550, mean_q: 11.904900, mean_eps: 0.742335\n",
      "  286702/2000000: episode: 1475, duration: 10.517s, episode steps: 273, steps per second: 26, episode reward: 184.000, mean reward: 0.674 [-1.000, 1.000], mean action: 3.179 [0.000, 6.000], mean observation: 172.576 [24.000, 255.000], loss: 0.051509, mean_absolute_error: 10.271803, mean_q: 11.983224, mean_eps: 0.742091\n",
      "  286960/2000000: episode: 1476, duration: 9.892s, episode steps: 258, steps per second: 26, episode reward: 175.600, mean reward: 0.681 [-1.000, 1.000], mean action: 3.124 [0.000, 6.000], mean observation: 171.768 [24.000, 255.000], loss: 0.052323, mean_absolute_error: 10.125021, mean_q: 11.811186, mean_eps: 0.741853\n",
      "  287204/2000000: episode: 1477, duration: 10.188s, episode steps: 244, steps per second: 24, episode reward: 164.900, mean reward: 0.676 [-1.000, 1.000], mean action: 2.939 [0.000, 6.000], mean observation: 171.674 [24.000, 255.000], loss: 0.057600, mean_absolute_error: 10.324466, mean_q: 12.063605, mean_eps: 0.741628\n",
      "  287458/2000000: episode: 1478, duration: 9.760s, episode steps: 254, steps per second: 26, episode reward: 190.200, mean reward: 0.749 [-1.000, 1.000], mean action: 3.063 [0.000, 6.000], mean observation: 171.650 [24.000, 255.000], loss: 0.051293, mean_absolute_error: 10.263213, mean_q: 11.967835, mean_eps: 0.741403\n",
      "  287734/2000000: episode: 1479, duration: 10.704s, episode steps: 276, steps per second: 26, episode reward: 121.300, mean reward: 0.439 [-1.000, 0.500], mean action: 3.109 [0.000, 6.000], mean observation: 173.064 [23.000, 255.000], loss: 0.040303, mean_absolute_error: 10.107571, mean_q: 11.804379, mean_eps: 0.741164\n",
      "  287985/2000000: episode: 1480, duration: 9.144s, episode steps: 251, steps per second: 27, episode reward: 70.400, mean reward: 0.280 [-1.000, 0.500], mean action: 3.076 [0.000, 6.000], mean observation: 172.939 [22.000, 255.000], loss: 0.051431, mean_absolute_error: 10.291250, mean_q: 12.025686, mean_eps: 0.740926\n",
      "  288273/2000000: episode: 1481, duration: 12.047s, episode steps: 288, steps per second: 24, episode reward: 203.700, mean reward: 0.707 [-1.000, 1.000], mean action: 3.392 [0.000, 6.000], mean observation: 172.738 [23.000, 255.000], loss: 0.043843, mean_absolute_error: 10.321233, mean_q: 12.043617, mean_eps: 0.740683\n",
      "  288544/2000000: episode: 1482, duration: 11.325s, episode steps: 271, steps per second: 24, episode reward: 149.100, mean reward: 0.550 [-1.000, 1.000], mean action: 3.085 [0.000, 6.000], mean observation: 172.075 [24.000, 255.000], loss: 0.048462, mean_absolute_error: 10.276631, mean_q: 11.987451, mean_eps: 0.740433\n",
      "  288803/2000000: episode: 1483, duration: 10.464s, episode steps: 259, steps per second: 25, episode reward: 129.500, mean reward: 0.500 [-1.000, 1.000], mean action: 3.367 [0.000, 6.000], mean observation: 172.818 [24.000, 255.000], loss: 0.047071, mean_absolute_error: 10.220040, mean_q: 11.922279, mean_eps: 0.740195\n",
      "  289032/2000000: episode: 1484, duration: 8.724s, episode steps: 229, steps per second: 26, episode reward: 116.800, mean reward: 0.510 [-1.000, 1.000], mean action: 2.961 [0.000, 6.000], mean observation: 171.151 [24.000, 255.000], loss: 0.045545, mean_absolute_error: 10.340491, mean_q: 12.079414, mean_eps: 0.739976\n",
      "  289182/2000000: episode: 1485, duration: 6.404s, episode steps: 150, steps per second: 23, episode reward: 84.800, mean reward: 0.565 [-1.000, 1.000], mean action: 3.333 [0.000, 6.000], mean observation: 173.294 [24.000, 255.000], loss: 0.051831, mean_absolute_error: 10.077096, mean_q: 11.766741, mean_eps: 0.739805\n",
      "  289424/2000000: episode: 1486, duration: 9.433s, episode steps: 242, steps per second: 26, episode reward: 133.100, mean reward: 0.550 [-1.000, 1.000], mean action: 2.864 [0.000, 6.000], mean observation: 171.514 [21.000, 255.000], loss: 0.046520, mean_absolute_error: 10.424476, mean_q: 12.169537, mean_eps: 0.739628\n",
      "  289720/2000000: episode: 1487, duration: 12.093s, episode steps: 296, steps per second: 24, episode reward: 197.800, mean reward: 0.668 [-1.000, 1.000], mean action: 3.378 [0.000, 6.000], mean observation: 172.557 [23.000, 255.000], loss: 0.048579, mean_absolute_error: 10.224434, mean_q: 11.943027, mean_eps: 0.739387\n",
      "  290010/2000000: episode: 1488, duration: 11.298s, episode steps: 290, steps per second: 26, episode reward: 101.100, mean reward: 0.349 [-1.000, 0.500], mean action: 3.086 [0.000, 6.000], mean observation: 172.294 [22.000, 255.000], loss: 0.053612, mean_absolute_error: 10.303992, mean_q: 12.037736, mean_eps: 0.739122\n",
      "  290275/2000000: episode: 1489, duration: 10.632s, episode steps: 265, steps per second: 25, episode reward: 160.000, mean reward: 0.604 [-1.000, 1.000], mean action: 3.060 [0.000, 6.000], mean observation: 172.148 [23.000, 255.000], loss: 0.067788, mean_absolute_error: 10.418763, mean_q: 12.182673, mean_eps: 0.738872\n",
      "  290454/2000000: episode: 1490, duration: 7.556s, episode steps: 179, steps per second: 24, episode reward: 98.200, mean reward: 0.549 [-1.000, 1.000], mean action: 3.279 [0.000, 6.000], mean observation: 173.318 [23.000, 255.000], loss: 0.061315, mean_absolute_error: 10.690087, mean_q: 12.501637, mean_eps: 0.738672\n",
      "  290685/2000000: episode: 1491, duration: 9.018s, episode steps: 231, steps per second: 26, episode reward: 156.800, mean reward: 0.679 [-1.000, 1.000], mean action: 2.896 [0.000, 6.000], mean observation: 170.786 [24.000, 255.000], loss: 0.050298, mean_absolute_error: 10.641395, mean_q: 12.453467, mean_eps: 0.738487\n",
      "  290957/2000000: episode: 1492, duration: 11.002s, episode steps: 272, steps per second: 25, episode reward: 202.400, mean reward: 0.744 [-1.000, 1.000], mean action: 3.320 [0.000, 6.000], mean observation: 171.693 [24.000, 255.000], loss: 0.058322, mean_absolute_error: 10.637283, mean_q: 12.442095, mean_eps: 0.738260\n",
      "  291201/2000000: episode: 1493, duration: 9.491s, episode steps: 244, steps per second: 26, episode reward: 165.000, mean reward: 0.676 [-1.000, 1.000], mean action: 3.164 [0.000, 6.000], mean observation: 171.070 [24.000, 255.000], loss: 0.058212, mean_absolute_error: 10.534764, mean_q: 12.307201, mean_eps: 0.738028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  291453/2000000: episode: 1494, duration: 9.866s, episode steps: 252, steps per second: 26, episode reward: 146.900, mean reward: 0.583 [-1.000, 1.000], mean action: 2.790 [0.000, 6.000], mean observation: 171.008 [24.000, 255.000], loss: 0.053724, mean_absolute_error: 10.761686, mean_q: 12.586861, mean_eps: 0.737805\n",
      "  291702/2000000: episode: 1495, duration: 9.651s, episode steps: 249, steps per second: 26, episode reward: 158.400, mean reward: 0.636 [-1.000, 1.000], mean action: 3.004 [0.000, 6.000], mean observation: 170.780 [23.000, 255.000], loss: 0.049286, mean_absolute_error: 10.653147, mean_q: 12.435465, mean_eps: 0.737580\n",
      "  291969/2000000: episode: 1496, duration: 10.508s, episode steps: 267, steps per second: 25, episode reward: 117.200, mean reward: 0.439 [-1.000, 0.500], mean action: 3.277 [0.000, 6.000], mean observation: 172.779 [24.000, 255.000], loss: 0.059521, mean_absolute_error: 10.768606, mean_q: 12.578316, mean_eps: 0.737348\n",
      "  292238/2000000: episode: 1497, duration: 10.734s, episode steps: 269, steps per second: 25, episode reward: 197.200, mean reward: 0.733 [-1.000, 1.000], mean action: 3.190 [0.000, 6.000], mean observation: 171.923 [22.000, 255.000], loss: 0.052815, mean_absolute_error: 10.574886, mean_q: 12.351750, mean_eps: 0.737106\n",
      "  292495/2000000: episode: 1498, duration: 9.799s, episode steps: 257, steps per second: 26, episode reward: 172.800, mean reward: 0.672 [-1.000, 1.000], mean action: 3.156 [0.000, 6.000], mean observation: 170.858 [24.000, 255.000], loss: 0.052461, mean_absolute_error: 10.428516, mean_q: 12.177622, mean_eps: 0.736871\n",
      "  292775/2000000: episode: 1499, duration: 10.862s, episode steps: 280, steps per second: 26, episode reward: 169.700, mean reward: 0.606 [-1.000, 1.000], mean action: 3.161 [0.000, 6.000], mean observation: 172.458 [23.000, 255.000], loss: 0.058312, mean_absolute_error: 10.672908, mean_q: 12.477578, mean_eps: 0.736629\n",
      "  293015/2000000: episode: 1500, duration: 8.877s, episode steps: 240, steps per second: 27, episode reward: 148.200, mean reward: 0.617 [-1.000, 1.000], mean action: 3.117 [0.000, 6.000], mean observation: 170.676 [24.000, 255.000], loss: 0.056268, mean_absolute_error: 10.527301, mean_q: 12.308406, mean_eps: 0.736395\n",
      "  293290/2000000: episode: 1501, duration: 11.202s, episode steps: 275, steps per second: 25, episode reward: 204.100, mean reward: 0.742 [-1.000, 1.000], mean action: 3.120 [0.000, 6.000], mean observation: 171.764 [23.000, 255.000], loss: 0.058484, mean_absolute_error: 10.723718, mean_q: 12.528574, mean_eps: 0.736163\n",
      "  293554/2000000: episode: 1502, duration: 10.539s, episode steps: 264, steps per second: 25, episode reward: 191.600, mean reward: 0.726 [-1.000, 1.000], mean action: 3.027 [0.000, 6.000], mean observation: 171.836 [25.000, 255.000], loss: 0.054888, mean_absolute_error: 10.512214, mean_q: 12.280227, mean_eps: 0.735920\n",
      "  293808/2000000: episode: 1503, duration: 9.844s, episode steps: 254, steps per second: 26, episode reward: 172.700, mean reward: 0.680 [-1.000, 1.000], mean action: 2.965 [0.000, 6.000], mean observation: 171.238 [24.000, 255.000], loss: 0.057645, mean_absolute_error: 10.538496, mean_q: 12.334634, mean_eps: 0.735688\n",
      "  294092/2000000: episode: 1504, duration: 11.429s, episode steps: 284, steps per second: 25, episode reward: 227.200, mean reward: 0.800 [-1.000, 1.000], mean action: 3.264 [0.000, 6.000], mean observation: 171.969 [24.000, 255.000], loss: 0.051039, mean_absolute_error: 10.724413, mean_q: 12.533223, mean_eps: 0.735447\n",
      "  294329/2000000: episode: 1505, duration: 9.949s, episode steps: 237, steps per second: 24, episode reward: 159.500, mean reward: 0.673 [-1.000, 1.000], mean action: 3.283 [0.000, 6.000], mean observation: 171.772 [24.000, 255.000], loss: 0.056970, mean_absolute_error: 10.754353, mean_q: 12.574942, mean_eps: 0.735211\n",
      "  294515/2000000: episode: 1506, duration: 6.861s, episode steps: 186, steps per second: 27, episode reward: 73.100, mean reward: 0.393 [-1.000, 0.500], mean action: 2.661 [0.000, 6.000], mean observation: 172.463 [24.000, 255.000], loss: 0.050260, mean_absolute_error: 10.764797, mean_q: 12.589383, mean_eps: 0.735020\n",
      "  294778/2000000: episode: 1507, duration: 10.799s, episode steps: 263, steps per second: 24, episode reward: 165.200, mean reward: 0.628 [-1.000, 1.000], mean action: 3.065 [0.000, 6.000], mean observation: 172.190 [24.000, 255.000], loss: 0.053910, mean_absolute_error: 10.784879, mean_q: 12.598560, mean_eps: 0.734819\n",
      "  295061/2000000: episode: 1508, duration: 11.649s, episode steps: 283, steps per second: 24, episode reward: 202.600, mean reward: 0.716 [-1.000, 1.000], mean action: 3.237 [0.000, 6.000], mean observation: 172.214 [23.000, 255.000], loss: 0.059770, mean_absolute_error: 10.903088, mean_q: 12.742817, mean_eps: 0.734572\n",
      "  295240/2000000: episode: 1509, duration: 7.577s, episode steps: 179, steps per second: 24, episode reward: 97.900, mean reward: 0.547 [-1.000, 1.000], mean action: 3.184 [0.000, 6.000], mean observation: 172.658 [24.000, 255.000], loss: 0.051159, mean_absolute_error: 10.929957, mean_q: 12.774183, mean_eps: 0.734365\n",
      "  295483/2000000: episode: 1510, duration: 9.663s, episode steps: 243, steps per second: 25, episode reward: 95.600, mean reward: 0.393 [-1.000, 0.500], mean action: 2.893 [0.000, 6.000], mean observation: 172.701 [23.000, 255.000], loss: 0.056916, mean_absolute_error: 10.761060, mean_q: 12.575935, mean_eps: 0.734176\n",
      "  295743/2000000: episode: 1511, duration: 10.394s, episode steps: 260, steps per second: 25, episode reward: 104.900, mean reward: 0.403 [-1.000, 0.500], mean action: 2.965 [0.000, 6.000], mean observation: 173.380 [24.000, 255.000], loss: 0.055330, mean_absolute_error: 10.724721, mean_q: 12.521314, mean_eps: 0.733949\n",
      "  295990/2000000: episode: 1512, duration: 10.073s, episode steps: 247, steps per second: 25, episode reward: 182.900, mean reward: 0.740 [-1.000, 1.000], mean action: 3.235 [0.000, 6.000], mean observation: 171.952 [24.000, 255.000], loss: 0.049766, mean_absolute_error: 10.666897, mean_q: 12.467123, mean_eps: 0.733721\n",
      "  296262/2000000: episode: 1513, duration: 11.536s, episode steps: 272, steps per second: 24, episode reward: 200.900, mean reward: 0.739 [-1.000, 1.000], mean action: 3.243 [0.000, 6.000], mean observation: 172.250 [25.000, 255.000], loss: 0.046539, mean_absolute_error: 10.646737, mean_q: 12.449095, mean_eps: 0.733487\n",
      "  296441/2000000: episode: 1514, duration: 7.769s, episode steps: 179, steps per second: 23, episode reward: 104.800, mean reward: 0.585 [-1.000, 1.000], mean action: 3.369 [0.000, 6.000], mean observation: 172.368 [22.000, 255.000], loss: 0.053954, mean_absolute_error: 10.870974, mean_q: 12.701491, mean_eps: 0.733283\n",
      "  296678/2000000: episode: 1515, duration: 9.818s, episode steps: 237, steps per second: 24, episode reward: 163.100, mean reward: 0.688 [-1.000, 1.000], mean action: 3.114 [0.000, 6.000], mean observation: 172.146 [23.000, 255.000], loss: 0.048249, mean_absolute_error: 10.748813, mean_q: 12.556210, mean_eps: 0.733096\n",
      "  296942/2000000: episode: 1516, duration: 10.879s, episode steps: 264, steps per second: 24, episode reward: 159.100, mean reward: 0.603 [-1.000, 1.000], mean action: 3.246 [0.000, 6.000], mean observation: 172.404 [25.000, 255.000], loss: 0.057445, mean_absolute_error: 11.141626, mean_q: 13.026184, mean_eps: 0.732871\n",
      "  297205/2000000: episode: 1517, duration: 10.164s, episode steps: 263, steps per second: 26, episode reward: 164.400, mean reward: 0.625 [-1.000, 1.000], mean action: 2.920 [0.000, 6.000], mean observation: 173.328 [20.000, 255.000], loss: 0.050957, mean_absolute_error: 10.851426, mean_q: 12.675258, mean_eps: 0.732633\n",
      "  297399/2000000: episode: 1518, duration: 7.396s, episode steps: 194, steps per second: 26, episode reward: 102.100, mean reward: 0.526 [-1.000, 1.000], mean action: 2.835 [0.000, 6.000], mean observation: 171.682 [23.000, 255.000], loss: 0.050553, mean_absolute_error: 10.905432, mean_q: 12.756946, mean_eps: 0.732428\n",
      "  297638/2000000: episode: 1519, duration: 8.906s, episode steps: 239, steps per second: 27, episode reward: 139.400, mean reward: 0.583 [-1.000, 1.000], mean action: 3.213 [0.000, 6.000], mean observation: 171.559 [23.000, 255.000], loss: 0.051281, mean_absolute_error: 11.152338, mean_q: 13.047671, mean_eps: 0.732234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  297876/2000000: episode: 1520, duration: 9.686s, episode steps: 238, steps per second: 25, episode reward: 168.100, mean reward: 0.706 [-1.000, 1.000], mean action: 3.160 [0.000, 6.000], mean observation: 171.743 [24.000, 255.000], loss: 0.048342, mean_absolute_error: 10.574033, mean_q: 12.344408, mean_eps: 0.732020\n",
      "  298109/2000000: episode: 1521, duration: 9.328s, episode steps: 233, steps per second: 25, episode reward: 176.100, mean reward: 0.756 [-1.000, 1.000], mean action: 2.987 [0.000, 6.000], mean observation: 171.875 [24.000, 255.000], loss: 0.055163, mean_absolute_error: 10.877304, mean_q: 12.717941, mean_eps: 0.731807\n",
      "  298326/2000000: episode: 1522, duration: 7.948s, episode steps: 217, steps per second: 27, episode reward: 89.800, mean reward: 0.414 [-1.000, 0.500], mean action: 2.903 [0.000, 6.000], mean observation: 172.226 [23.000, 255.000], loss: 0.053259, mean_absolute_error: 10.980014, mean_q: 12.811782, mean_eps: 0.731604\n",
      "  298559/2000000: episode: 1523, duration: 8.657s, episode steps: 233, steps per second: 27, episode reward: 154.700, mean reward: 0.664 [-1.000, 1.000], mean action: 2.966 [0.000, 6.000], mean observation: 171.593 [24.000, 255.000], loss: 0.047877, mean_absolute_error: 10.866732, mean_q: 12.690597, mean_eps: 0.731402\n",
      "  298829/2000000: episode: 1524, duration: 10.642s, episode steps: 270, steps per second: 25, episode reward: 161.100, mean reward: 0.597 [-1.000, 1.000], mean action: 3.200 [0.000, 6.000], mean observation: 173.376 [24.000, 255.000], loss: 0.051664, mean_absolute_error: 10.799584, mean_q: 12.614345, mean_eps: 0.731175\n",
      "  299109/2000000: episode: 1525, duration: 11.471s, episode steps: 280, steps per second: 24, episode reward: 167.100, mean reward: 0.597 [-1.000, 1.000], mean action: 3.411 [0.000, 6.000], mean observation: 172.607 [23.000, 255.000], loss: 0.051100, mean_absolute_error: 10.798240, mean_q: 12.624544, mean_eps: 0.730927\n",
      "  299377/2000000: episode: 1526, duration: 10.958s, episode steps: 268, steps per second: 24, episode reward: 190.900, mean reward: 0.712 [-1.000, 1.000], mean action: 3.366 [0.000, 6.000], mean observation: 172.518 [24.000, 255.000], loss: 0.058155, mean_absolute_error: 10.967524, mean_q: 12.811023, mean_eps: 0.730680\n",
      "  299639/2000000: episode: 1527, duration: 10.989s, episode steps: 262, steps per second: 24, episode reward: 184.100, mean reward: 0.703 [-1.000, 1.000], mean action: 3.206 [0.000, 6.000], mean observation: 172.290 [24.000, 255.000], loss: 0.048961, mean_absolute_error: 10.931818, mean_q: 12.770788, mean_eps: 0.730443\n",
      "  299867/2000000: episode: 1528, duration: 8.851s, episode steps: 228, steps per second: 26, episode reward: 163.200, mean reward: 0.716 [-1.000, 1.000], mean action: 2.789 [0.000, 6.000], mean observation: 171.479 [24.000, 255.000], loss: 0.049642, mean_absolute_error: 10.990406, mean_q: 12.847387, mean_eps: 0.730223\n",
      "  300113/2000000: episode: 1529, duration: 9.736s, episode steps: 246, steps per second: 25, episode reward: 178.800, mean reward: 0.727 [-1.000, 1.000], mean action: 3.134 [0.000, 6.000], mean observation: 172.035 [24.000, 255.000], loss: 0.082013, mean_absolute_error: 11.247957, mean_q: 13.149704, mean_eps: 0.730009\n",
      "  300346/2000000: episode: 1530, duration: 9.292s, episode steps: 233, steps per second: 25, episode reward: 160.900, mean reward: 0.691 [-1.000, 1.000], mean action: 3.056 [0.000, 6.000], mean observation: 171.818 [20.000, 255.000], loss: 0.072412, mean_absolute_error: 11.507983, mean_q: 13.455708, mean_eps: 0.729793\n",
      "  300631/2000000: episode: 1531, duration: 11.197s, episode steps: 285, steps per second: 25, episode reward: 151.100, mean reward: 0.530 [-1.000, 1.000], mean action: 3.214 [0.000, 6.000], mean observation: 173.477 [22.000, 255.000], loss: 0.062023, mean_absolute_error: 11.050179, mean_q: 12.898064, mean_eps: 0.729561\n",
      "  300868/2000000: episode: 1532, duration: 9.302s, episode steps: 237, steps per second: 25, episode reward: 163.300, mean reward: 0.689 [-1.000, 1.000], mean action: 3.025 [0.000, 6.000], mean observation: 171.589 [23.000, 255.000], loss: 0.059978, mean_absolute_error: 11.136076, mean_q: 13.022336, mean_eps: 0.729327\n",
      "  301114/2000000: episode: 1533, duration: 10.034s, episode steps: 246, steps per second: 25, episode reward: 171.100, mean reward: 0.696 [-1.000, 1.000], mean action: 2.825 [0.000, 6.000], mean observation: 172.779 [22.000, 255.000], loss: 0.061107, mean_absolute_error: 11.220073, mean_q: 13.120299, mean_eps: 0.729109\n",
      "  301365/2000000: episode: 1534, duration: 9.887s, episode steps: 251, steps per second: 25, episode reward: 153.600, mean reward: 0.612 [-1.000, 1.000], mean action: 2.880 [0.000, 6.000], mean observation: 172.672 [24.000, 255.000], loss: 0.054355, mean_absolute_error: 11.182431, mean_q: 13.074154, mean_eps: 0.728884\n",
      "  301578/2000000: episode: 1535, duration: 7.749s, episode steps: 213, steps per second: 27, episode reward: 72.600, mean reward: 0.341 [-1.000, 0.500], mean action: 2.770 [0.000, 6.000], mean observation: 173.095 [24.000, 255.000], loss: 0.068886, mean_absolute_error: 11.358113, mean_q: 13.275316, mean_eps: 0.728675\n",
      "  301840/2000000: episode: 1536, duration: 10.176s, episode steps: 262, steps per second: 26, episode reward: 161.000, mean reward: 0.615 [-1.000, 1.000], mean action: 2.889 [0.000, 6.000], mean observation: 172.916 [23.000, 255.000], loss: 0.063016, mean_absolute_error: 11.227777, mean_q: 13.126356, mean_eps: 0.728463\n",
      "  302089/2000000: episode: 1537, duration: 9.636s, episode steps: 249, steps per second: 26, episode reward: 114.400, mean reward: 0.459 [-1.000, 1.000], mean action: 3.165 [0.000, 6.000], mean observation: 172.841 [23.000, 255.000], loss: 0.065039, mean_absolute_error: 11.315805, mean_q: 13.221562, mean_eps: 0.728232\n",
      "  302342/2000000: episode: 1538, duration: 9.518s, episode steps: 253, steps per second: 27, episode reward: 175.200, mean reward: 0.692 [-1.000, 1.000], mean action: 3.119 [0.000, 6.000], mean observation: 171.772 [24.000, 255.000], loss: 0.057728, mean_absolute_error: 11.416724, mean_q: 13.346498, mean_eps: 0.728006\n",
      "  302594/2000000: episode: 1539, duration: 9.867s, episode steps: 252, steps per second: 26, episode reward: 174.100, mean reward: 0.691 [-1.000, 1.000], mean action: 3.115 [0.000, 6.000], mean observation: 172.053 [24.000, 255.000], loss: 0.059902, mean_absolute_error: 11.291623, mean_q: 13.184038, mean_eps: 0.727779\n",
      "  302774/2000000: episode: 1540, duration: 6.399s, episode steps: 180, steps per second: 28, episode reward: 58.100, mean reward: 0.323 [-1.000, 0.500], mean action: 2.728 [0.000, 6.000], mean observation: 173.402 [24.000, 255.000], loss: 0.063988, mean_absolute_error: 11.065089, mean_q: 12.915951, mean_eps: 0.727584\n",
      "  303018/2000000: episode: 1541, duration: 9.499s, episode steps: 244, steps per second: 26, episode reward: 163.600, mean reward: 0.670 [-1.000, 1.000], mean action: 2.951 [0.000, 6.000], mean observation: 171.467 [22.000, 255.000], loss: 0.054950, mean_absolute_error: 11.251071, mean_q: 13.140258, mean_eps: 0.727394\n",
      "  303280/2000000: episode: 1542, duration: 10.020s, episode steps: 262, steps per second: 26, episode reward: 162.900, mean reward: 0.622 [-1.000, 1.000], mean action: 2.878 [0.000, 6.000], mean observation: 172.761 [24.000, 255.000], loss: 0.057200, mean_absolute_error: 11.266570, mean_q: 13.169242, mean_eps: 0.727167\n",
      "  303551/2000000: episode: 1543, duration: 10.688s, episode steps: 271, steps per second: 25, episode reward: 197.900, mean reward: 0.730 [-1.000, 1.000], mean action: 3.244 [0.000, 6.000], mean observation: 172.678 [24.000, 255.000], loss: 0.057260, mean_absolute_error: 11.265694, mean_q: 13.169715, mean_eps: 0.726927\n",
      "  303808/2000000: episode: 1544, duration: 9.781s, episode steps: 257, steps per second: 26, episode reward: 169.900, mean reward: 0.661 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 171.768 [23.000, 255.000], loss: 0.059018, mean_absolute_error: 11.288861, mean_q: 13.190442, mean_eps: 0.726690\n",
      "  304091/2000000: episode: 1545, duration: 10.910s, episode steps: 283, steps per second: 26, episode reward: 173.400, mean reward: 0.613 [-1.000, 1.000], mean action: 3.159 [0.000, 6.000], mean observation: 173.101 [24.000, 255.000], loss: 0.056891, mean_absolute_error: 11.456778, mean_q: 13.391509, mean_eps: 0.726447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  304325/2000000: episode: 1546, duration: 8.803s, episode steps: 234, steps per second: 27, episode reward: 145.900, mean reward: 0.624 [-1.000, 1.000], mean action: 3.218 [0.000, 6.000], mean observation: 171.702 [23.000, 255.000], loss: 0.071124, mean_absolute_error: 11.523122, mean_q: 13.484852, mean_eps: 0.726213\n",
      "  304576/2000000: episode: 1547, duration: 9.897s, episode steps: 251, steps per second: 25, episode reward: 180.000, mean reward: 0.717 [-1.000, 1.000], mean action: 3.295 [0.000, 6.000], mean observation: 172.020 [24.000, 255.000], loss: 0.061618, mean_absolute_error: 11.338889, mean_q: 13.251302, mean_eps: 0.725995\n",
      "  304812/2000000: episode: 1548, duration: 9.477s, episode steps: 236, steps per second: 25, episode reward: 174.400, mean reward: 0.739 [-1.000, 1.000], mean action: 3.123 [0.000, 6.000], mean observation: 171.412 [23.000, 255.000], loss: 0.055700, mean_absolute_error: 11.109517, mean_q: 12.976601, mean_eps: 0.725777\n",
      "  305092/2000000: episode: 1549, duration: 11.212s, episode steps: 280, steps per second: 25, episode reward: 140.100, mean reward: 0.500 [-1.000, 1.000], mean action: 3.143 [0.000, 6.000], mean observation: 173.207 [21.000, 255.000], loss: 0.068105, mean_absolute_error: 11.302266, mean_q: 13.199055, mean_eps: 0.725545\n",
      "  305358/2000000: episode: 1550, duration: 10.573s, episode steps: 266, steps per second: 25, episode reward: 166.200, mean reward: 0.625 [-1.000, 1.000], mean action: 3.102 [0.000, 6.000], mean observation: 172.916 [22.000, 255.000], loss: 0.056408, mean_absolute_error: 11.313927, mean_q: 13.210400, mean_eps: 0.725298\n",
      "  305606/2000000: episode: 1551, duration: 9.657s, episode steps: 248, steps per second: 26, episode reward: 184.100, mean reward: 0.742 [-1.000, 1.000], mean action: 3.101 [0.000, 6.000], mean observation: 171.766 [21.000, 255.000], loss: 0.051607, mean_absolute_error: 11.158301, mean_q: 13.028854, mean_eps: 0.725066\n",
      "  305863/2000000: episode: 1552, duration: 9.988s, episode steps: 257, steps per second: 26, episode reward: 189.400, mean reward: 0.737 [-1.000, 1.000], mean action: 3.167 [0.000, 6.000], mean observation: 171.695 [24.000, 255.000], loss: 0.056722, mean_absolute_error: 11.556067, mean_q: 13.518870, mean_eps: 0.724839\n",
      "  306134/2000000: episode: 1553, duration: 10.773s, episode steps: 271, steps per second: 25, episode reward: 150.500, mean reward: 0.555 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 172.690 [23.000, 255.000], loss: 0.058260, mean_absolute_error: 11.307186, mean_q: 13.209312, mean_eps: 0.724602\n",
      "  306324/2000000: episode: 1554, duration: 8.026s, episode steps: 190, steps per second: 24, episode reward: 99.000, mean reward: 0.521 [-1.000, 1.000], mean action: 3.342 [0.000, 6.000], mean observation: 173.461 [21.000, 255.000], loss: 0.053684, mean_absolute_error: 11.355176, mean_q: 13.259963, mean_eps: 0.724395\n",
      "  306576/2000000: episode: 1555, duration: 9.935s, episode steps: 252, steps per second: 25, episode reward: 177.700, mean reward: 0.705 [-1.000, 1.000], mean action: 3.175 [0.000, 6.000], mean observation: 171.539 [24.000, 255.000], loss: 0.058519, mean_absolute_error: 11.270924, mean_q: 13.176047, mean_eps: 0.724197\n",
      "  306836/2000000: episode: 1556, duration: 10.040s, episode steps: 260, steps per second: 26, episode reward: 160.700, mean reward: 0.618 [-1.000, 1.000], mean action: 3.027 [0.000, 6.000], mean observation: 171.626 [23.000, 255.000], loss: 0.057684, mean_absolute_error: 11.546789, mean_q: 13.500885, mean_eps: 0.723966\n",
      "  307138/2000000: episode: 1557, duration: 12.111s, episode steps: 302, steps per second: 25, episode reward: 213.700, mean reward: 0.708 [-1.000, 1.000], mean action: 3.493 [0.000, 6.000], mean observation: 172.345 [24.000, 255.000], loss: 0.060283, mean_absolute_error: 11.340385, mean_q: 13.249951, mean_eps: 0.723713\n",
      "  307406/2000000: episode: 1558, duration: 10.512s, episode steps: 268, steps per second: 25, episode reward: 198.500, mean reward: 0.741 [-1.000, 1.000], mean action: 3.235 [0.000, 6.000], mean observation: 172.415 [24.000, 255.000], loss: 0.060730, mean_absolute_error: 11.342935, mean_q: 13.242592, mean_eps: 0.723455\n",
      "  307646/2000000: episode: 1559, duration: 9.229s, episode steps: 240, steps per second: 26, episode reward: 168.400, mean reward: 0.702 [-1.000, 1.000], mean action: 3.004 [0.000, 6.000], mean observation: 171.123 [24.000, 255.000], loss: 0.057125, mean_absolute_error: 11.483368, mean_q: 13.414816, mean_eps: 0.723227\n",
      "  307900/2000000: episode: 1560, duration: 10.130s, episode steps: 254, steps per second: 25, episode reward: 181.300, mean reward: 0.714 [-1.000, 1.000], mean action: 3.130 [0.000, 6.000], mean observation: 171.747 [24.000, 255.000], loss: 0.057197, mean_absolute_error: 11.417837, mean_q: 13.344111, mean_eps: 0.723005\n",
      "  308102/2000000: episode: 1561, duration: 7.118s, episode steps: 202, steps per second: 28, episode reward: 67.500, mean reward: 0.334 [-1.000, 0.500], mean action: 2.782 [0.000, 6.000], mean observation: 172.626 [24.000, 255.000], loss: 0.059520, mean_absolute_error: 11.145234, mean_q: 13.019460, mean_eps: 0.722800\n",
      "  308244/2000000: episode: 1562, duration: 5.170s, episode steps: 142, steps per second: 27, episode reward: 54.300, mean reward: 0.382 [-1.000, 0.500], mean action: 2.408 [0.000, 6.000], mean observation: 172.306 [24.000, 255.000], loss: 0.059649, mean_absolute_error: 11.535326, mean_q: 13.480319, mean_eps: 0.722645\n",
      "  308514/2000000: episode: 1563, duration: 10.664s, episode steps: 270, steps per second: 25, episode reward: 169.100, mean reward: 0.626 [-1.000, 1.000], mean action: 3.022 [0.000, 6.000], mean observation: 172.376 [23.000, 255.000], loss: 0.055919, mean_absolute_error: 11.365877, mean_q: 13.289340, mean_eps: 0.722460\n",
      "  308769/2000000: episode: 1564, duration: 9.916s, episode steps: 255, steps per second: 26, episode reward: 173.400, mean reward: 0.680 [-1.000, 1.000], mean action: 2.945 [0.000, 6.000], mean observation: 171.654 [24.000, 255.000], loss: 0.055013, mean_absolute_error: 11.196141, mean_q: 13.074400, mean_eps: 0.722222\n",
      "  309015/2000000: episode: 1565, duration: 9.212s, episode steps: 246, steps per second: 27, episode reward: 160.700, mean reward: 0.653 [-1.000, 1.000], mean action: 3.130 [0.000, 6.000], mean observation: 171.290 [25.000, 255.000], loss: 0.048631, mean_absolute_error: 11.192502, mean_q: 13.070505, mean_eps: 0.721997\n"
     ]
    }
   ],
   "source": [
    "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Concatenate\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from configparser import ConfigParser\n",
    "import os\n",
    "from os.path import join, pardir, exists\n",
    "\n",
    "from gym_airsim.airsim_car_env import AirSimCarEnv\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  #dynamically grow the memory used on the GPU\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "class AirSimCarProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "num_actions = int(config['car_agent']['actions'])\n",
    "                    \n",
    "WINDOW_LENGTH = 4\n",
    "INPUT_SHAPE = (84, 84)\n",
    "\n",
    "env = AirSimCarEnv()\n",
    "np.random.seed(123)\n",
    "\n",
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "def build_callbacks(env_name):\n",
    "    log_dir = 'logs'\n",
    "    if not exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    checkpoint_weights_filename = join(log_dir, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
    "    log_filename = join(log_dir,'dqn_{}_log.json'.format(env_name))\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=25000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    return callbacks\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),  attr='eps', value_max=1., \n",
    "                              value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "processor = AirSimCarProcessor()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=num_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.0001), metrics=['mae'])\n",
    "\n",
    "callbacks = build_callbacks('AirSimCarRL')\n",
    "\n",
    "dqn.fit(env, nb_steps=2000000,\n",
    "        visualize=False,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs'\n",
    "if not exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "weights_filename = join(log_dir,'dqn_{}_weights.h5f'.format('AirSimCarRL'))\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
